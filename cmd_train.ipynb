{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cmd_train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1Mrwlgkjv2G7LOLpOtCehzWxaTRu9sXPA","authorship_tag":"ABX9TyMEs/DfhGRTESZkd/lhX3ai"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"dymDs0Po0aES","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659164156993,"user_tz":-540,"elapsed":38,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"e34d93e4-f62e-4202-f63f-a765a298af71"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Jul 30 06:53:43 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp"],"metadata":{"id":"EfCxDoEXuzgg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659164157810,"user_tz":-540,"elapsed":831,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"cc237276-d57c-4e08-f8a8-0cfc14a315f7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp\n"]}]},{"cell_type":"code","source":["!python -m pip install -r requirements.txt"],"metadata":{"id":"mshVPsrku0kd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659164715847,"user_tz":-540,"elapsed":558044,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"8116a082-5c66-4fa5-d98a-9237ced5324f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.8.0\n","  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n","\u001b[K     |████████████████████████████████| 735.5 MB 15 kB/s \n","\u001b[?25hCollecting transformers==3.0.0\n","  Downloading transformers-3.0.0-py3-none-any.whl (754 kB)\n","\u001b[K     |████████████████████████████████| 754 kB 47.4 MB/s \n","\u001b[?25hCollecting torch_scatter==2.0.4\n","  Downloading torch_scatter-2.0.4.tar.gz (20 kB)\n","Collecting dgl==0.5.3\n","  Downloading dgl-0.5.3-cp37-cp37m-manylinux1_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 51.2 MB/s \n","\u001b[?25hCollecting hydra==2.4\n","  Downloading Hydra-2.4.tar.gz (80 kB)\n","\u001b[K     |████████████████████████████████| 80 kB 9.7 MB/s \n","\u001b[?25hCollecting omegaconf==2.2.2\n","  Downloading omegaconf-2.2.2-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 7.5 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->-r requirements.txt (line 2)) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->-r requirements.txt (line 2)) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0->-r requirements.txt (line 3)) (4.64.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 18.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0->-r requirements.txt (line 3)) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0->-r requirements.txt (line 3)) (3.7.1)\n","Collecting tokenizers==0.8.0-rc4\n","  Downloading tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 37.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0->-r requirements.txt (line 3)) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0->-r requirements.txt (line 3)) (2.23.0)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 55.9 MB/s \n","\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl==0.5.3->-r requirements.txt (line 5)) (2.6.3)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl==0.5.3->-r requirements.txt (line 5)) (1.7.3)\n","Collecting antlr4-python3-runtime==4.9.*\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[K     |████████████████████████████████| 117 kB 47.1 MB/s \n","\u001b[?25hCollecting PyYAML>=5.1.0\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 55.7 MB/s \n","\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0->-r requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0->-r requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0->-r requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0->-r requirements.txt (line 3)) (2022.6.15)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0->-r requirements.txt (line 3)) (3.0.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0->-r requirements.txt (line 3)) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0->-r requirements.txt (line 3)) (1.1.0)\n","Building wheels for collected packages: torch-scatter, hydra, antlr4-python3-runtime, sacremoses\n","  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-scatter: filename=torch_scatter-2.0.4-cp37-cp37m-linux_x86_64.whl size=14800897 sha256=0badf728738464d99b2990aadd5da3ddf7b2e558a3a4e5565f6a1afa72c3876e\n","  Stored in directory: /root/.cache/pip/wheels/4a/3d/01/e408ecc11389070cbacee91b70c978bf3557130e9aaa18a95b\n","  Building wheel for hydra (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hydra: filename=Hydra-2.4-cp37-cp37m-linux_x86_64.whl size=219229 sha256=6ba3bb70ab7a0894fd85db5fa662268b1a9f65909c6f62a8723d5245f30b1aea\n","  Stored in directory: /root/.cache/pip/wheels/f5/65/98/3603b340344bd22dfe1c809365013fc4d425d83d89c3e2cd17\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=82561a285b9f49f1cb4bdaece3e958aedc9110bf80911f00a58e524ac4351dab\n","  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=655e453e0a3a02c13f6072f45fe205769b2799c5172e49650f1d57fe21ad9326\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built torch-scatter hydra antlr4-python3-runtime sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, PyYAML, antlr4-python3-runtime, transformers, torch-scatter, torch, omegaconf, hydra, dgl\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.0+cu113\n","    Uninstalling torch-1.12.0+cu113:\n","      Successfully uninstalled torch-1.12.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.8.0 which is incompatible.\n","torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.8.0 which is incompatible.\n","torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n","Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.9.3 dgl-0.5.3 hydra-2.4 omegaconf-2.2.2 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.8.0rc4 torch-1.8.0 torch-scatter-2.0.4 transformers-3.0.0\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"G8LmQt_jcvSZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659164859392,"user_tz":-540,"elapsed":143562,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"6ba98262-f495-4e1a-853e-4bbc2d611e6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.9.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n","\u001b[K     |█████████████                   | 834.1 MB 54.0 MB/s eta 0:00:23tcmalloc: large alloc 1147494400 bytes == 0x3a40a000 @  0x7f1eae124615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████▌               | 1055.7 MB 1.3 MB/s eta 0:13:06tcmalloc: large alloc 1434370048 bytes == 0x7ea60000 @  0x7f1eae124615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |█████████████████████           | 1336.2 MB 60.3 MB/s eta 0:00:12tcmalloc: large alloc 1792966656 bytes == 0x3892000 @  0x7f1eae124615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |██████████████████████████▌     | 1691.1 MB 1.3 MB/s eta 0:04:29tcmalloc: large alloc 2241208320 bytes == 0x6e67a000 @  0x7f1eae124615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████████████████████| 2041.3 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 2041348096 bytes == 0xf3fdc000 @  0x7f1eae1231e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n","tcmalloc: large alloc 2551685120 bytes == 0x1e1f64000 @  0x7f1eae124615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n","\u001b[K     |████████████████████████████████| 2041.3 MB 7.3 kB/s \n","\u001b[?25hCollecting torchvision==0.10.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)\n","\u001b[K     |████████████████████████████████| 23.2 MB 1.3 MB/s \n","\u001b[?25hCollecting torchaudio===0.9.0\n","  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu111) (4.1.1)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (1.21.6)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.8.0\n","    Uninstalling torch-1.8.0:\n","      Successfully uninstalled torch-1.8.0\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.13.0+cu113\n","    Uninstalling torchvision-0.13.0+cu113:\n","      Successfully uninstalled torchvision-0.13.0+cu113\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 0.12.0+cu113\n","    Uninstalling torchaudio-0.12.0+cu113:\n","      Successfully uninstalled torchaudio-0.12.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\n","Successfully installed torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111\n"]}],"source":["!python -m pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","source":["!python -m pip uninstall torch-scatter -y"],"metadata":{"id":"JJqgY88ou7Qp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659164860210,"user_tz":-540,"elapsed":830,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"eb3accd6-7336-44db-87b1-e8a3536e042f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch-scatter 2.0.4\n","Uninstalling torch-scatter-2.0.4:\n","  Successfully uninstalled torch-scatter-2.0.4\n"]}]},{"cell_type":"code","source":["!python -m pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.1+cu111.html"],"metadata":{"id":"7c7V5B7yc9Cc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659164866054,"user_tz":-540,"elapsed":5852,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"024f8aaf-87dc-46dc-de60-6d5602f249f2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.9.1+cu111.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.9.0%2Bcu111/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (10.4 MB)\n","\u001b[K     |████████████████████████████████| 10.4 MB 2.8 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n"]}]},{"cell_type":"code","source":["!python -c \"import torch_scatter; print(torch_scatter.__version__)\""],"metadata":{"id":"l2wm30u-c-i0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659164867476,"user_tz":-540,"elapsed":1433,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"94113b42-fea8-4469-ea2c-894e03636871"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.9\n"]}]},{"cell_type":"code","source":["!pip install hydra-core --upgrade"],"metadata":{"id":"GgPMOQSL4HVZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659164871947,"user_tz":-540,"elapsed":4477,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"7a0d0dd9-5233-4154-f6d8-ff3765832ace"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting hydra-core\n","  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n","\u001b[K     |████████████████████████████████| 151 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core) (5.9.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.7/dist-packages (from hydra-core) (4.9.3)\n","Requirement already satisfied: omegaconf~=2.2 in /usr/local/lib/python3.7/dist-packages (from hydra-core) (2.2.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from hydra-core) (21.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf~=2.2->hydra-core) (6.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->hydra-core) (3.0.9)\n","Installing collected packages: hydra-core\n","Successfully installed hydra-core-1.2.0\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp/data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FT5vluqcNpey","executionInfo":{"status":"ok","timestamp":1658391076700,"user_tz":-540,"elapsed":430,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"509c51db-1dbe-4933-9f2e-c92aba3546bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp/data\n"]}]},{"cell_type":"code","source":["!python preprocess.py --dataset FB15kET"],"metadata":{"id":"PZROa2w4NUfm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python preprocess.py --dataset YAGO43kET"],"metadata":{"id":"mDB7erFzN4St"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd /content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp"],"metadata":{"id":"hsnRA4_pUSzk","executionInfo":{"status":"ok","timestamp":1659180200751,"user_tz":-540,"elapsed":7,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"256844ff-7ad2-4671-f390-de6412dec335","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp\n"]}]},{"cell_type":"code","source":["!HYDRA_FULL_ERROR=1 python run.py hydra.job.chdir=False hydra.verbose=True"],"metadata":{"id":"586hjT34u-VB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c3cda84c-0769-4f76-ce01-2bb6cb3d4fb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using backend: pytorch\n","run.py:18: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"config\", config_name='config')\n","/usr/local/lib/python3.7/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n","  warnings.warn(msg, UserWarning)\n","[2022-07-30 10:01:56,537][HYDRA] Hydra 1.2.0\n","[2022-07-30 10:01:56,538][HYDRA] ===========\n","[2022-07-30 10:01:56,538][HYDRA] Installed Hydra Plugins\n","[2022-07-30 10:01:56,538][HYDRA] ***********************\n","[2022-07-30 10:01:56,538][HYDRA] \tConfigSource:\n","[2022-07-30 10:01:56,538][HYDRA] \t-------------\n","[2022-07-30 10:01:56,538][HYDRA] \t\tFileConfigSource\n","[2022-07-30 10:01:56,538][HYDRA] \t\tImportlibResourcesConfigSource\n","[2022-07-30 10:01:56,538][HYDRA] \t\tStructuredConfigSource\n","[2022-07-30 10:01:56,538][HYDRA] \tCompletionPlugin:\n","[2022-07-30 10:01:56,538][HYDRA] \t-----------------\n","[2022-07-30 10:01:56,538][HYDRA] \t\tBashCompletion\n","[2022-07-30 10:01:56,538][HYDRA] \t\tFishCompletion\n","[2022-07-30 10:01:56,538][HYDRA] \t\tZshCompletion\n","[2022-07-30 10:01:56,539][HYDRA] \tLauncher:\n","[2022-07-30 10:01:56,539][HYDRA] \t---------\n","[2022-07-30 10:01:56,539][HYDRA] \t\tBasicLauncher\n","[2022-07-30 10:01:56,539][HYDRA] \tSweeper:\n","[2022-07-30 10:01:56,539][HYDRA] \t--------\n","[2022-07-30 10:01:56,539][HYDRA] \t\tBasicSweeper\n","[2022-07-30 10:01:56,539][HYDRA] \n","[2022-07-30 10:01:56,539][HYDRA] Config search path\n","[2022-07-30 10:01:56,539][HYDRA] ******************\n","[2022-07-30 10:01:56,703][HYDRA] | Provider | Search path                                                           |\n","[2022-07-30 10:01:56,704][HYDRA] ------------------------------------------------------------------------------------\n","[2022-07-30 10:01:56,704][HYDRA] | hydra    | pkg://hydra.conf                                                      |\n","[2022-07-30 10:01:56,704][HYDRA] | main     | file:///content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp/config |\n","[2022-07-30 10:01:56,704][HYDRA] | schema   | structured://                                                         |\n","[2022-07-30 10:01:56,704][HYDRA] ------------------------------------------------------------------------------------\n","[2022-07-30 10:01:56,792][HYDRA] \n","[2022-07-30 10:01:56,792][HYDRA] Defaults Tree\n","[2022-07-30 10:01:56,792][HYDRA] *************\n","[2022-07-30 10:01:56,792][HYDRA] <root>:\n","[2022-07-30 10:01:56,792][HYDRA]   hydra/config:\n","[2022-07-30 10:01:56,792][HYDRA]     hydra/output: default\n","[2022-07-30 10:01:56,792][HYDRA]     hydra/launcher: basic\n","[2022-07-30 10:01:56,793][HYDRA]     hydra/sweeper: basic\n","[2022-07-30 10:01:56,793][HYDRA]     hydra/help: default\n","[2022-07-30 10:01:56,793][HYDRA]     hydra/hydra_help: default\n","[2022-07-30 10:01:56,793][HYDRA]     hydra/hydra_logging: default\n","[2022-07-30 10:01:56,793][HYDRA]     hydra/job_logging: default\n","[2022-07-30 10:01:56,793][HYDRA]     hydra/callbacks: null\n","[2022-07-30 10:01:56,793][HYDRA]     hydra/env: default\n","[2022-07-30 10:01:56,793][HYDRA]     _self_\n","[2022-07-30 10:01:56,793][HYDRA]   config:\n","[2022-07-30 10:01:56,793][HYDRA]     data/FB15kET\n","[2022-07-30 10:01:56,793][HYDRA]     model/T5\n","[2022-07-30 10:01:56,793][HYDRA]     _self_\n","[2022-07-30 10:01:56,880][HYDRA] \n","[2022-07-30 10:01:56,880][HYDRA] Defaults List\n","[2022-07-30 10:01:56,880][HYDRA] *************\n","[2022-07-30 10:01:56,880][HYDRA] | Config path                 | Package             | _self_ | Parent       | \n","[2022-07-30 10:01:56,880][HYDRA] ------------------------------------------------------------------------------\n","[2022-07-30 10:01:56,880][HYDRA] | hydra/output/default        | hydra               | False  | hydra/config |\n","[2022-07-30 10:01:56,880][HYDRA] | hydra/launcher/basic        | hydra.launcher      | False  | hydra/config |\n","[2022-07-30 10:01:56,880][HYDRA] | hydra/sweeper/basic         | hydra.sweeper       | False  | hydra/config |\n","[2022-07-30 10:01:56,880][HYDRA] | hydra/help/default          | hydra.help          | False  | hydra/config |\n","[2022-07-30 10:01:56,881][HYDRA] | hydra/hydra_help/default    | hydra.hydra_help    | False  | hydra/config |\n","[2022-07-30 10:01:56,881][HYDRA] | hydra/hydra_logging/default | hydra.hydra_logging | False  | hydra/config |\n","[2022-07-30 10:01:56,881][HYDRA] | hydra/job_logging/default   | hydra.job_logging   | False  | hydra/config |\n","[2022-07-30 10:01:56,881][HYDRA] | hydra/env/default           | hydra.env           | False  | hydra/config |\n","[2022-07-30 10:01:56,881][HYDRA] | hydra/config                | hydra               | True   | <root>       |\n","[2022-07-30 10:01:56,881][HYDRA] | data/FB15kET                | data                | False  | config       |\n","[2022-07-30 10:01:56,881][HYDRA] | model/T5                    | model               | False  | config       |\n","[2022-07-30 10:01:56,881][HYDRA] | config                      |                     | True   | <root>       |\n","[2022-07-30 10:01:56,881][HYDRA] ------------------------------------------------------------------------------\n","[2022-07-30 10:01:57,027][HYDRA] Config\n","[2022-07-30 10:01:57,027][HYDRA] ******\n","[2022-07-30 10:01:57,032][HYDRA] data:\n","  name: FB15kET\n","  data_dir: data\n","  overwrite: false\n","  is_unigraph: false\n","  change_mid_to_name: true\n","  lowest_level: true\n","  remove_under_score: true\n","model:\n","  cuda: true\n","  debug: false\n","  save_dir: save\n","  name: T5\n","  train_dataset: ET_1_1_train.txt\n","  pretrained_model: t5-base\n","  append_another_bos: false\n","  append_sep_token: true\n","  is_in_edge: true\n","  is_out_edge: true\n","  max_epoch: 100\n","  train_batch_size: 8\n","  valid_batch_size: 8\n","  test_batch_size: 8\n","  num_workers: 0\n","  temperature: 0.5\n","  repetition_penalty: 1.5\n","  n_gpus: 1\n","  max_input_length: 256\n","  max_output_length: 128\n","  gradient_accumulation_steps: 1\n","  max_grad_norm: 1.0\n","  early_stopping: false\n","  optimizer:\n","    algorithm: Adam\n","    learning_rate: 0.0001\n","  valid:\n","    valid_dataset: ET_1_1_valid.txt\n","    valid_epoch: 1\n","    num_beams: 5\n","    length_penalty: 1.0\n","    max_output_length: 128\n","    clean_up_spaces: true\n","    save_outputs: true\n","    save_one_batch: true\n","  test:\n","    test_dataset: ET_1_1_test.txt\n","    save_outputs: true\n","preprocess:\n","  load_ET: false\n","  load_KG: true\n","  neighbor_sampling: true\n","  neighbor_num: 10\n","  num_layers: 1\n","\n","[2022-07-30 10:01:57,119][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): s3.amazonaws.com:443\n","[2022-07-30 10:01:57,248][urllib3.connectionpool][DEBUG] - https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/t5-spiece.model HTTP/1.1\" 200 0\n","[2022-07-30 10:01:57,250][transformers.tokenization_utils_base][INFO] - loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n","[2022-07-30 10:02:14,903][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): s3.amazonaws.com:443\n","[2022-07-30 10:02:15,035][urllib3.connectionpool][DEBUG] - https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/t5-base-config.json HTTP/1.1\" 200 0\n","[2022-07-30 10:02:15,037][filelock][DEBUG] - Attempting to acquire lock 140635474266192 on /root/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b.lock\n","[2022-07-30 10:02:15,037][filelock][DEBUG] - Lock 140635474266192 acquired on /root/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b.lock\n","[2022-07-30 10:02:15,038][transformers.file_utils][INFO] - https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp00k7pj77\n","[2022-07-30 10:02:15,040][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): s3.amazonaws.com:443\n","[2022-07-30 10:02:15,165][urllib3.connectionpool][DEBUG] - https://s3.amazonaws.com:443 \"GET /models.huggingface.co/bert/t5-base-config.json HTTP/1.1\" 200 1199\n","Downloading: 100% 1.20k/1.20k [00:00<00:00, 884kB/s]\n","[2022-07-30 10:02:15,168][transformers.file_utils][INFO] - storing https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json in cache at /root/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b\n","[2022-07-30 10:02:15,168][transformers.file_utils][INFO] - creating metadata file for /root/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b\n","[2022-07-30 10:02:15,169][filelock][DEBUG] - Attempting to release lock 140635474266192 on /root/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b.lock\n","[2022-07-30 10:02:15,169][filelock][DEBUG] - Lock 140635474266192 released on /root/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b.lock\n","[2022-07-30 10:02:15,170][transformers.configuration_utils][INFO] - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json from cache at /root/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b\n","[2022-07-30 10:02:15,170][transformers.configuration_utils][INFO] - Model config T5Config {\n","  \"architectures\": [\n","    \"T5WithLMHeadModel\"\n","  ],\n","  \"d_ff\": 3072,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"vocab_size\": 32128\n","}\n","\n","[2022-07-30 10:02:15,172][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): cdn.huggingface.co:443\n","[2022-07-30 10:02:15,418][urllib3.connectionpool][DEBUG] - https://cdn.huggingface.co:443 \"HEAD /t5-base-pytorch_model.bin HTTP/1.1\" 200 0\n","[2022-07-30 10:02:15,421][filelock][DEBUG] - Attempting to acquire lock 140636722961488 on /root/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa.lock\n","[2022-07-30 10:02:15,421][filelock][DEBUG] - Lock 140636722961488 acquired on /root/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa.lock\n","[2022-07-30 10:02:15,421][transformers.file_utils][INFO] - https://cdn.huggingface.co/t5-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpu80puw82\n","[2022-07-30 10:02:15,423][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): cdn.huggingface.co:443\n","[2022-07-30 10:02:15,669][urllib3.connectionpool][DEBUG] - https://cdn.huggingface.co:443 \"GET /t5-base-pytorch_model.bin HTTP/1.1\" 200 891691430\n","Downloading: 100% 892M/892M [00:18<00:00, 47.8MB/s]\n","[2022-07-30 10:02:34,337][transformers.file_utils][INFO] - storing https://cdn.huggingface.co/t5-base-pytorch_model.bin in cache at /root/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa\n","[2022-07-30 10:02:34,338][transformers.file_utils][INFO] - creating metadata file for /root/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa\n","[2022-07-30 10:02:34,339][filelock][DEBUG] - Attempting to release lock 140636722961488 on /root/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa.lock\n","[2022-07-30 10:02:34,339][filelock][DEBUG] - Lock 140636722961488 released on /root/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa.lock\n","[2022-07-30 10:02:34,339][transformers.modeling_utils][INFO] - loading weights file https://cdn.huggingface.co/t5-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa\n","[2022-07-30 10:02:41,128][transformers.modeling_utils][INFO] - All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n","\n","[2022-07-30 10:02:41,129][transformers.modeling_utils][WARNING] - Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[2022-07-30 10:02:44,991][__main__][DEBUG] - Parameter model.shared.weight: torch.Size([32128, 768]), require_grad=True\n","[2022-07-30 10:02:44,991][__main__][DEBUG] - Parameter model.encoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,992][__main__][DEBUG] - Parameter model.encoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,992][__main__][DEBUG] - Parameter model.encoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,992][__main__][DEBUG] - Parameter model.encoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,992][__main__][DEBUG] - Parameter model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 12]), require_grad=True\n","[2022-07-30 10:02:44,992][__main__][DEBUG] - Parameter model.encoder.block.0.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:44,993][__main__][DEBUG] - Parameter model.encoder.block.0.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:44,993][__main__][DEBUG] - Parameter model.encoder.block.0.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:44,993][__main__][DEBUG] - Parameter model.encoder.block.0.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:44,993][__main__][DEBUG] - Parameter model.encoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,993][__main__][DEBUG] - Parameter model.encoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,994][__main__][DEBUG] - Parameter model.encoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,994][__main__][DEBUG] - Parameter model.encoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,994][__main__][DEBUG] - Parameter model.encoder.block.1.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:44,994][__main__][DEBUG] - Parameter model.encoder.block.1.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:44,994][__main__][DEBUG] - Parameter model.encoder.block.1.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:44,994][__main__][DEBUG] - Parameter model.encoder.block.1.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:44,995][__main__][DEBUG] - Parameter model.encoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,995][__main__][DEBUG] - Parameter model.encoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,995][__main__][DEBUG] - Parameter model.encoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,995][__main__][DEBUG] - Parameter model.encoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,995][__main__][DEBUG] - Parameter model.encoder.block.2.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:44,995][__main__][DEBUG] - Parameter model.encoder.block.2.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:44,996][__main__][DEBUG] - Parameter model.encoder.block.2.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:44,996][__main__][DEBUG] - Parameter model.encoder.block.2.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:44,996][__main__][DEBUG] - Parameter model.encoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,996][__main__][DEBUG] - Parameter model.encoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,996][__main__][DEBUG] - Parameter model.encoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,997][__main__][DEBUG] - Parameter model.encoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,997][__main__][DEBUG] - Parameter model.encoder.block.3.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:44,997][__main__][DEBUG] - Parameter model.encoder.block.3.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:44,997][__main__][DEBUG] - Parameter model.encoder.block.3.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:44,997][__main__][DEBUG] - Parameter model.encoder.block.3.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:44,997][__main__][DEBUG] - Parameter model.encoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,998][__main__][DEBUG] - Parameter model.encoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,998][__main__][DEBUG] - Parameter model.encoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,998][__main__][DEBUG] - Parameter model.encoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,998][__main__][DEBUG] - Parameter model.encoder.block.4.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:44,998][__main__][DEBUG] - Parameter model.encoder.block.4.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:44,999][__main__][DEBUG] - Parameter model.encoder.block.4.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:44,999][__main__][DEBUG] - Parameter model.encoder.block.4.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:44,999][__main__][DEBUG] - Parameter model.encoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,999][__main__][DEBUG] - Parameter model.encoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:44,999][__main__][DEBUG] - Parameter model.encoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,000][__main__][DEBUG] - Parameter model.encoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,000][__main__][DEBUG] - Parameter model.encoder.block.5.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,000][__main__][DEBUG] - Parameter model.encoder.block.5.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,000][__main__][DEBUG] - Parameter model.encoder.block.5.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,000][__main__][DEBUG] - Parameter model.encoder.block.5.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,000][__main__][DEBUG] - Parameter model.encoder.block.6.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,001][__main__][DEBUG] - Parameter model.encoder.block.6.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,001][__main__][DEBUG] - Parameter model.encoder.block.6.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,001][__main__][DEBUG] - Parameter model.encoder.block.6.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,001][__main__][DEBUG] - Parameter model.encoder.block.6.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,001][__main__][DEBUG] - Parameter model.encoder.block.6.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,002][__main__][DEBUG] - Parameter model.encoder.block.6.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,002][__main__][DEBUG] - Parameter model.encoder.block.6.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,002][__main__][DEBUG] - Parameter model.encoder.block.7.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,002][__main__][DEBUG] - Parameter model.encoder.block.7.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,002][__main__][DEBUG] - Parameter model.encoder.block.7.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,002][__main__][DEBUG] - Parameter model.encoder.block.7.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,003][__main__][DEBUG] - Parameter model.encoder.block.7.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,003][__main__][DEBUG] - Parameter model.encoder.block.7.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,003][__main__][DEBUG] - Parameter model.encoder.block.7.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,003][__main__][DEBUG] - Parameter model.encoder.block.7.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,003][__main__][DEBUG] - Parameter model.encoder.block.8.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,004][__main__][DEBUG] - Parameter model.encoder.block.8.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,004][__main__][DEBUG] - Parameter model.encoder.block.8.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,004][__main__][DEBUG] - Parameter model.encoder.block.8.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,004][__main__][DEBUG] - Parameter model.encoder.block.8.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,004][__main__][DEBUG] - Parameter model.encoder.block.8.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,004][__main__][DEBUG] - Parameter model.encoder.block.8.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,005][__main__][DEBUG] - Parameter model.encoder.block.8.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,005][__main__][DEBUG] - Parameter model.encoder.block.9.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,005][__main__][DEBUG] - Parameter model.encoder.block.9.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,005][__main__][DEBUG] - Parameter model.encoder.block.9.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,005][__main__][DEBUG] - Parameter model.encoder.block.9.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,005][__main__][DEBUG] - Parameter model.encoder.block.9.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,006][__main__][DEBUG] - Parameter model.encoder.block.9.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,006][__main__][DEBUG] - Parameter model.encoder.block.9.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,006][__main__][DEBUG] - Parameter model.encoder.block.9.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,006][__main__][DEBUG] - Parameter model.encoder.block.10.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,006][__main__][DEBUG] - Parameter model.encoder.block.10.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,006][__main__][DEBUG] - Parameter model.encoder.block.10.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,006][__main__][DEBUG] - Parameter model.encoder.block.10.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,007][__main__][DEBUG] - Parameter model.encoder.block.10.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,007][__main__][DEBUG] - Parameter model.encoder.block.10.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,007][__main__][DEBUG] - Parameter model.encoder.block.10.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,007][__main__][DEBUG] - Parameter model.encoder.block.10.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,007][__main__][DEBUG] - Parameter model.encoder.block.11.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,007][__main__][DEBUG] - Parameter model.encoder.block.11.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,007][__main__][DEBUG] - Parameter model.encoder.block.11.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,008][__main__][DEBUG] - Parameter model.encoder.block.11.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,008][__main__][DEBUG] - Parameter model.encoder.block.11.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,008][__main__][DEBUG] - Parameter model.encoder.block.11.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,008][__main__][DEBUG] - Parameter model.encoder.block.11.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,008][__main__][DEBUG] - Parameter model.encoder.block.11.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,008][__main__][DEBUG] - Parameter model.encoder.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,009][__main__][DEBUG] - Parameter model.decoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,009][__main__][DEBUG] - Parameter model.decoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,009][__main__][DEBUG] - Parameter model.decoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,009][__main__][DEBUG] - Parameter model.decoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,009][__main__][DEBUG] - Parameter model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 12]), require_grad=True\n","[2022-07-30 10:02:45,009][__main__][DEBUG] - Parameter model.decoder.block.0.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,010][__main__][DEBUG] - Parameter model.decoder.block.0.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,010][__main__][DEBUG] - Parameter model.decoder.block.0.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,010][__main__][DEBUG] - Parameter model.decoder.block.0.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,010][__main__][DEBUG] - Parameter model.decoder.block.0.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,010][__main__][DEBUG] - Parameter model.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight: torch.Size([32, 12]), require_grad=True\n","[2022-07-30 10:02:45,010][__main__][DEBUG] - Parameter model.decoder.block.0.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,011][__main__][DEBUG] - Parameter model.decoder.block.0.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,011][__main__][DEBUG] - Parameter model.decoder.block.0.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,011][__main__][DEBUG] - Parameter model.decoder.block.0.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,011][__main__][DEBUG] - Parameter model.decoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,011][__main__][DEBUG] - Parameter model.decoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,011][__main__][DEBUG] - Parameter model.decoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,011][__main__][DEBUG] - Parameter model.decoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,012][__main__][DEBUG] - Parameter model.decoder.block.1.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,012][__main__][DEBUG] - Parameter model.decoder.block.1.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,053][__main__][DEBUG] - Parameter model.decoder.block.1.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,053][__main__][DEBUG] - Parameter model.decoder.block.1.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,053][__main__][DEBUG] - Parameter model.decoder.block.1.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,054][__main__][DEBUG] - Parameter model.decoder.block.1.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,054][__main__][DEBUG] - Parameter model.decoder.block.1.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,054][__main__][DEBUG] - Parameter model.decoder.block.1.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,055][__main__][DEBUG] - Parameter model.decoder.block.1.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,055][__main__][DEBUG] - Parameter model.decoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,056][__main__][DEBUG] - Parameter model.decoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,056][__main__][DEBUG] - Parameter model.decoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,056][__main__][DEBUG] - Parameter model.decoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,056][__main__][DEBUG] - Parameter model.decoder.block.2.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,057][__main__][DEBUG] - Parameter model.decoder.block.2.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,057][__main__][DEBUG] - Parameter model.decoder.block.2.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,057][__main__][DEBUG] - Parameter model.decoder.block.2.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,058][__main__][DEBUG] - Parameter model.decoder.block.2.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,058][__main__][DEBUG] - Parameter model.decoder.block.2.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,058][__main__][DEBUG] - Parameter model.decoder.block.2.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,058][__main__][DEBUG] - Parameter model.decoder.block.2.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,059][__main__][DEBUG] - Parameter model.decoder.block.2.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,059][__main__][DEBUG] - Parameter model.decoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,059][__main__][DEBUG] - Parameter model.decoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,060][__main__][DEBUG] - Parameter model.decoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,060][__main__][DEBUG] - Parameter model.decoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,060][__main__][DEBUG] - Parameter model.decoder.block.3.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,060][__main__][DEBUG] - Parameter model.decoder.block.3.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,061][__main__][DEBUG] - Parameter model.decoder.block.3.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,061][__main__][DEBUG] - Parameter model.decoder.block.3.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,061][__main__][DEBUG] - Parameter model.decoder.block.3.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,061][__main__][DEBUG] - Parameter model.decoder.block.3.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,062][__main__][DEBUG] - Parameter model.decoder.block.3.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,062][__main__][DEBUG] - Parameter model.decoder.block.3.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,062][__main__][DEBUG] - Parameter model.decoder.block.3.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,062][__main__][DEBUG] - Parameter model.decoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,063][__main__][DEBUG] - Parameter model.decoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,063][__main__][DEBUG] - Parameter model.decoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,063][__main__][DEBUG] - Parameter model.decoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,064][__main__][DEBUG] - Parameter model.decoder.block.4.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,064][__main__][DEBUG] - Parameter model.decoder.block.4.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,064][__main__][DEBUG] - Parameter model.decoder.block.4.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,064][__main__][DEBUG] - Parameter model.decoder.block.4.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,065][__main__][DEBUG] - Parameter model.decoder.block.4.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,065][__main__][DEBUG] - Parameter model.decoder.block.4.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,065][__main__][DEBUG] - Parameter model.decoder.block.4.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,065][__main__][DEBUG] - Parameter model.decoder.block.4.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,066][__main__][DEBUG] - Parameter model.decoder.block.4.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,066][__main__][DEBUG] - Parameter model.decoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,066][__main__][DEBUG] - Parameter model.decoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,067][__main__][DEBUG] - Parameter model.decoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,067][__main__][DEBUG] - Parameter model.decoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,067][__main__][DEBUG] - Parameter model.decoder.block.5.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,067][__main__][DEBUG] - Parameter model.decoder.block.5.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,067][__main__][DEBUG] - Parameter model.decoder.block.5.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,068][__main__][DEBUG] - Parameter model.decoder.block.5.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,068][__main__][DEBUG] - Parameter model.decoder.block.5.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,068][__main__][DEBUG] - Parameter model.decoder.block.5.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,068][__main__][DEBUG] - Parameter model.decoder.block.5.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,068][__main__][DEBUG] - Parameter model.decoder.block.5.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,068][__main__][DEBUG] - Parameter model.decoder.block.5.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,069][__main__][DEBUG] - Parameter model.decoder.block.6.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,069][__main__][DEBUG] - Parameter model.decoder.block.6.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,069][__main__][DEBUG] - Parameter model.decoder.block.6.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,069][__main__][DEBUG] - Parameter model.decoder.block.6.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,069][__main__][DEBUG] - Parameter model.decoder.block.6.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,069][__main__][DEBUG] - Parameter model.decoder.block.6.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,069][__main__][DEBUG] - Parameter model.decoder.block.6.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,070][__main__][DEBUG] - Parameter model.decoder.block.6.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,070][__main__][DEBUG] - Parameter model.decoder.block.6.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,070][__main__][DEBUG] - Parameter model.decoder.block.6.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,070][__main__][DEBUG] - Parameter model.decoder.block.6.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,070][__main__][DEBUG] - Parameter model.decoder.block.6.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,070][__main__][DEBUG] - Parameter model.decoder.block.6.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,071][__main__][DEBUG] - Parameter model.decoder.block.7.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,071][__main__][DEBUG] - Parameter model.decoder.block.7.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,071][__main__][DEBUG] - Parameter model.decoder.block.7.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,071][__main__][DEBUG] - Parameter model.decoder.block.7.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,071][__main__][DEBUG] - Parameter model.decoder.block.7.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,071][__main__][DEBUG] - Parameter model.decoder.block.7.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,072][__main__][DEBUG] - Parameter model.decoder.block.7.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,072][__main__][DEBUG] - Parameter model.decoder.block.7.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,072][__main__][DEBUG] - Parameter model.decoder.block.7.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,072][__main__][DEBUG] - Parameter model.decoder.block.7.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,072][__main__][DEBUG] - Parameter model.decoder.block.7.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,072][__main__][DEBUG] - Parameter model.decoder.block.7.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,072][__main__][DEBUG] - Parameter model.decoder.block.7.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,073][__main__][DEBUG] - Parameter model.decoder.block.8.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,073][__main__][DEBUG] - Parameter model.decoder.block.8.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,073][__main__][DEBUG] - Parameter model.decoder.block.8.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,073][__main__][DEBUG] - Parameter model.decoder.block.8.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,073][__main__][DEBUG] - Parameter model.decoder.block.8.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,073][__main__][DEBUG] - Parameter model.decoder.block.8.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,074][__main__][DEBUG] - Parameter model.decoder.block.8.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,074][__main__][DEBUG] - Parameter model.decoder.block.8.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,074][__main__][DEBUG] - Parameter model.decoder.block.8.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,074][__main__][DEBUG] - Parameter model.decoder.block.8.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,074][__main__][DEBUG] - Parameter model.decoder.block.8.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,074][__main__][DEBUG] - Parameter model.decoder.block.8.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,074][__main__][DEBUG] - Parameter model.decoder.block.8.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,075][__main__][DEBUG] - Parameter model.decoder.block.9.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,075][__main__][DEBUG] - Parameter model.decoder.block.9.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,075][__main__][DEBUG] - Parameter model.decoder.block.9.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,075][__main__][DEBUG] - Parameter model.decoder.block.9.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,075][__main__][DEBUG] - Parameter model.decoder.block.9.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,075][__main__][DEBUG] - Parameter model.decoder.block.9.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,075][__main__][DEBUG] - Parameter model.decoder.block.9.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,076][__main__][DEBUG] - Parameter model.decoder.block.9.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,076][__main__][DEBUG] - Parameter model.decoder.block.9.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,076][__main__][DEBUG] - Parameter model.decoder.block.9.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,076][__main__][DEBUG] - Parameter model.decoder.block.9.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,076][__main__][DEBUG] - Parameter model.decoder.block.9.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,076][__main__][DEBUG] - Parameter model.decoder.block.9.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,077][__main__][DEBUG] - Parameter model.decoder.block.10.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,077][__main__][DEBUG] - Parameter model.decoder.block.10.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,077][__main__][DEBUG] - Parameter model.decoder.block.10.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,077][__main__][DEBUG] - Parameter model.decoder.block.10.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,077][__main__][DEBUG] - Parameter model.decoder.block.10.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,077][__main__][DEBUG] - Parameter model.decoder.block.10.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,077][__main__][DEBUG] - Parameter model.decoder.block.10.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,078][__main__][DEBUG] - Parameter model.decoder.block.10.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,078][__main__][DEBUG] - Parameter model.decoder.block.10.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,078][__main__][DEBUG] - Parameter model.decoder.block.10.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,078][__main__][DEBUG] - Parameter model.decoder.block.10.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,078][__main__][DEBUG] - Parameter model.decoder.block.10.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,078][__main__][DEBUG] - Parameter model.decoder.block.10.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,078][__main__][DEBUG] - Parameter model.decoder.block.11.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,079][__main__][DEBUG] - Parameter model.decoder.block.11.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,079][__main__][DEBUG] - Parameter model.decoder.block.11.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,079][__main__][DEBUG] - Parameter model.decoder.block.11.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,079][__main__][DEBUG] - Parameter model.decoder.block.11.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,079][__main__][DEBUG] - Parameter model.decoder.block.11.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,079][__main__][DEBUG] - Parameter model.decoder.block.11.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,080][__main__][DEBUG] - Parameter model.decoder.block.11.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,080][__main__][DEBUG] - Parameter model.decoder.block.11.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 10:02:45,080][__main__][DEBUG] - Parameter model.decoder.block.11.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,080][__main__][DEBUG] - Parameter model.decoder.block.11.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 10:02:45,080][__main__][DEBUG] - Parameter model.decoder.block.11.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 10:02:45,080][__main__][DEBUG] - Parameter model.decoder.block.11.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 10:02:45,081][__main__][DEBUG] - Parameter model.decoder.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","Epoch:   0% 0/100 [00:00<?, ?it/s][2022-07-30 10:02:45,084][__main__][DEBUG] - Starting training!\n","\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A[2022-07-30 10:02:45,450][__main__][DEBUG] - batch 0: tensor([[ 784, 9413,  908,  ..., 6327,  908,    1],\n","        [ 784, 9413,  908,  ..., 6327,  908,    1],\n","        [ 784, 9413,  908,  ..., 6327,  908,    1],\n","        ...,\n","        [ 784, 9413,  908,  ..., 6327,  908,    1],\n","        [ 784, 9413,  908,  ..., 6327,  908,    1],\n","        [ 784, 9413,  908,  ..., 6327,  908,    1]], device='cuda:0') \n","[2022-07-30 10:02:45,452][__main__][DEBUG] - batch 1: tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0') \n","[2022-07-30 10:02:45,454][__main__][DEBUG] - batch 2: tensor([[ 2859,   784,     7,  ...,     0,     0,     0],\n","        [ 2760,  3295,   784,  ...,     0,     0,     0],\n","        [  814,   784,     7,  ...,     0,     0,     0],\n","        ...,\n","        [ 2100,  5533,  6488,  ...,     0,     0,     0],\n","        [    3, 18984,   643,  ...,     0,     0,     0],\n","        [ 2859,   784,     7,  ...,     0,     0,     0]], device='cuda:0') \n","[2022-07-30 10:02:45,456][__main__][DEBUG] - batch 3: tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0') \n","\n","Iteration:   1% 1/106 [00:00<01:28,  1.19it/s]\u001b[A\n","Iteration:   2% 2/106 [00:01<01:35,  1.09it/s]\u001b[A\n","Iteration:   3% 3/106 [00:02<01:29,  1.15it/s]\u001b[A\n","Iteration:   4% 4/106 [00:03<01:31,  1.12it/s]\u001b[A\n","Iteration:   5% 5/106 [00:04<01:32,  1.09it/s]\u001b[A\n","Iteration:   6% 6/106 [00:06<01:54,  1.14s/it]\u001b[A\n","Iteration:   7% 7/106 [00:06<01:44,  1.06s/it]\u001b[A\n","Iteration:   8% 8/106 [00:08<01:42,  1.05s/it]\u001b[A\n","Iteration:   8% 9/106 [00:08<01:28,  1.10it/s]\u001b[A\n","Iteration:   9% 10/106 [00:09<01:24,  1.14it/s]\u001b[A\n","Iteration:  10% 11/106 [00:10<01:30,  1.05it/s]\u001b[A\n","Iteration:  11% 12/106 [00:11<01:24,  1.11it/s]\u001b[A\n","Iteration:  12% 13/106 [00:12<01:32,  1.00it/s]\u001b[A\n","Iteration:  13% 14/106 [00:13<01:23,  1.10it/s]\u001b[A\n","Iteration:  14% 15/106 [00:13<01:14,  1.22it/s]\u001b[A\n","Iteration:  15% 16/106 [00:14<01:15,  1.19it/s]\u001b[A\n","Iteration:  16% 17/106 [00:15<01:15,  1.18it/s]\u001b[A\n","Iteration:  17% 18/106 [00:16<01:13,  1.20it/s]\u001b[A\n","Iteration:  18% 19/106 [00:17<01:22,  1.05it/s]\u001b[A\n","Iteration:  19% 20/106 [00:18<01:13,  1.17it/s]\u001b[A\n","Iteration:  20% 21/106 [00:19<01:10,  1.21it/s]\u001b[A\n","Iteration:  21% 22/106 [00:19<01:09,  1.21it/s]\u001b[A\n","Iteration:  22% 23/106 [00:20<01:12,  1.15it/s]\u001b[A\n","Iteration:  23% 24/106 [00:21<01:08,  1.19it/s]\u001b[A\n","Iteration:  24% 25/106 [00:22<01:07,  1.19it/s]\u001b[A\n","Iteration:  25% 26/106 [00:23<01:14,  1.07it/s]\u001b[A\n","Iteration:  25% 27/106 [00:25<01:31,  1.16s/it]\u001b[A\n","Iteration:  26% 28/106 [00:26<01:23,  1.07s/it]\u001b[A\n","Iteration:  27% 29/106 [00:26<01:14,  1.04it/s]\u001b[A\n","Iteration:  28% 30/106 [00:27<01:06,  1.13it/s]\u001b[A\n","Iteration:  29% 31/106 [00:28<01:08,  1.10it/s]\u001b[A\n","Iteration:  30% 32/106 [00:29<01:03,  1.17it/s]\u001b[A\n","Iteration:  31% 33/106 [00:30<01:04,  1.13it/s]\u001b[A\n","Iteration:  32% 34/106 [00:31<01:02,  1.16it/s]\u001b[A\n","Iteration:  33% 35/106 [00:31<00:56,  1.26it/s]\u001b[A\n","Iteration:  34% 36/106 [00:32<01:01,  1.14it/s]\u001b[A\n","Iteration:  35% 37/106 [00:33<00:58,  1.19it/s]\u001b[A\n","Iteration:  36% 38/106 [00:35<01:12,  1.07s/it]\u001b[A\n","Iteration:  37% 39/106 [00:35<01:02,  1.07it/s]\u001b[A\n","Iteration:  38% 40/106 [00:36<00:57,  1.14it/s]\u001b[A\n","Iteration:  39% 41/106 [00:37<00:51,  1.25it/s]\u001b[A\n","Iteration:  40% 42/106 [00:38<00:59,  1.08it/s]\u001b[A\n","Iteration:  41% 43/106 [00:39<00:55,  1.14it/s]\u001b[A\n","Iteration:  42% 44/106 [00:39<00:51,  1.21it/s]\u001b[A\n","Iteration:  42% 45/106 [00:40<00:46,  1.32it/s]\u001b[A\n","Iteration:  43% 46/106 [00:41<00:47,  1.25it/s]\u001b[A\n","Iteration:  44% 47/106 [00:42<00:49,  1.19it/s]\u001b[A\n","Iteration:  45% 48/106 [00:42<00:47,  1.23it/s]\u001b[A\n","Iteration:  46% 49/106 [00:43<00:43,  1.31it/s]\u001b[A\n","Iteration:  47% 50/106 [00:44<00:49,  1.14it/s]\u001b[A\n","Iteration:  48% 51/106 [00:45<00:53,  1.02it/s]\u001b[A\n","Iteration:  49% 52/106 [00:46<00:52,  1.04it/s]\u001b[A\n","Iteration:  50% 53/106 [00:47<00:51,  1.03it/s]\u001b[A\n","Iteration:  51% 54/106 [00:48<00:46,  1.11it/s]\u001b[A\n","Iteration:  52% 55/106 [00:49<00:46,  1.11it/s]\u001b[A\n","Iteration:  53% 56/106 [00:50<00:42,  1.17it/s]\u001b[A\n","Iteration:  54% 57/106 [00:51<00:48,  1.01it/s]\u001b[A\n","Iteration:  55% 58/106 [00:52<00:45,  1.05it/s]\u001b[A\n","Iteration:  56% 59/106 [00:53<00:41,  1.12it/s]\u001b[A\n","Iteration:  57% 60/106 [00:54<00:40,  1.13it/s]\u001b[A\n","Iteration:  58% 61/106 [00:54<00:40,  1.11it/s]\u001b[A\n","Iteration:  58% 62/106 [00:55<00:39,  1.10it/s]\u001b[A\n","Iteration:  59% 63/106 [00:56<00:39,  1.09it/s]\u001b[A\n","Iteration:  60% 64/106 [00:57<00:39,  1.05it/s]\u001b[A\n","Iteration:  61% 65/106 [00:58<00:36,  1.13it/s]\u001b[A\n","Iteration:  62% 66/106 [00:59<00:32,  1.24it/s]\u001b[A\n","Iteration:  63% 67/106 [01:00<00:32,  1.21it/s]\u001b[A\n","Iteration:  64% 68/106 [01:00<00:32,  1.18it/s]\u001b[A\n","Iteration:  65% 69/106 [01:01<00:29,  1.24it/s]\u001b[A\n","Iteration:  66% 70/106 [01:02<00:29,  1.24it/s]\u001b[A\n","Iteration:  67% 71/106 [01:03<00:30,  1.14it/s]\u001b[A\n","Iteration:  68% 72/106 [01:04<00:28,  1.18it/s]\u001b[A\n","Iteration:  69% 73/106 [01:05<00:28,  1.15it/s]\u001b[A\n","Iteration:  70% 74/106 [01:05<00:25,  1.23it/s]\u001b[A\n","Iteration:  71% 75/106 [01:06<00:23,  1.31it/s]\u001b[A\n","Iteration:  72% 76/106 [01:07<00:25,  1.19it/s]\u001b[A\n","Iteration:  73% 77/106 [01:08<00:24,  1.18it/s]\u001b[A\n","Iteration:  74% 78/106 [01:09<00:24,  1.14it/s]\u001b[A\n","Iteration:  75% 79/106 [01:10<00:23,  1.16it/s]\u001b[A\n","Iteration:  75% 80/106 [01:10<00:21,  1.23it/s]\u001b[A\n","Iteration:  76% 81/106 [01:11<00:21,  1.15it/s]\u001b[A\n","Iteration:  77% 82/106 [01:12<00:19,  1.23it/s]\u001b[A\n","Iteration:  78% 83/106 [01:13<00:18,  1.23it/s]\u001b[A\n","Iteration:  79% 84/106 [01:14<00:18,  1.17it/s]\u001b[A\n","Iteration:  80% 85/106 [01:15<00:19,  1.09it/s]\u001b[A\n","Iteration:  81% 86/106 [01:16<00:19,  1.01it/s]\u001b[A\n","Iteration:  82% 87/106 [01:17<00:17,  1.10it/s]\u001b[A\n","Iteration:  83% 88/106 [01:18<00:15,  1.13it/s]\u001b[A\n","Iteration:  84% 89/106 [01:18<00:14,  1.16it/s]\u001b[A\n","Iteration:  85% 90/106 [01:19<00:12,  1.28it/s]\u001b[A\n","Iteration:  86% 91/106 [01:20<00:12,  1.21it/s]\u001b[A\n","Iteration:  87% 92/106 [01:21<00:10,  1.31it/s]\u001b[A\n","Iteration:  88% 93/106 [01:22<00:11,  1.16it/s]\u001b[A\n","Iteration:  89% 94/106 [01:22<00:09,  1.23it/s]\u001b[A\n","Iteration:  90% 95/106 [01:23<00:08,  1.36it/s]\u001b[A\n","Iteration:  91% 96/106 [01:24<00:08,  1.21it/s]\u001b[A\n","Iteration:  92% 97/106 [01:25<00:08,  1.04it/s]\u001b[A\n","Iteration:  92% 98/106 [01:26<00:07,  1.04it/s]\u001b[A\n","Iteration:  93% 99/106 [01:27<00:06,  1.06it/s]\u001b[A\n","Iteration:  94% 100/106 [01:28<00:05,  1.10it/s]\u001b[A\n","Iteration:  95% 101/106 [01:29<00:04,  1.13it/s]\u001b[A\n","Iteration:  96% 102/106 [01:30<00:03,  1.16it/s]\u001b[A\n","Iteration:  97% 103/106 [01:31<00:02,  1.13it/s]\u001b[A\n","Iteration:  98% 104/106 [01:32<00:02,  1.07s/it]\u001b[A\n","Iteration:  99% 105/106 [01:33<00:00,  1.00it/s]\u001b[A\n","Iteration: 100% 106/106 [01:34<00:00,  1.13it/s]\n","[2022-07-30 10:08:33,634][__main__][DEBUG] - epoch is 0\n","[2022-07-30 10:08:33,635][__main__][DEBUG] - validation loss is tensor(0.1156, device='cuda:0')\n","Epoch:   1% 1/100 [05:51<9:39:33, 351.25s/it]\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/106 [00:01<01:46,  1.01s/it]\u001b[A\n","Iteration:   2% 2/106 [00:01<01:39,  1.05it/s]\u001b[A\n","Iteration:   3% 3/106 [00:02<01:31,  1.13it/s]\u001b[A\n","Iteration:   4% 4/106 [00:03<01:23,  1.22it/s]\u001b[A\n","Iteration:   5% 5/106 [00:04<01:24,  1.19it/s]\u001b[A\n","Iteration:   6% 6/106 [00:05<01:21,  1.22it/s]\u001b[A\n","Iteration:   7% 7/106 [00:06<01:27,  1.14it/s]\u001b[A\n","Iteration:   8% 8/106 [00:07<01:36,  1.02it/s]\u001b[A\n","Iteration:   8% 9/106 [00:08<01:31,  1.06it/s]\u001b[A\n","Iteration:   9% 10/106 [00:09<01:32,  1.04it/s]\u001b[A\n","Iteration:  10% 11/106 [00:10<01:34,  1.00it/s]\u001b[A\n","Iteration:  11% 12/106 [00:11<01:40,  1.07s/it]\u001b[A\n","Iteration:  12% 13/106 [00:12<01:43,  1.11s/it]\u001b[A\n","Iteration:  13% 14/106 [00:13<01:35,  1.04s/it]\u001b[A\n","Iteration:  14% 15/106 [00:14<01:28,  1.03it/s]\u001b[A\n","Iteration:  15% 16/106 [00:15<01:25,  1.05it/s]\u001b[A\n","Iteration:  16% 17/106 [00:16<01:24,  1.06it/s]\u001b[A\n","Iteration:  17% 18/106 [00:16<01:15,  1.16it/s]\u001b[A\n","Iteration:  18% 19/106 [00:17<01:10,  1.23it/s]\u001b[A\n","Iteration:  19% 20/106 [00:18<01:13,  1.17it/s]\u001b[A\n","Iteration:  20% 21/106 [00:19<01:17,  1.10it/s]\u001b[A\n","Iteration:  21% 22/106 [00:20<01:15,  1.12it/s]\u001b[A\n","Iteration:  22% 23/106 [00:21<01:08,  1.22it/s]\u001b[A\n","Iteration:  23% 24/106 [00:21<01:02,  1.32it/s]\u001b[A\n","Iteration:  24% 25/106 [00:22<00:59,  1.36it/s]\u001b[A\n","Iteration:  25% 26/106 [00:22<00:55,  1.43it/s]\u001b[A\n","Iteration:  25% 27/106 [00:23<00:54,  1.45it/s]\u001b[A\n","Iteration:  26% 28/106 [00:24<00:52,  1.49it/s]\u001b[A\n","Iteration:  27% 29/106 [00:24<00:50,  1.52it/s]\u001b[A\n","Iteration:  28% 30/106 [00:25<00:48,  1.57it/s]\u001b[A\n","Iteration:  29% 31/106 [00:26<00:59,  1.27it/s]\u001b[A\n","Iteration:  30% 32/106 [00:27<00:55,  1.33it/s]\u001b[A\n","Iteration:  31% 33/106 [00:28<01:01,  1.19it/s]\u001b[A\n","Iteration:  32% 34/106 [00:29<01:07,  1.07it/s]\u001b[A\n","Iteration:  33% 35/106 [00:30<01:13,  1.04s/it]\u001b[A\n","Iteration:  34% 36/106 [00:31<01:08,  1.03it/s]\u001b[A\n","Iteration:  35% 37/106 [00:32<01:06,  1.04it/s]\u001b[A\n","Iteration:  36% 38/106 [00:33<01:03,  1.07it/s]\u001b[A\n","Iteration:  37% 39/106 [00:34<01:02,  1.08it/s]\u001b[A\n","Iteration:  38% 40/106 [00:35<01:03,  1.03it/s]\u001b[A\n","Iteration:  39% 41/106 [00:36<00:59,  1.10it/s]\u001b[A\n","Iteration:  40% 42/106 [00:37<00:56,  1.13it/s]\u001b[A\n","Iteration:  41% 43/106 [00:37<00:57,  1.09it/s]\u001b[A\n","Iteration:  42% 44/106 [00:39<01:03,  1.02s/it]\u001b[A\n","Iteration:  42% 45/106 [00:39<00:55,  1.09it/s]\u001b[A\n","Iteration:  43% 46/106 [00:40<00:57,  1.05it/s]\u001b[A\n","Iteration:  44% 47/106 [00:41<00:52,  1.13it/s]\u001b[A\n","Iteration:  45% 48/106 [00:42<00:50,  1.16it/s]\u001b[A\n","Iteration:  46% 49/106 [00:43<00:54,  1.04it/s]\u001b[A\n","Iteration:  47% 50/106 [00:44<00:51,  1.09it/s]\u001b[A\n","Iteration:  48% 51/106 [00:46<01:00,  1.10s/it]\u001b[A\n","Iteration:  49% 52/106 [00:47<01:01,  1.14s/it]\u001b[A\n","Iteration:  50% 53/106 [00:48<00:55,  1.05s/it]\u001b[A\n","Iteration:  51% 54/106 [00:48<00:50,  1.02it/s]\u001b[A\n","Iteration:  52% 55/106 [00:49<00:47,  1.08it/s]\u001b[A\n","Iteration:  53% 56/106 [00:50<00:44,  1.11it/s]\u001b[A\n","Iteration:  54% 57/106 [00:51<00:41,  1.18it/s]\u001b[A\n","Iteration:  55% 58/106 [00:51<00:38,  1.26it/s]\u001b[A\n","Iteration:  56% 59/106 [00:52<00:36,  1.28it/s]\u001b[A\n","Iteration:  57% 60/106 [00:53<00:35,  1.31it/s]\u001b[A\n","Iteration:  58% 61/106 [00:54<00:39,  1.15it/s]\u001b[A\n","Iteration:  58% 62/106 [00:55<00:38,  1.14it/s]\u001b[A\n","Iteration:  59% 63/106 [00:56<00:34,  1.25it/s]\u001b[A\n","Iteration:  60% 64/106 [00:56<00:33,  1.24it/s]\u001b[A\n","Iteration:  61% 65/106 [00:57<00:32,  1.26it/s]\u001b[A\n","Iteration:  62% 66/106 [00:58<00:32,  1.23it/s]\u001b[A\n","Iteration:  63% 67/106 [00:59<00:30,  1.28it/s]\u001b[A\n","Iteration:  64% 68/106 [00:59<00:29,  1.29it/s]\u001b[A\n","Iteration:  65% 69/106 [01:00<00:28,  1.29it/s]\u001b[A\n","Iteration:  66% 70/106 [01:01<00:27,  1.33it/s]\u001b[A\n","Iteration:  67% 71/106 [01:02<00:26,  1.34it/s]\u001b[A\n","Iteration:  68% 72/106 [01:03<00:29,  1.15it/s]\u001b[A\n","Iteration:  69% 73/106 [01:04<00:30,  1.09it/s]\u001b[A\n","Iteration:  70% 74/106 [01:04<00:26,  1.22it/s]\u001b[A\n","Iteration:  71% 75/106 [01:05<00:24,  1.29it/s]\u001b[A\n","Iteration:  72% 76/106 [01:06<00:24,  1.20it/s]\u001b[A\n","Iteration:  73% 77/106 [01:07<00:24,  1.18it/s]\u001b[A\n","Iteration:  74% 78/106 [01:08<00:23,  1.19it/s]\u001b[A\n","Iteration:  75% 79/106 [01:09<00:27,  1.03s/it]\u001b[A\n","Iteration:  75% 80/106 [01:10<00:25,  1.02it/s]\u001b[A\n","Iteration:  76% 81/106 [01:11<00:22,  1.10it/s]\u001b[A\n","Iteration:  77% 82/106 [01:12<00:23,  1.02it/s]\u001b[A\n","Iteration:  78% 83/106 [01:13<00:22,  1.01it/s]\u001b[A\n","Iteration:  79% 84/106 [01:14<00:21,  1.04it/s]\u001b[A\n","Iteration:  80% 85/106 [01:15<00:19,  1.06it/s]\u001b[A\n","Iteration:  81% 86/106 [01:16<00:18,  1.09it/s]\u001b[A\n","Iteration:  82% 87/106 [01:16<00:16,  1.18it/s]\u001b[A\n","Iteration:  83% 88/106 [01:17<00:14,  1.25it/s]\u001b[A\n","Iteration:  84% 89/106 [01:18<00:13,  1.26it/s]\u001b[A\n","Iteration:  85% 90/106 [01:19<00:12,  1.27it/s]\u001b[A\n","Iteration:  86% 91/106 [01:20<00:13,  1.14it/s]\u001b[A\n","Iteration:  87% 92/106 [01:20<00:11,  1.17it/s]\u001b[A\n","Iteration:  88% 93/106 [01:21<00:11,  1.15it/s]\u001b[A\n","Iteration:  89% 94/106 [01:22<00:10,  1.14it/s]\u001b[A\n","Iteration:  90% 95/106 [01:24<00:13,  1.22s/it]\u001b[A\n","Iteration:  91% 96/106 [01:25<00:12,  1.21s/it]\u001b[A\n","Iteration:  92% 97/106 [01:27<00:11,  1.24s/it]\u001b[A\n","Iteration:  92% 98/106 [01:27<00:08,  1.08s/it]\u001b[A\n","Iteration:  93% 99/106 [01:28<00:06,  1.00it/s]\u001b[A\n","Iteration:  94% 100/106 [01:29<00:05,  1.03it/s]\u001b[A\n","Iteration:  95% 101/106 [01:30<00:04,  1.04it/s]\u001b[A\n","Iteration:  96% 102/106 [01:31<00:03,  1.04it/s]\u001b[A\n","Iteration:  97% 103/106 [01:32<00:03,  1.08s/it]\u001b[A\n","Iteration:  98% 104/106 [01:33<00:01,  1.03it/s]\u001b[A\n","Iteration:  99% 105/106 [01:34<00:00,  1.16it/s]\u001b[A\n","Iteration: 100% 106/106 [01:34<00:00,  1.12it/s]\n","[2022-07-30 10:14:24,932][__main__][DEBUG] - epoch is 1\n","[2022-07-30 10:14:24,934][__main__][DEBUG] - validation loss is tensor(0.1067, device='cuda:0')\n","Epoch:   2% 2/100 [11:42<9:33:53, 351.37s/it]\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/106 [00:00<01:41,  1.04it/s]\u001b[A\n","Iteration:   2% 2/106 [00:01<01:44,  1.00s/it]\u001b[A\n","Iteration:   3% 3/106 [00:03<02:11,  1.28s/it]\u001b[A\n","Iteration:   4% 4/106 [00:04<01:56,  1.14s/it]\u001b[A\n","Iteration:   5% 5/106 [00:05<01:47,  1.07s/it]\u001b[A\n","Iteration:   6% 6/106 [00:06<01:48,  1.09s/it]\u001b[A\n","Iteration:   7% 7/106 [00:07<01:46,  1.07s/it]\u001b[A\n","Iteration:   8% 8/106 [00:08<01:46,  1.08s/it]\u001b[A\n","Iteration:   8% 9/106 [00:09<01:36,  1.01it/s]\u001b[A\n","Iteration:   9% 10/106 [00:10<01:33,  1.03it/s]\u001b[A\n","Iteration:  10% 11/106 [00:11<01:27,  1.08it/s]\u001b[A\n","Iteration:  11% 12/106 [00:12<01:27,  1.08it/s]\u001b[A\n","Iteration:  12% 13/106 [00:13<01:29,  1.04it/s]\u001b[A\n","Iteration:  13% 14/106 [00:14<01:32,  1.00s/it]\u001b[A\n","Iteration:  14% 15/106 [00:15<01:27,  1.04it/s]\u001b[A\n","Iteration:  15% 16/106 [00:15<01:20,  1.11it/s]\u001b[A\n","Iteration:  16% 17/106 [00:16<01:13,  1.21it/s]\u001b[A\n","Iteration:  17% 18/106 [00:17<01:14,  1.18it/s]\u001b[A\n","Iteration:  18% 19/106 [00:18<01:08,  1.28it/s]\u001b[A\n","Iteration:  19% 20/106 [00:18<01:08,  1.26it/s]\u001b[A\n","Iteration:  20% 21/106 [00:20<01:14,  1.14it/s]\u001b[A\n","Iteration:  21% 22/106 [00:20<01:13,  1.14it/s]\u001b[A\n","Iteration:  22% 23/106 [00:21<01:09,  1.20it/s]\u001b[A\n","Iteration:  23% 24/106 [00:22<01:16,  1.08it/s]\u001b[A\n","Iteration:  24% 25/106 [00:23<01:14,  1.08it/s]\u001b[A\n","Iteration:  25% 26/106 [00:24<01:10,  1.13it/s]\u001b[A\n","Iteration:  25% 27/106 [00:25<01:04,  1.23it/s]\u001b[A\n","Iteration:  26% 28/106 [00:25<01:02,  1.26it/s]\u001b[A\n","Iteration:  27% 29/106 [00:26<01:02,  1.24it/s]\u001b[A\n","Iteration:  28% 30/106 [00:27<01:04,  1.18it/s]\u001b[A\n","Iteration:  29% 31/106 [00:28<01:09,  1.09it/s]\u001b[A\n","Iteration:  30% 32/106 [00:30<01:19,  1.08s/it]\u001b[A\n","Iteration:  31% 33/106 [00:30<01:11,  1.02it/s]\u001b[A\n","Iteration:  32% 34/106 [00:31<01:04,  1.12it/s]\u001b[A\n","Iteration:  33% 35/106 [00:32<01:11,  1.00s/it]\u001b[A\n","Iteration:  34% 36/106 [00:33<01:04,  1.08it/s]\u001b[A\n","Iteration:  35% 37/106 [00:34<01:04,  1.06it/s]\u001b[A\n","Iteration:  36% 38/106 [00:35<01:02,  1.09it/s]\u001b[A\n","Iteration:  37% 39/106 [00:36<01:02,  1.06it/s]\u001b[A\n","Iteration:  38% 40/106 [00:37<01:02,  1.06it/s]\u001b[A\n","Iteration:  39% 41/106 [00:38<00:56,  1.14it/s]\u001b[A\n","Iteration:  40% 42/106 [00:38<00:51,  1.24it/s]\u001b[A\n","Iteration:  41% 43/106 [00:39<00:45,  1.37it/s]\u001b[A\n","Iteration:  42% 44/106 [00:40<00:46,  1.33it/s]\u001b[A\n","Iteration:  42% 45/106 [00:40<00:46,  1.32it/s]\u001b[A\n","Iteration:  43% 46/106 [00:41<00:44,  1.34it/s]\u001b[A\n","Iteration:  44% 47/106 [00:42<00:52,  1.13it/s]\u001b[A\n","Iteration:  45% 48/106 [00:43<00:46,  1.25it/s]\u001b[A\n","Iteration:  46% 49/106 [00:44<00:44,  1.28it/s]\u001b[A\n","Iteration:  47% 50/106 [00:45<00:49,  1.14it/s]\u001b[A\n","Iteration:  48% 51/106 [00:45<00:44,  1.22it/s]\u001b[A\n","Iteration:  49% 52/106 [00:46<00:43,  1.25it/s]\u001b[A\n","Iteration:  50% 53/106 [00:47<00:46,  1.14it/s]\u001b[A\n","Iteration:  51% 54/106 [00:48<00:47,  1.09it/s]\u001b[A\n","Iteration:  52% 55/106 [00:49<00:43,  1.18it/s]\u001b[A\n","Iteration:  53% 56/106 [00:50<00:43,  1.15it/s]\u001b[A\n","Iteration:  54% 57/106 [00:51<00:50,  1.02s/it]\u001b[A\n","Iteration:  55% 58/106 [00:52<00:47,  1.00it/s]\u001b[A\n","Iteration:  56% 59/106 [00:53<00:47,  1.00s/it]\u001b[A\n","Iteration:  57% 60/106 [00:54<00:42,  1.08it/s]\u001b[A\n","Iteration:  58% 61/106 [00:55<00:42,  1.06it/s]\u001b[A\n","Iteration:  58% 62/106 [00:56<00:44,  1.01s/it]\u001b[A\n","Iteration:  59% 63/106 [00:57<00:38,  1.12it/s]\u001b[A\n","Iteration:  60% 64/106 [00:58<00:40,  1.04it/s]\u001b[A\n","Iteration:  61% 65/106 [00:59<00:37,  1.08it/s]\u001b[A\n","Iteration:  62% 66/106 [00:59<00:33,  1.19it/s]\u001b[A\n","Iteration:  63% 67/106 [01:00<00:32,  1.21it/s]\u001b[A\n","Iteration:  64% 68/106 [01:01<00:32,  1.17it/s]\u001b[A\n","Iteration:  65% 69/106 [01:02<00:29,  1.24it/s]\u001b[A\n","Iteration:  66% 70/106 [01:03<00:29,  1.24it/s]\u001b[A\n","Iteration:  67% 71/106 [01:03<00:28,  1.21it/s]\u001b[A\n","Iteration:  68% 72/106 [01:04<00:29,  1.14it/s]\u001b[A\n","Iteration:  69% 73/106 [01:05<00:29,  1.13it/s]\u001b[A\n","Iteration:  70% 74/106 [01:07<00:31,  1.00it/s]\u001b[A\n","Iteration:  71% 75/106 [01:07<00:28,  1.08it/s]\u001b[A\n","Iteration:  72% 76/106 [01:08<00:26,  1.14it/s]\u001b[A\n","Iteration:  73% 77/106 [01:10<00:30,  1.05s/it]\u001b[A\n","Iteration:  74% 78/106 [01:10<00:26,  1.04it/s]\u001b[A\n","Iteration:  75% 79/106 [01:11<00:25,  1.06it/s]\u001b[A\n","Iteration:  75% 80/106 [01:12<00:23,  1.09it/s]\u001b[A\n","Iteration:  76% 81/106 [01:13<00:25,  1.02s/it]\u001b[A\n","Iteration:  77% 82/106 [01:14<00:22,  1.08it/s]\u001b[A\n","Iteration:  78% 83/106 [01:15<00:22,  1.01it/s]\u001b[A\n","Iteration:  79% 84/106 [01:16<00:19,  1.10it/s]\u001b[A\n","Iteration:  80% 85/106 [01:17<00:17,  1.22it/s]\u001b[A\n","Iteration:  81% 86/106 [01:17<00:16,  1.25it/s]\u001b[A\n","Iteration:  82% 87/106 [01:18<00:15,  1.19it/s]\u001b[A\n","Iteration:  83% 88/106 [01:19<00:15,  1.16it/s]\u001b[A\n","Iteration:  84% 89/106 [01:20<00:14,  1.15it/s]\u001b[A\n","Iteration:  85% 90/106 [01:21<00:13,  1.22it/s]\u001b[A\n","Iteration:  86% 91/106 [01:21<00:11,  1.29it/s]\u001b[A\n","Iteration:  87% 92/106 [01:23<00:12,  1.11it/s]\u001b[A\n","Iteration:  88% 93/106 [01:23<00:11,  1.16it/s]\u001b[A\n","Iteration:  89% 94/106 [01:24<00:10,  1.19it/s]\u001b[A\n","Iteration:  90% 95/106 [01:25<00:10,  1.05it/s]\u001b[A\n","Iteration:  91% 96/106 [01:26<00:08,  1.18it/s]\u001b[A\n","Iteration:  92% 97/106 [01:27<00:07,  1.22it/s]\u001b[A\n","Iteration:  92% 98/106 [01:28<00:06,  1.17it/s]\u001b[A\n","Iteration:  93% 99/106 [01:29<00:06,  1.16it/s]\u001b[A\n","Iteration:  94% 100/106 [01:29<00:05,  1.14it/s]\u001b[A\n","Iteration:  95% 101/106 [01:30<00:04,  1.12it/s]\u001b[A\n","Iteration:  96% 102/106 [01:31<00:03,  1.26it/s]\u001b[A\n","Iteration:  97% 103/106 [01:32<00:02,  1.19it/s]\u001b[A\n","Iteration:  98% 104/106 [01:33<00:01,  1.18it/s]\u001b[A\n","Iteration:  99% 105/106 [01:34<00:00,  1.19it/s]\u001b[A\n","Iteration: 100% 106/106 [01:34<00:00,  1.12it/s]\n","[2022-07-30 10:20:16,628][__main__][DEBUG] - epoch is 2\n","[2022-07-30 10:20:16,629][__main__][DEBUG] - validation loss is tensor(0.1045, device='cuda:0')\n","Epoch:   3% 3/100 [17:34<9:28:15, 351.50s/it]\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/106 [00:00<01:37,  1.07it/s]\u001b[A\n","Iteration:   2% 2/106 [00:01<01:29,  1.16it/s]\u001b[A\n","Iteration:   3% 3/106 [00:03<01:51,  1.08s/it]\u001b[A\n","Iteration:   4% 4/106 [00:04<01:46,  1.04s/it]\u001b[A\n","Iteration:   5% 5/106 [00:04<01:38,  1.03it/s]\u001b[A\n","Iteration:   6% 6/106 [00:05<01:33,  1.07it/s]\u001b[A\n","Iteration:   7% 7/106 [00:06<01:34,  1.05it/s]\u001b[A\n","Iteration:   8% 8/106 [00:07<01:34,  1.04it/s]\u001b[A\n","Iteration:   8% 9/106 [00:08<01:25,  1.13it/s]\u001b[A\n","Iteration:   9% 10/106 [00:09<01:33,  1.03it/s]\u001b[A\n","Iteration:  10% 11/106 [00:10<01:29,  1.07it/s]\u001b[A\n","Iteration:  11% 12/106 [00:11<01:30,  1.04it/s]\u001b[A\n","Iteration:  12% 13/106 [00:12<01:31,  1.02it/s]\u001b[A\n","Iteration:  13% 14/106 [00:14<01:50,  1.20s/it]\u001b[A\n","Iteration:  14% 15/106 [00:15<01:42,  1.12s/it]\u001b[A\n","Iteration:  15% 16/106 [00:16<01:46,  1.18s/it]\u001b[A\n","Iteration:  16% 17/106 [00:17<01:38,  1.10s/it]\u001b[A\n","Iteration:  17% 18/106 [00:18<01:27,  1.00it/s]\u001b[A\n","Iteration:  18% 19/106 [00:19<01:22,  1.06it/s]\u001b[A\n","Iteration:  19% 20/106 [00:19<01:16,  1.12it/s]\u001b[A\n","Iteration:  20% 21/106 [00:20<01:12,  1.18it/s]\u001b[A\n","Iteration:  21% 22/106 [00:21<01:11,  1.17it/s]\u001b[A\n","Iteration:  22% 23/106 [00:21<01:04,  1.29it/s]\u001b[A\n","Iteration:  23% 24/106 [00:22<01:03,  1.30it/s]\u001b[A\n","Iteration:  24% 25/106 [00:23<01:02,  1.30it/s]\u001b[A\n","Iteration:  25% 26/106 [00:24<01:02,  1.29it/s]\u001b[A\n","Iteration:  25% 27/106 [00:25<01:00,  1.31it/s]\u001b[A\n","Iteration:  26% 28/106 [00:26<01:08,  1.14it/s]\u001b[A\n","Iteration:  27% 29/106 [00:27<01:06,  1.16it/s]\u001b[A\n","Iteration:  28% 30/106 [00:28<01:08,  1.10it/s]\u001b[A\n","Iteration:  29% 31/106 [00:29<01:12,  1.04it/s]\u001b[A\n","Iteration:  30% 32/106 [00:29<01:05,  1.13it/s]\u001b[A\n","Iteration:  31% 33/106 [00:30<01:05,  1.11it/s]\u001b[A\n","Iteration:  32% 34/106 [00:31<01:10,  1.02it/s]\u001b[A\n","Iteration:  33% 35/106 [00:32<01:06,  1.07it/s]\u001b[A\n","Iteration:  34% 36/106 [00:33<01:00,  1.16it/s]\u001b[A\n","Iteration:  35% 37/106 [00:34<01:01,  1.12it/s]\u001b[A\n","Iteration:  36% 38/106 [00:35<01:03,  1.08it/s]\u001b[A\n","Iteration:  37% 39/106 [00:36<01:02,  1.07it/s]\u001b[A\n","Iteration:  38% 40/106 [00:37<01:02,  1.05it/s]\u001b[A\n","Iteration:  39% 41/106 [00:38<00:56,  1.16it/s]\u001b[A\n","Iteration:  40% 42/106 [00:38<00:53,  1.21it/s]\u001b[A\n","Iteration:  41% 43/106 [00:39<00:59,  1.06it/s]\u001b[A\n","Iteration:  42% 44/106 [00:40<00:50,  1.22it/s]\u001b[A\n","Iteration:  42% 45/106 [00:41<00:47,  1.27it/s]\u001b[A\n","Iteration:  43% 46/106 [00:41<00:46,  1.29it/s]\u001b[A\n","Iteration:  44% 47/106 [00:42<00:49,  1.20it/s]\u001b[A\n","Iteration:  45% 48/106 [00:44<00:53,  1.09it/s]\u001b[A\n","Iteration:  46% 49/106 [00:44<00:50,  1.13it/s]\u001b[A\n","Iteration:  47% 50/106 [00:45<00:45,  1.23it/s]\u001b[A\n","Iteration:  48% 51/106 [00:46<00:49,  1.11it/s]\u001b[A\n","Iteration:  49% 52/106 [00:47<00:45,  1.20it/s]\u001b[A\n","Iteration:  50% 53/106 [00:47<00:40,  1.30it/s]\u001b[A\n","Iteration:  51% 54/106 [00:48<00:39,  1.32it/s]\u001b[A\n","Iteration:  52% 55/106 [00:49<00:44,  1.15it/s]\u001b[A\n","Iteration:  53% 56/106 [00:50<00:47,  1.06it/s]\u001b[A\n","Iteration:  54% 57/106 [00:51<00:42,  1.15it/s]\u001b[A\n","Iteration:  55% 58/106 [00:52<00:39,  1.21it/s]\u001b[A\n","Iteration:  56% 59/106 [00:52<00:35,  1.33it/s]\u001b[A\n","Iteration:  57% 60/106 [00:53<00:39,  1.15it/s]\u001b[A\n","Iteration:  58% 61/106 [00:55<00:43,  1.02it/s]\u001b[A\n","Iteration:  58% 62/106 [00:55<00:38,  1.14it/s]\u001b[A\n","Iteration:  59% 63/106 [00:56<00:38,  1.10it/s]\u001b[A\n","Iteration:  60% 64/106 [00:57<00:36,  1.16it/s]\u001b[A\n","Iteration:  61% 65/106 [00:58<00:31,  1.30it/s]\u001b[A\n","Iteration:  62% 66/106 [00:58<00:31,  1.27it/s]\u001b[A\n","Iteration:  63% 67/106 [00:59<00:30,  1.30it/s]\u001b[A\n","Iteration:  64% 68/106 [01:00<00:28,  1.31it/s]\u001b[A\n","Iteration:  65% 69/106 [01:01<00:32,  1.13it/s]\u001b[A\n","Iteration:  66% 70/106 [01:02<00:31,  1.16it/s]\u001b[A\n","Iteration:  67% 71/106 [01:03<00:31,  1.10it/s]\u001b[A\n","Iteration:  68% 72/106 [01:04<00:34,  1.02s/it]\u001b[A\n","Iteration:  69% 73/106 [01:05<00:32,  1.03it/s]\u001b[A\n","Iteration:  70% 74/106 [01:06<00:28,  1.12it/s]\u001b[A\n","Iteration:  71% 75/106 [01:07<00:26,  1.15it/s]\u001b[A\n","Iteration:  72% 76/106 [01:08<00:28,  1.07it/s]\u001b[A\n","Iteration:  73% 77/106 [01:09<00:29,  1.02s/it]\u001b[A\n","Iteration:  74% 78/106 [01:10<00:25,  1.10it/s]\u001b[A\n","Iteration:  75% 79/106 [01:10<00:21,  1.25it/s]\u001b[A\n","Iteration:  75% 80/106 [01:11<00:21,  1.24it/s]\u001b[A\n","Iteration:  76% 81/106 [01:12<00:24,  1.03it/s]\u001b[A\n","Iteration:  77% 82/106 [01:13<00:22,  1.05it/s]\u001b[A\n","Iteration:  78% 83/106 [01:14<00:20,  1.11it/s]\u001b[A\n","Iteration:  79% 84/106 [01:15<00:22,  1.01s/it]\u001b[A\n","Iteration:  80% 85/106 [01:16<00:21,  1.00s/it]\u001b[A\n","Iteration:  81% 86/106 [01:18<00:21,  1.08s/it]\u001b[A\n","Iteration:  82% 87/106 [01:18<00:18,  1.03it/s]\u001b[A\n","Iteration:  83% 88/106 [01:19<00:18,  1.03s/it]\u001b[A\n","Iteration:  84% 89/106 [01:20<00:17,  1.04s/it]\u001b[A\n","Iteration:  85% 90/106 [01:21<00:15,  1.01it/s]\u001b[A\n","Iteration:  86% 91/106 [01:22<00:13,  1.08it/s]\u001b[A\n","Iteration:  87% 92/106 [01:23<00:13,  1.07it/s]\u001b[A\n","Iteration:  88% 93/106 [01:24<00:11,  1.11it/s]\u001b[A\n","Iteration:  89% 94/106 [01:25<00:10,  1.20it/s]\u001b[A\n","Iteration:  90% 95/106 [01:25<00:09,  1.22it/s]\u001b[A\n","Iteration:  91% 96/106 [01:26<00:08,  1.20it/s]\u001b[A\n","Iteration:  92% 97/106 [01:27<00:07,  1.24it/s]\u001b[A\n","Iteration:  92% 98/106 [01:28<00:06,  1.21it/s]\u001b[A\n","Iteration:  93% 99/106 [01:29<00:06,  1.07it/s]\u001b[A\n","Iteration:  94% 100/106 [01:30<00:05,  1.08it/s]\u001b[A\n","Iteration:  95% 101/106 [01:31<00:04,  1.12it/s]\u001b[A\n","Iteration:  96% 102/106 [01:31<00:03,  1.23it/s]\u001b[A\n","Iteration:  97% 103/106 [01:33<00:02,  1.08it/s]\u001b[A\n","Iteration:  98% 104/106 [01:33<00:01,  1.21it/s]\u001b[A\n","Iteration:  99% 105/106 [01:34<00:00,  1.07it/s]\u001b[A\n","Iteration: 100% 106/106 [01:35<00:00,  1.11it/s]\n","[2022-07-30 10:26:08,991][__main__][DEBUG] - epoch is 3\n","[2022-07-30 10:26:08,993][__main__][DEBUG] - validation loss is tensor(0.1022, device='cuda:0')\n","Epoch:   4% 4/100 [23:26<9:23:05, 351.93s/it]\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/106 [00:01<02:00,  1.14s/it]\u001b[A\n","Iteration:   2% 2/106 [00:02<01:42,  1.02it/s]\u001b[A\n","Iteration:   3% 3/106 [00:03<01:49,  1.07s/it]\u001b[A\n","Iteration:   4% 4/106 [00:04<02:03,  1.21s/it]\u001b[A\n","Iteration:   5% 5/106 [00:05<01:38,  1.03it/s]\u001b[A\n","Iteration:   6% 6/106 [00:05<01:28,  1.13it/s]\u001b[A\n","Iteration:   7% 7/106 [00:06<01:32,  1.07it/s]\u001b[A\n","Iteration:   8% 8/106 [00:07<01:35,  1.03it/s]\u001b[A\n","Iteration:   8% 9/106 [00:09<01:42,  1.05s/it]\u001b[A\n","Iteration:   9% 10/106 [00:10<01:47,  1.12s/it]\u001b[A\n","Iteration:  10% 11/106 [00:11<01:40,  1.06s/it]\u001b[A\n","Iteration:  11% 12/106 [00:12<01:35,  1.02s/it]\u001b[A\n","Iteration:  12% 13/106 [00:13<01:26,  1.08it/s]\u001b[A\n","Iteration:  13% 14/106 [00:14<01:34,  1.02s/it]\u001b[A\n","Iteration:  14% 15/106 [00:15<01:26,  1.05it/s]\u001b[A\n","Iteration:  15% 16/106 [00:15<01:16,  1.18it/s]\u001b[A\n","Iteration:  16% 17/106 [00:16<01:15,  1.19it/s]\u001b[A\n","Iteration:  17% 18/106 [00:17<01:22,  1.07it/s]\u001b[A\n","Iteration:  18% 19/106 [00:18<01:20,  1.08it/s]\u001b[A\n","Iteration:  19% 20/106 [00:19<01:19,  1.09it/s]\u001b[A\n","Iteration:  20% 21/106 [00:20<01:14,  1.14it/s]\u001b[A\n","Iteration:  21% 22/106 [00:20<01:08,  1.22it/s]\u001b[A\n","Iteration:  22% 23/106 [00:21<01:11,  1.17it/s]\u001b[A\n","Iteration:  23% 24/106 [00:22<01:08,  1.19it/s]\u001b[A\n","Iteration:  24% 25/106 [00:23<01:02,  1.29it/s]\u001b[A\n","Iteration:  25% 26/106 [00:24<01:07,  1.18it/s]\u001b[A\n","Iteration:  25% 27/106 [00:25<01:03,  1.24it/s]\u001b[A\n","Iteration:  26% 28/106 [00:25<01:01,  1.26it/s]\u001b[A\n","Iteration:  27% 29/106 [00:26<01:00,  1.26it/s]\u001b[A\n","Iteration:  28% 30/106 [00:27<01:13,  1.03it/s]\u001b[A\n","Iteration:  29% 31/106 [00:28<01:08,  1.09it/s]\u001b[A\n","Iteration:  30% 32/106 [00:30<01:26,  1.17s/it]\u001b[A\n","Iteration:  31% 33/106 [00:31<01:16,  1.05s/it]\u001b[A\n","Iteration:  32% 34/106 [00:32<01:10,  1.02it/s]\u001b[A\n","Iteration:  33% 35/106 [00:32<01:08,  1.04it/s]\u001b[A\n","Iteration:  34% 36/106 [00:33<01:04,  1.08it/s]\u001b[A\n","Iteration:  35% 37/106 [00:34<00:57,  1.20it/s]\u001b[A\n","Iteration:  36% 38/106 [00:35<00:57,  1.19it/s]\u001b[A\n","Iteration:  37% 39/106 [00:36<00:56,  1.18it/s]\u001b[A\n","Iteration:  38% 40/106 [00:37<01:09,  1.06s/it]\u001b[A\n","Iteration:  39% 41/106 [00:38<01:05,  1.01s/it]\u001b[A\n","Iteration:  40% 42/106 [00:39<00:59,  1.08it/s]\u001b[A\n","Iteration:  41% 43/106 [00:40<01:04,  1.03s/it]\u001b[A\n","Iteration:  42% 44/106 [00:41<01:01,  1.00it/s]\u001b[A\n","Iteration:  42% 45/106 [00:42<00:57,  1.07it/s]\u001b[A\n","Iteration:  43% 46/106 [00:43<00:57,  1.05it/s]\u001b[A\n","Iteration:  44% 47/106 [00:44<00:54,  1.09it/s]\u001b[A\n","Iteration:  45% 48/106 [00:45<00:51,  1.12it/s]\u001b[A\n","Iteration:  46% 49/106 [00:46<00:53,  1.07it/s]\u001b[A\n","Iteration:  47% 50/106 [00:46<00:45,  1.22it/s]\u001b[A\n","Iteration:  48% 51/106 [00:47<00:45,  1.20it/s]\u001b[A\n","Iteration:  49% 52/106 [00:48<00:53,  1.02it/s]\u001b[A\n","Iteration:  50% 53/106 [00:49<00:49,  1.06it/s]\u001b[A\n","Iteration:  51% 54/106 [00:50<00:45,  1.14it/s]\u001b[A\n","Iteration:  52% 55/106 [00:51<00:45,  1.13it/s]\u001b[A\n","Iteration:  53% 56/106 [00:52<00:42,  1.17it/s]\u001b[A\n","Iteration:  54% 57/106 [00:52<00:39,  1.23it/s]\u001b[A\n","Iteration:  55% 58/106 [00:53<00:37,  1.27it/s]\u001b[A\n","Iteration:  56% 59/106 [00:54<00:35,  1.31it/s]\u001b[A\n","Iteration:  57% 60/106 [00:54<00:35,  1.31it/s]\u001b[A\n","Iteration:  58% 61/106 [00:55<00:33,  1.32it/s]\u001b[A\n","Iteration:  58% 62/106 [00:56<00:33,  1.33it/s]\u001b[A\n","Iteration:  59% 63/106 [00:57<00:33,  1.28it/s]\u001b[A\n","Iteration:  60% 64/106 [00:58<00:35,  1.20it/s]\u001b[A\n","Iteration:  61% 65/106 [00:58<00:31,  1.30it/s]\u001b[A\n","Iteration:  62% 66/106 [00:59<00:33,  1.18it/s]\u001b[A\n","Iteration:  63% 67/106 [01:00<00:34,  1.14it/s]\u001b[A\n","Iteration:  64% 68/106 [01:01<00:33,  1.12it/s]\u001b[A\n","Iteration:  65% 69/106 [01:02<00:31,  1.18it/s]\u001b[A\n","Iteration:  66% 70/106 [01:03<00:32,  1.11it/s]\u001b[A\n","Iteration:  67% 71/106 [01:04<00:31,  1.13it/s]\u001b[A\n","Iteration:  68% 72/106 [01:05<00:29,  1.14it/s]\u001b[A\n","Iteration:  69% 73/106 [01:06<00:27,  1.18it/s]\u001b[A\n","Iteration:  70% 74/106 [01:07<00:31,  1.01it/s]\u001b[A\n","Iteration:  71% 75/106 [01:08<00:30,  1.02it/s]\u001b[A\n","Iteration:  72% 76/106 [01:09<00:29,  1.03it/s]\u001b[A\n","Iteration:  73% 77/106 [01:10<00:26,  1.10it/s]\u001b[A\n","Iteration:  74% 78/106 [01:10<00:23,  1.19it/s]\u001b[A\n","Iteration:  75% 79/106 [01:11<00:23,  1.16it/s]\u001b[A\n","Iteration:  75% 80/106 [01:12<00:23,  1.10it/s]\u001b[A\n","Iteration:  76% 81/106 [01:13<00:21,  1.15it/s]\u001b[A\n","Iteration:  77% 82/106 [01:14<00:21,  1.12it/s]\u001b[A\n","Iteration:  78% 83/106 [01:15<00:22,  1.01it/s]\u001b[A\n","Iteration:  79% 84/106 [01:16<00:21,  1.03it/s]\u001b[A\n","Iteration:  80% 85/106 [01:17<00:19,  1.06it/s]\u001b[A\n","Iteration:  81% 86/106 [01:18<00:17,  1.12it/s]\u001b[A\n","Iteration:  82% 87/106 [01:19<00:17,  1.11it/s]\u001b[A\n","Iteration:  83% 88/106 [01:19<00:15,  1.16it/s]\u001b[A\n","Iteration:  84% 89/106 [01:20<00:14,  1.17it/s]\u001b[A\n","Iteration:  85% 90/106 [01:21<00:13,  1.17it/s]\u001b[A\n","Iteration:  86% 91/106 [01:22<00:12,  1.16it/s]\u001b[A\n","Iteration:  87% 92/106 [01:23<00:11,  1.22it/s]\u001b[A\n","Iteration:  88% 93/106 [01:23<00:10,  1.28it/s]\u001b[A\n","Iteration:  89% 94/106 [01:24<00:09,  1.32it/s]\u001b[A\n","Iteration:  90% 95/106 [01:25<00:09,  1.20it/s]\u001b[A\n","Iteration:  91% 96/106 [01:26<00:09,  1.03it/s]\u001b[A\n","Iteration:  92% 97/106 [01:27<00:08,  1.12it/s]\u001b[A\n","Iteration:  92% 98/106 [01:28<00:08,  1.00s/it]\u001b[A\n","Iteration:  93% 99/106 [01:29<00:06,  1.11it/s]\u001b[A\n","Iteration:  94% 100/106 [01:30<00:06,  1.07s/it]\u001b[A\n","Iteration:  95% 101/106 [01:31<00:04,  1.01it/s]\u001b[A\n","Iteration:  96% 102/106 [01:32<00:03,  1.14it/s]\u001b[A\n","Iteration:  97% 103/106 [01:33<00:02,  1.01it/s]\u001b[A\n","Iteration:  98% 104/106 [01:34<00:01,  1.10it/s]\u001b[A\n","Iteration:  99% 105/106 [01:35<00:00,  1.17it/s]\u001b[A\n","Iteration: 100% 106/106 [01:35<00:00,  1.11it/s]\n","[2022-07-30 10:32:02,577][__main__][DEBUG] - epoch is 4\n","[2022-07-30 10:32:02,578][__main__][DEBUG] - validation loss is tensor(0.1032, device='cuda:0')\n","Epoch:   5% 5/100 [29:20<9:18:03, 352.46s/it]\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/106 [00:00<01:05,  1.60it/s]\u001b[A\n","Iteration:   2% 2/106 [00:01<01:16,  1.35it/s]\u001b[A\n","Iteration:   3% 3/106 [00:02<01:10,  1.46it/s]\u001b[A\n","Iteration:   4% 4/106 [00:02<01:13,  1.39it/s]\u001b[A\n","Iteration:   5% 5/106 [00:03<01:17,  1.30it/s]\u001b[A\n","Iteration:   6% 6/106 [00:04<01:19,  1.26it/s]\u001b[A\n","Iteration:   7% 7/106 [00:05<01:15,  1.30it/s]\u001b[A\n","Iteration:   8% 8/106 [00:05<01:13,  1.33it/s]\u001b[A\n","Iteration:   8% 9/106 [00:07<01:40,  1.04s/it]\u001b[A\n","Iteration:   9% 10/106 [00:08<01:35,  1.01it/s]\u001b[A\n","Iteration:  10% 11/106 [00:09<01:32,  1.03it/s]\u001b[A\n","Iteration:  11% 12/106 [00:10<01:24,  1.12it/s]\u001b[A\n","Iteration:  12% 13/106 [00:11<01:27,  1.06it/s]\u001b[A\n","Iteration:  13% 14/106 [00:12<01:24,  1.10it/s]\u001b[A\n","Iteration:  14% 15/106 [00:13<01:30,  1.01it/s]\u001b[A\n","Iteration:  15% 16/106 [00:14<01:45,  1.17s/it]\u001b[A\n","Iteration:  16% 17/106 [00:15<01:33,  1.05s/it]\u001b[A\n","Iteration:  17% 18/106 [00:16<01:32,  1.05s/it]\u001b[A\n","Iteration:  18% 19/106 [00:17<01:29,  1.03s/it]\u001b[A\n","Iteration:  19% 20/106 [00:18<01:35,  1.11s/it]\u001b[A\n","Iteration:  20% 21/106 [00:20<01:50,  1.30s/it]\u001b[A\n","Iteration:  21% 22/106 [00:21<01:35,  1.13s/it]\u001b[A\n","Iteration:  22% 23/106 [00:22<01:31,  1.10s/it]\u001b[A\n","Iteration:  23% 24/106 [00:23<01:20,  1.01it/s]\u001b[A\n","Iteration:  24% 25/106 [00:24<01:17,  1.04it/s]\u001b[A\n","Iteration:  25% 26/106 [00:24<01:13,  1.09it/s]\u001b[A\n","Iteration:  25% 27/106 [00:25<01:08,  1.15it/s]\u001b[A\n","Iteration:  26% 28/106 [00:26<01:09,  1.12it/s]\u001b[A\n","Iteration:  27% 29/106 [00:27<01:07,  1.13it/s]\u001b[A\n","Iteration:  28% 30/106 [00:28<01:02,  1.22it/s]\u001b[A\n","Iteration:  29% 31/106 [00:28<01:00,  1.24it/s]\u001b[A\n","Iteration:  30% 32/106 [00:29<01:01,  1.20it/s]\u001b[A\n","Iteration:  31% 33/106 [00:30<01:05,  1.12it/s]\u001b[A\n","Iteration:  32% 34/106 [00:32<01:13,  1.03s/it]\u001b[A\n","Iteration:  33% 35/106 [00:32<01:07,  1.06it/s]\u001b[A\n","Iteration:  34% 36/106 [00:33<01:04,  1.09it/s]\u001b[A\n","Iteration:  35% 37/106 [00:34<01:03,  1.08it/s]\u001b[A\n","Iteration:  36% 38/106 [00:35<01:03,  1.08it/s]\u001b[A\n","Iteration:  37% 39/106 [00:36<00:56,  1.18it/s]\u001b[A\n","Iteration:  38% 40/106 [00:37<00:53,  1.23it/s]\u001b[A\n","Iteration:  39% 41/106 [00:37<00:52,  1.23it/s]\u001b[A\n","Iteration:  40% 42/106 [00:38<00:51,  1.24it/s]\u001b[A\n","Iteration:  41% 43/106 [00:39<00:59,  1.07it/s]\u001b[A\n","Iteration:  42% 44/106 [00:40<00:52,  1.17it/s]\u001b[A\n","Iteration:  42% 45/106 [00:41<00:55,  1.11it/s]\u001b[A\n","Iteration:  43% 46/106 [00:42<00:51,  1.16it/s]\u001b[A\n","Iteration:  44% 47/106 [00:42<00:47,  1.25it/s]\u001b[A\n","Iteration:  45% 48/106 [00:44<00:54,  1.06it/s]\u001b[A\n","Iteration:  46% 49/106 [00:45<00:53,  1.07it/s]\u001b[A\n","Iteration:  47% 50/106 [00:46<00:55,  1.01it/s]\u001b[A\n","Iteration:  48% 51/106 [00:47<00:51,  1.08it/s]\u001b[A\n","Iteration:  49% 52/106 [00:47<00:48,  1.11it/s]\u001b[A\n","Iteration:  50% 53/106 [00:48<00:44,  1.19it/s]\u001b[A\n","Iteration:  51% 54/106 [00:49<00:41,  1.26it/s]\u001b[A\n","Iteration:  52% 55/106 [00:50<00:41,  1.23it/s]\u001b[A\n","Iteration:  53% 56/106 [00:50<00:37,  1.34it/s]\u001b[A\n","Iteration:  54% 57/106 [00:51<00:38,  1.26it/s]\u001b[A\n","Iteration:  55% 58/106 [00:52<00:37,  1.27it/s]\u001b[A\n","Iteration:  56% 59/106 [00:53<00:40,  1.17it/s]\u001b[A\n","Iteration:  57% 60/106 [00:54<00:37,  1.23it/s]\u001b[A\n","Iteration:  58% 61/106 [00:54<00:36,  1.24it/s]\u001b[A\n","Iteration:  58% 62/106 [00:55<00:34,  1.29it/s]\u001b[A\n","Iteration:  59% 63/106 [00:56<00:38,  1.12it/s]\u001b[A\n","Iteration:  60% 64/106 [00:57<00:38,  1.08it/s]\u001b[A\n","Iteration:  61% 65/106 [00:58<00:34,  1.18it/s]\u001b[A\n","Iteration:  62% 66/106 [00:59<00:36,  1.09it/s]\u001b[A\n","Iteration:  63% 67/106 [01:00<00:36,  1.08it/s]\u001b[A\n","Iteration:  64% 68/106 [01:01<00:34,  1.10it/s]\u001b[A\n","Iteration:  65% 69/106 [01:02<00:32,  1.14it/s]\u001b[A\n","Iteration:  66% 70/106 [01:02<00:29,  1.22it/s]\u001b[A\n","Iteration:  67% 71/106 [01:03<00:29,  1.19it/s]\u001b[A\n","Iteration:  68% 72/106 [01:04<00:29,  1.15it/s]\u001b[A\n","Iteration:  69% 73/106 [01:05<00:29,  1.12it/s]\u001b[A\n","Iteration:  70% 74/106 [01:06<00:27,  1.17it/s]\u001b[A\n","Iteration:  71% 75/106 [01:07<00:26,  1.17it/s]\u001b[A\n","Iteration:  72% 76/106 [01:07<00:23,  1.26it/s]\u001b[A\n","Iteration:  73% 77/106 [01:08<00:24,  1.17it/s]\u001b[A\n","Iteration:  74% 78/106 [01:09<00:22,  1.25it/s]\u001b[A\n","Iteration:  75% 79/106 [01:10<00:20,  1.31it/s]\u001b[A\n","Iteration:  75% 80/106 [01:10<00:19,  1.34it/s]\u001b[A\n","Iteration:  76% 81/106 [01:11<00:20,  1.22it/s]\u001b[A\n","Iteration:  77% 82/106 [01:12<00:20,  1.16it/s]\u001b[A\n","Iteration:  78% 83/106 [01:14<00:22,  1.02it/s]\u001b[A\n","Iteration:  79% 84/106 [01:15<00:23,  1.05s/it]\u001b[A\n","Iteration:  80% 85/106 [01:16<00:22,  1.07s/it]\u001b[A\n","Iteration:  81% 86/106 [01:17<00:20,  1.05s/it]\u001b[A\n","Iteration:  82% 87/106 [01:18<00:17,  1.09it/s]\u001b[A\n","Iteration:  83% 88/106 [01:19<00:19,  1.09s/it]\u001b[A\n","Iteration:  84% 89/106 [01:20<00:19,  1.16s/it]\u001b[A\n","Iteration:  85% 90/106 [01:21<00:16,  1.02s/it]\u001b[A\n","Iteration:  86% 91/106 [01:22<00:15,  1.00s/it]\u001b[A\n","Iteration:  87% 92/106 [01:23<00:14,  1.07s/it]\u001b[A\n","Iteration:  88% 93/106 [01:24<00:12,  1.05it/s]\u001b[A\n","Iteration:  89% 94/106 [01:25<00:10,  1.13it/s]\u001b[A\n","Iteration:  90% 95/106 [01:26<00:09,  1.13it/s]\u001b[A\n","Iteration:  91% 96/106 [01:26<00:08,  1.23it/s]\u001b[A\n","Iteration:  92% 97/106 [01:27<00:07,  1.15it/s]\u001b[A\n","Iteration:  92% 98/106 [01:28<00:07,  1.03it/s]\u001b[A\n","Iteration:  93% 99/106 [01:29<00:06,  1.10it/s]\u001b[A\n","Iteration:  94% 100/106 [01:30<00:05,  1.14it/s]\u001b[A\n","Iteration:  95% 101/106 [01:31<00:04,  1.16it/s]\u001b[A\n","Iteration:  96% 102/106 [01:32<00:03,  1.17it/s]\u001b[A\n","Iteration:  97% 103/106 [01:33<00:02,  1.16it/s]\u001b[A\n","Iteration:  98% 104/106 [01:34<00:01,  1.13it/s]\u001b[A\n","Iteration:  99% 105/106 [01:34<00:00,  1.17it/s]\u001b[A\n","Iteration: 100% 106/106 [01:35<00:00,  1.11it/s]\n","[2022-07-30 10:37:55,758][__main__][DEBUG] - epoch is 5\n","[2022-07-30 10:37:55,760][__main__][DEBUG] - validation loss is tensor(0.1048, device='cuda:0')\n","Epoch:   6% 6/100 [35:13<9:12:32, 352.68s/it]\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/106 [00:01<01:53,  1.09s/it]\u001b[A\n","Iteration:   2% 2/106 [00:02<01:49,  1.06s/it]\u001b[A\n","Iteration:   3% 3/106 [00:03<01:45,  1.02s/it]\u001b[A\n","Iteration:   4% 4/106 [00:03<01:32,  1.10it/s]\u001b[A\n","Iteration:   5% 5/106 [00:05<01:41,  1.00s/it]\u001b[A\n","Iteration:   6% 6/106 [00:05<01:31,  1.09it/s]\u001b[A\n","Iteration:   7% 7/106 [00:06<01:30,  1.10it/s]\u001b[A\n","Iteration:   8% 8/106 [00:08<01:57,  1.20s/it]\u001b[A\n","Iteration:   8% 9/106 [00:09<01:59,  1.24s/it]\u001b[A\n","Iteration:   9% 10/106 [00:11<01:58,  1.23s/it]\u001b[A\n","Iteration:  10% 11/106 [00:11<01:42,  1.08s/it]\u001b[A\n","Iteration:  11% 12/106 [00:12<01:38,  1.05s/it]\u001b[A\n","Iteration:  12% 13/106 [00:13<01:41,  1.09s/it]\u001b[A\n","Iteration:  13% 14/106 [00:15<02:03,  1.34s/it]\u001b[A\n","Iteration:  14% 15/106 [00:16<01:46,  1.17s/it]\u001b[A\n","Iteration:  15% 16/106 [00:17<01:30,  1.01s/it]\u001b[A\n","Iteration:  16% 17/106 [00:18<01:24,  1.06it/s]\u001b[A\n","Iteration:  17% 18/106 [00:19<01:26,  1.02it/s]\u001b[A\n","Iteration:  18% 19/106 [00:19<01:21,  1.07it/s]\u001b[A\n","Iteration:  19% 20/106 [00:20<01:12,  1.19it/s]\u001b[A\n","Iteration:  20% 21/106 [00:21<01:18,  1.09it/s]\u001b[A\n","Iteration:  21% 22/106 [00:22<01:13,  1.14it/s]\u001b[A\n","Iteration:  22% 23/106 [00:23<01:06,  1.26it/s]\u001b[A\n","Iteration:  23% 24/106 [00:23<01:06,  1.23it/s]\u001b[A\n","Iteration:  24% 25/106 [00:24<01:05,  1.23it/s]\u001b[A\n","Iteration:  25% 26/106 [00:25<01:06,  1.19it/s]\u001b[A\n","Iteration:  25% 27/106 [00:26<01:01,  1.29it/s]\u001b[A\n","Iteration:  26% 28/106 [00:27<01:04,  1.22it/s]\u001b[A\n","Iteration:  27% 29/106 [00:27<01:00,  1.28it/s]\u001b[A\n","Iteration:  28% 30/106 [00:28<01:04,  1.18it/s]\u001b[A\n","Iteration:  29% 31/106 [00:29<00:58,  1.28it/s]\u001b[A\n","Iteration:  30% 32/106 [00:30<00:58,  1.27it/s]\u001b[A\n","Iteration:  31% 33/106 [00:31<00:56,  1.28it/s]\u001b[A\n","Iteration:  32% 34/106 [00:32<01:00,  1.19it/s]\u001b[A\n","Iteration:  33% 35/106 [00:32<00:59,  1.19it/s]\u001b[A\n","Iteration:  34% 36/106 [00:33<01:02,  1.12it/s]\u001b[A\n","Iteration:  35% 37/106 [00:34<00:58,  1.17it/s]\u001b[A\n","Iteration:  36% 38/106 [00:35<00:57,  1.17it/s]\u001b[A\n","Iteration:  37% 39/106 [00:36<00:55,  1.20it/s]\u001b[A\n","Iteration:  38% 40/106 [00:36<00:51,  1.27it/s]\u001b[A\n","Iteration:  39% 41/106 [00:37<00:52,  1.25it/s]\u001b[A\n","Iteration:  40% 42/106 [00:38<00:50,  1.27it/s]\u001b[A\n","Iteration:  41% 43/106 [00:39<00:49,  1.26it/s]\u001b[A\n","Iteration:  42% 44/106 [00:40<00:51,  1.20it/s]\u001b[A\n","Iteration:  42% 45/106 [00:41<00:54,  1.11it/s]\u001b[A\n","Iteration:  43% 46/106 [00:42<00:58,  1.03it/s]\u001b[A\n","Iteration:  44% 47/106 [00:43<00:52,  1.13it/s]\u001b[A\n","Iteration:  45% 48/106 [00:44<00:50,  1.14it/s]\u001b[A\n","Iteration:  46% 49/106 [00:44<00:46,  1.24it/s]\u001b[A\n","Iteration:  47% 50/106 [00:46<00:54,  1.02it/s]\u001b[A\n","Iteration:  48% 51/106 [00:46<00:51,  1.06it/s]\u001b[A\n","Iteration:  49% 52/106 [00:47<00:53,  1.01it/s]\u001b[A\n","Iteration:  50% 53/106 [00:48<00:47,  1.12it/s]\u001b[A\n","Iteration:  51% 54/106 [00:49<00:46,  1.11it/s]\u001b[A\n","Iteration:  52% 55/106 [00:50<00:52,  1.03s/it]\u001b[A\n","Iteration:  53% 56/106 [00:51<00:46,  1.07it/s]\u001b[A\n","Iteration:  54% 57/106 [00:52<00:48,  1.02it/s]\u001b[A\n","Iteration:  55% 58/106 [00:53<00:47,  1.02it/s]\u001b[A\n","Iteration:  56% 59/106 [00:54<00:44,  1.06it/s]\u001b[A\n","Iteration:  57% 60/106 [00:56<00:50,  1.10s/it]\u001b[A\n","Iteration:  58% 61/106 [00:56<00:42,  1.05it/s]\u001b[A\n","Iteration:  58% 62/106 [00:57<00:36,  1.20it/s]\u001b[A\n","Iteration:  59% 63/106 [00:58<00:42,  1.02it/s]\u001b[A\n","Iteration:  60% 64/106 [00:59<00:37,  1.13it/s]\u001b[A\n","Iteration:  61% 65/106 [00:59<00:35,  1.16it/s]\u001b[A\n","Iteration:  62% 66/106 [01:00<00:33,  1.20it/s]\u001b[A\n","Iteration:  63% 67/106 [01:01<00:32,  1.21it/s]\u001b[A\n","Iteration:  64% 68/106 [01:02<00:34,  1.09it/s]\u001b[A\n","Iteration:  65% 69/106 [01:03<00:32,  1.13it/s]\u001b[A\n","Iteration:  66% 70/106 [01:04<00:31,  1.15it/s]\u001b[A\n","Iteration:  67% 71/106 [01:05<00:28,  1.22it/s]\u001b[A\n","Iteration:  68% 72/106 [01:05<00:29,  1.17it/s]\u001b[A\n","Iteration:  69% 73/106 [01:06<00:26,  1.24it/s]\u001b[A\n","Iteration:  70% 74/106 [01:07<00:25,  1.25it/s]\u001b[A\n","Iteration:  71% 75/106 [01:08<00:24,  1.26it/s]\u001b[A\n","Iteration:  72% 76/106 [01:09<00:24,  1.21it/s]\u001b[A\n","Iteration:  73% 77/106 [01:10<00:26,  1.10it/s]\u001b[A\n","Iteration:  74% 78/106 [01:11<00:24,  1.14it/s]\u001b[A\n","Iteration:  75% 79/106 [01:12<00:28,  1.04s/it]\u001b[A\n","Iteration:  75% 80/106 [01:13<00:29,  1.15s/it]\u001b[A\n","Iteration:  76% 81/106 [01:14<00:25,  1.02s/it]\u001b[A\n","Iteration:  77% 82/106 [01:15<00:22,  1.05it/s]\u001b[A\n","Iteration:  78% 83/106 [01:16<00:19,  1.15it/s]\u001b[A\n","Iteration:  79% 84/106 [01:17<00:21,  1.04it/s]\u001b[A\n","Iteration:  80% 85/106 [01:18<00:20,  1.02it/s]\u001b[A\n","Iteration:  81% 86/106 [01:19<00:18,  1.09it/s]\u001b[A\n","Iteration:  82% 87/106 [01:19<00:16,  1.16it/s]\u001b[A\n","Iteration:  83% 88/106 [01:20<00:15,  1.16it/s]\u001b[A\n","Iteration:  84% 89/106 [01:21<00:15,  1.07it/s]\u001b[A\n","Iteration:  85% 90/106 [01:22<00:16,  1.01s/it]\u001b[A\n","Iteration:  86% 91/106 [01:23<00:14,  1.04it/s]\u001b[A\n","Iteration:  87% 92/106 [01:24<00:13,  1.05it/s]\u001b[A\n","Iteration:  88% 93/106 [01:25<00:11,  1.16it/s]\u001b[A\n","Iteration:  89% 94/106 [01:26<00:09,  1.22it/s]\u001b[A\n","Iteration:  90% 95/106 [01:26<00:09,  1.20it/s]\u001b[A\n","Iteration:  91% 96/106 [01:27<00:08,  1.20it/s]\u001b[A\n","Iteration:  92% 97/106 [01:28<00:07,  1.27it/s]\u001b[A\n","Iteration:  92% 98/106 [01:29<00:06,  1.29it/s]\u001b[A\n","Iteration:  93% 99/106 [01:30<00:05,  1.23it/s]\u001b[A\n","Iteration:  94% 100/106 [01:31<00:05,  1.15it/s]\u001b[A\n","Iteration:  95% 101/106 [01:31<00:04,  1.16it/s]\u001b[A\n","Iteration:  96% 102/106 [01:33<00:03,  1.01it/s]\u001b[A\n","Iteration:  97% 103/106 [01:33<00:02,  1.10it/s]\u001b[A\n","Iteration:  98% 104/106 [01:34<00:01,  1.20it/s]\u001b[A\n","Iteration:  99% 105/106 [01:35<00:00,  1.08it/s]\u001b[A\n","Iteration: 100% 106/106 [01:36<00:00,  1.10it/s]\n","[2022-07-30 10:43:50,494][__main__][DEBUG] - epoch is 6\n","[2022-07-30 10:43:50,496][__main__][DEBUG] - validation loss is tensor(0.1051, device='cuda:0')\n","Epoch:   7% 7/100 [41:08<9:07:42, 353.36s/it]\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/106 [00:00<01:18,  1.34it/s]\u001b[A\n","Iteration:   2% 2/106 [00:01<01:25,  1.21it/s]\u001b[A\n","Iteration:   3% 3/106 [00:02<01:31,  1.12it/s]\u001b[A\n","Iteration:   4% 4/106 [00:03<01:23,  1.23it/s]\u001b[A\n","Iteration:   5% 5/106 [00:04<01:25,  1.18it/s]\u001b[A\n","Iteration:   6% 6/106 [00:04<01:19,  1.26it/s]\u001b[A\n","Iteration:   7% 7/106 [00:05<01:24,  1.17it/s]\u001b[A\n","Iteration:   8% 8/106 [00:07<01:33,  1.04it/s]\u001b[A\n","Iteration:   8% 9/106 [00:08<01:47,  1.10s/it]\u001b[A\n","Iteration:   9% 10/106 [00:09<01:46,  1.11s/it]\u001b[A\n","Iteration:  10% 11/106 [00:10<01:42,  1.07s/it]\u001b[A\n","Iteration:  11% 12/106 [00:11<01:31,  1.03it/s]\u001b[A\n","Iteration:  12% 13/106 [00:12<01:35,  1.03s/it]\u001b[A\n","Iteration:  13% 14/106 [00:13<01:26,  1.07it/s]\u001b[A\n","Iteration:  14% 15/106 [00:14<01:23,  1.09it/s]\u001b[A\n","Iteration:  15% 16/106 [00:15<01:23,  1.07it/s]\u001b[A\n","Iteration:  16% 17/106 [00:16<01:25,  1.04it/s]\u001b[A\n","Iteration:  17% 18/106 [00:17<01:25,  1.03it/s]\u001b[A\n","Iteration:  18% 19/106 [00:18<01:27,  1.01s/it]\u001b[A\n","Iteration:  19% 20/106 [00:18<01:20,  1.07it/s]\u001b[A\n","Iteration:  20% 21/106 [00:20<01:30,  1.07s/it]\u001b[A\n","Iteration:  21% 22/106 [00:21<01:26,  1.03s/it]\u001b[A\n","Iteration:  22% 23/106 [00:21<01:16,  1.08it/s]\u001b[A\n","Iteration:  23% 24/106 [00:22<01:14,  1.10it/s]\u001b[A\n","Iteration:  24% 25/106 [00:23<01:15,  1.07it/s]\u001b[A\n","Iteration:  25% 26/106 [00:24<01:07,  1.18it/s]\u001b[A\n","Iteration:  25% 27/106 [00:25<01:05,  1.20it/s]\u001b[A\n","Iteration:  26% 28/106 [00:26<01:20,  1.03s/it]\u001b[A\n","Iteration:  27% 29/106 [00:27<01:15,  1.02it/s]\u001b[A\n","Iteration:  28% 30/106 [00:28<01:12,  1.05it/s]\u001b[A\n","Iteration:  29% 31/106 [00:29<01:08,  1.09it/s]\u001b[A\n","Iteration:  30% 32/106 [00:30<01:15,  1.02s/it]\u001b[A\n","Iteration:  31% 33/106 [00:31<01:04,  1.12it/s]\u001b[A\n","Iteration:  32% 34/106 [00:32<01:07,  1.07it/s]\u001b[A\n","Iteration:  33% 35/106 [00:33<01:06,  1.07it/s]\u001b[A\n","Iteration:  34% 36/106 [00:33<01:00,  1.15it/s]\u001b[A\n","Iteration:  35% 37/106 [00:34<00:55,  1.24it/s]\u001b[A\n","Iteration:  36% 38/106 [00:35<00:55,  1.22it/s]\u001b[A\n","Iteration:  37% 39/106 [00:36<00:57,  1.17it/s]\u001b[A\n","Iteration:  38% 40/106 [00:37<01:09,  1.06s/it]\u001b[A\n","Iteration:  39% 41/106 [00:38<01:01,  1.06it/s]\u001b[A\n","Iteration:  40% 42/106 [00:39<00:58,  1.10it/s]\u001b[A\n","Iteration:  41% 43/106 [00:39<00:52,  1.20it/s]\u001b[A\n","Iteration:  42% 44/106 [00:40<00:52,  1.18it/s]\u001b[A\n","Iteration:  42% 45/106 [00:41<00:54,  1.11it/s]\u001b[A\n","Iteration:  43% 46/106 [00:42<00:50,  1.20it/s]\u001b[A\n","Iteration:  44% 47/106 [00:43<00:45,  1.30it/s]\u001b[A\n","Iteration:  45% 48/106 [00:43<00:43,  1.33it/s]\u001b[A\n","Iteration:  46% 49/106 [00:44<00:42,  1.33it/s]\u001b[A\n","Iteration:  47% 50/106 [00:45<00:42,  1.32it/s]\u001b[A\n","Iteration:  48% 51/106 [00:46<00:46,  1.19it/s]\u001b[A\n","Iteration:  49% 52/106 [00:47<00:52,  1.03it/s]\u001b[A\n","Iteration:  50% 53/106 [00:48<00:48,  1.09it/s]\u001b[A\n","Iteration:  51% 54/106 [00:49<00:43,  1.19it/s]\u001b[A\n","Iteration:  52% 55/106 [00:49<00:41,  1.23it/s]\u001b[A\n","Iteration:  53% 56/106 [00:51<00:45,  1.09it/s]\u001b[A\n","Iteration:  54% 57/106 [00:52<00:50,  1.03s/it]\u001b[A\n","Iteration:  55% 58/106 [00:53<00:49,  1.02s/it]\u001b[A\n","Iteration:  56% 59/106 [00:54<00:45,  1.03it/s]\u001b[A\n","Iteration:  57% 60/106 [00:55<00:44,  1.04it/s]\u001b[A\n","Iteration:  58% 61/106 [00:55<00:38,  1.16it/s]\u001b[A\n","Iteration:  58% 62/106 [00:56<00:36,  1.20it/s]\u001b[A\n","Iteration:  59% 63/106 [00:57<00:33,  1.28it/s]\u001b[A\n","Iteration:  60% 64/106 [00:57<00:30,  1.38it/s]\u001b[A\n","Iteration:  61% 65/106 [00:58<00:31,  1.29it/s]\u001b[A\n","Iteration:  62% 66/106 [00:59<00:32,  1.23it/s]\u001b[A\n","Iteration:  63% 67/106 [01:00<00:31,  1.22it/s]\u001b[A\n","Iteration:  64% 68/106 [01:01<00:31,  1.21it/s]\u001b[A\n","Iteration:  65% 69/106 [01:02<00:31,  1.18it/s]\u001b[A\n","Iteration:  66% 70/106 [01:03<00:32,  1.12it/s]\u001b[A\n","Iteration:  67% 71/106 [01:03<00:30,  1.16it/s]\u001b[A\n","Iteration:  68% 72/106 [01:04<00:26,  1.28it/s]\u001b[A\n","Iteration:  69% 73/106 [01:05<00:27,  1.21it/s]\u001b[A\n","Iteration:  70% 74/106 [01:06<00:24,  1.29it/s]\u001b[A\n","Iteration:  71% 75/106 [01:06<00:23,  1.34it/s]\u001b[A\n","Iteration:  72% 76/106 [01:07<00:23,  1.26it/s]\u001b[A\n","Iteration:  73% 77/106 [01:08<00:25,  1.14it/s]\u001b[A\n","Iteration:  74% 78/106 [01:09<00:26,  1.05it/s]\u001b[A\n","Iteration:  75% 79/106 [01:10<00:24,  1.12it/s]\u001b[A\n","Iteration:  75% 80/106 [01:11<00:24,  1.07it/s]\u001b[A\n","Iteration:  76% 81/106 [01:12<00:24,  1.04it/s]\u001b[A\n","Iteration:  77% 82/106 [01:13<00:20,  1.14it/s]\u001b[A\n","Iteration:  78% 83/106 [01:14<00:20,  1.12it/s]\u001b[A\n","Iteration:  79% 84/106 [01:15<00:19,  1.14it/s]\u001b[A\n","Iteration:  80% 85/106 [01:15<00:17,  1.19it/s]\u001b[A\n","Iteration:  81% 86/106 [01:16<00:16,  1.22it/s]\u001b[A\n","Iteration:  82% 87/106 [01:17<00:16,  1.17it/s]\u001b[A\n","Iteration:  83% 88/106 [01:18<00:15,  1.17it/s]\u001b[A\n","Iteration:  84% 89/106 [01:19<00:16,  1.02it/s]\u001b[A\n","Iteration:  85% 90/106 [01:20<00:15,  1.06it/s]\u001b[A\n","Iteration:  86% 91/106 [01:21<00:13,  1.14it/s]\u001b[A\n","Iteration:  87% 92/106 [01:22<00:11,  1.18it/s]\u001b[A\n","Iteration:  88% 93/106 [01:23<00:13,  1.05s/it]\u001b[A\n","Iteration:  89% 94/106 [01:24<00:10,  1.10it/s]\u001b[A\n","Iteration:  90% 95/106 [01:25<00:09,  1.16it/s]\u001b[A\n","Iteration:  91% 96/106 [01:25<00:08,  1.14it/s]\u001b[A\n","Iteration:  92% 97/106 [01:26<00:07,  1.19it/s]\u001b[A\n","Iteration:  92% 98/106 [01:28<00:08,  1.03s/it]\u001b[A\n","Iteration:  93% 99/106 [01:28<00:06,  1.06it/s]\u001b[A\n","Iteration:  94% 100/106 [01:30<00:06,  1.07s/it]\u001b[A\n","Iteration:  95% 101/106 [01:31<00:04,  1.00it/s]\u001b[A\n","Iteration:  96% 102/106 [01:31<00:03,  1.03it/s]\u001b[A\n","Iteration:  97% 103/106 [01:32<00:02,  1.10it/s]\u001b[A\n","Iteration:  98% 104/106 [01:33<00:01,  1.07it/s]\u001b[A\n","Iteration:  99% 105/106 [01:34<00:00,  1.08it/s]\u001b[A\n","Iteration: 100% 106/106 [01:35<00:00,  1.11it/s]\n","[2022-07-30 10:49:43,413][__main__][DEBUG] - epoch is 7\n","[2022-07-30 10:49:43,415][__main__][DEBUG] - validation loss is tensor(0.1048, device='cuda:0')\n","Epoch:   8% 8/100 [47:01<9:01:32, 353.18s/it]\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/106 [00:00<01:04,  1.63it/s]\u001b[A\n","Iteration:   2% 2/106 [00:01<01:17,  1.34it/s]\u001b[A\n","Iteration:   3% 3/106 [00:02<01:25,  1.21it/s]\u001b[A\n","Iteration:   4% 4/106 [00:03<01:18,  1.30it/s]\u001b[A\n","Iteration:   5% 5/106 [00:03<01:21,  1.24it/s]\u001b[A\n","Iteration:   6% 6/106 [00:04<01:26,  1.16it/s]\u001b[A\n","Iteration:   7% 7/106 [00:05<01:21,  1.21it/s]\u001b[A\n","Iteration:   8% 8/106 [00:06<01:25,  1.15it/s]\u001b[A\n","Iteration:   8% 9/106 [00:07<01:15,  1.29it/s]\u001b[A\n","Iteration:   9% 10/106 [00:08<01:35,  1.01it/s]\u001b[A\n","Iteration:  10% 11/106 [00:10<01:50,  1.16s/it]\u001b[A\n","Iteration:  11% 12/106 [00:10<01:38,  1.05s/it]\u001b[A\n","Iteration:  12% 13/106 [00:11<01:30,  1.03it/s]\u001b[A\n","Iteration:  13% 14/106 [00:13<01:37,  1.06s/it]\u001b[A\n","Iteration:  14% 15/106 [00:14<01:42,  1.12s/it]\u001b[A\n","Iteration:  15% 16/106 [00:15<01:52,  1.25s/it]\u001b[A\n","Iteration:  16% 17/106 [00:16<01:33,  1.05s/it]\u001b[A\n","Iteration:  17% 18/106 [00:17<01:27,  1.00it/s]\u001b[A\n","Iteration:  18% 19/106 [00:18<01:31,  1.06s/it]\u001b[A\n","Iteration:  19% 20/106 [00:19<01:23,  1.03it/s]\u001b[A\n","Iteration:  20% 21/106 [00:20<01:17,  1.09it/s]\u001b[A\n","Iteration:  21% 22/106 [00:20<01:12,  1.16it/s]\u001b[A\n","Iteration:  22% 23/106 [00:21<01:11,  1.16it/s]\u001b[A\n","Iteration:  23% 24/106 [00:22<01:10,  1.16it/s]\u001b[A\n","Iteration:  24% 25/106 [00:23<01:07,  1.21it/s]\u001b[A\n","Iteration:  25% 26/106 [00:24<01:19,  1.00it/s]\u001b[A\n","Iteration:  25% 27/106 [00:25<01:15,  1.04it/s]\u001b[A\n","Iteration:  26% 28/106 [00:26<01:21,  1.05s/it]\u001b[A\n","Iteration:  27% 29/106 [00:27<01:21,  1.06s/it]\u001b[A\n","Iteration:  28% 30/106 [00:28<01:13,  1.04it/s]\u001b[A\n","Iteration:  29% 31/106 [00:29<01:13,  1.03it/s]\u001b[A\n","Iteration:  30% 32/106 [00:30<01:05,  1.12it/s]\u001b[A\n","Iteration:  31% 33/106 [00:31<01:01,  1.18it/s]\u001b[A\n","Iteration:  32% 34/106 [00:32<01:05,  1.11it/s]\u001b[A\n","Iteration:  33% 35/106 [00:32<01:00,  1.17it/s]\u001b[A\n","Iteration:  34% 36/106 [00:33<00:59,  1.17it/s]\u001b[A\n","Iteration:  35% 37/106 [00:34<00:59,  1.16it/s]\u001b[A\n","Iteration:  36% 38/106 [00:35<00:56,  1.21it/s]\u001b[A\n","Iteration:  37% 39/106 [00:36<00:52,  1.27it/s]\u001b[A\n","Iteration:  38% 40/106 [00:36<00:53,  1.24it/s]\u001b[A\n","Iteration:  39% 41/106 [00:37<00:52,  1.23it/s]\u001b[A\n","Iteration:  40% 42/106 [00:38<00:51,  1.25it/s]\u001b[A\n","Iteration:  41% 43/106 [00:39<00:55,  1.13it/s]\u001b[A\n","Iteration:  42% 44/106 [00:40<00:52,  1.17it/s]\u001b[A\n","Iteration:  42% 45/106 [00:41<00:52,  1.17it/s]\u001b[A\n","Iteration:  43% 46/106 [00:41<00:47,  1.28it/s]\u001b[A\n","Iteration:  44% 47/106 [00:42<00:44,  1.33it/s]\u001b[A\n","Iteration:  45% 48/106 [00:43<00:46,  1.25it/s]\u001b[A\n","Iteration:  46% 49/106 [00:44<00:44,  1.29it/s]\u001b[A\n","Iteration:  47% 50/106 [00:46<01:05,  1.16s/it]\u001b[A\n","Iteration:  48% 51/106 [00:46<00:55,  1.01s/it]\u001b[A\n","Iteration:  49% 52/106 [00:47<00:51,  1.04it/s]\u001b[A\n","Iteration:  50% 53/106 [00:48<00:51,  1.04it/s]\u001b[A\n","Iteration:  51% 54/106 [00:49<00:47,  1.09it/s]\u001b[A\n","Iteration:  52% 55/106 [00:50<00:42,  1.21it/s]\u001b[A\n","Iteration:  53% 56/106 [00:50<00:42,  1.17it/s]\u001b[A\n","Iteration:  54% 57/106 [00:51<00:39,  1.25it/s]\u001b[A\n","Iteration:  55% 58/106 [00:52<00:40,  1.20it/s]\u001b[A\n","Iteration:  56% 59/106 [00:53<00:38,  1.21it/s]\u001b[A\n","Iteration:  57% 60/106 [00:54<00:35,  1.31it/s]\u001b[A\n","Iteration:  58% 61/106 [00:54<00:35,  1.28it/s]\u001b[A\n","Iteration:  58% 62/106 [00:55<00:37,  1.18it/s]\u001b[A\n","Iteration:  59% 63/106 [00:56<00:36,  1.16it/s]\u001b[A\n","Iteration:  60% 64/106 [00:57<00:36,  1.16it/s]\u001b[A\n","Iteration:  61% 65/106 [00:58<00:34,  1.19it/s]\u001b[A\n","Iteration:  62% 66/106 [00:59<00:38,  1.04it/s]\u001b[A\n","Iteration:  63% 67/106 [01:00<00:35,  1.11it/s]\u001b[A\n","Iteration:  64% 68/106 [01:01<00:32,  1.18it/s]\u001b[A\n","Iteration:  65% 69/106 [01:01<00:31,  1.17it/s]\u001b[A\n","Iteration:  66% 70/106 [01:02<00:31,  1.15it/s]\u001b[A\n","Iteration:  67% 71/106 [01:03<00:28,  1.23it/s]\u001b[A\n","Iteration:  68% 72/106 [01:04<00:26,  1.27it/s]\u001b[A\n","Iteration:  69% 73/106 [01:04<00:25,  1.31it/s]\u001b[A\n","Iteration:  70% 74/106 [01:05<00:23,  1.34it/s]\u001b[A\n","Iteration:  71% 75/106 [01:06<00:23,  1.33it/s]\u001b[A\n","Iteration:  72% 76/106 [01:08<00:32,  1.07s/it]\u001b[A\n","Iteration:  73% 77/106 [01:09<00:30,  1.07s/it]\u001b[A\n","Iteration:  74% 78/106 [01:09<00:26,  1.06it/s]\u001b[A\n","Iteration:  75% 79/106 [01:10<00:23,  1.13it/s]\u001b[A\n","Iteration:  75% 80/106 [01:11<00:24,  1.06it/s]\u001b[A\n","Iteration:  76% 81/106 [01:12<00:21,  1.14it/s]\u001b[A\n","Iteration:  77% 82/106 [01:13<00:22,  1.07it/s]\u001b[A\n","Iteration:  78% 83/106 [01:14<00:22,  1.02it/s]\u001b[A\n","Iteration:  79% 84/106 [01:15<00:22,  1.03s/it]\u001b[A\n","Iteration:  80% 85/106 [01:16<00:20,  1.01it/s]\u001b[A\n","Iteration:  81% 86/106 [01:17<00:17,  1.12it/s]\u001b[A\n","Iteration:  82% 87/106 [01:18<00:16,  1.14it/s]\u001b[A\n","Iteration:  83% 88/106 [01:19<00:15,  1.18it/s]\u001b[A\n","Iteration:  84% 89/106 [01:19<00:13,  1.22it/s]\u001b[A\n","Iteration:  85% 90/106 [01:20<00:13,  1.22it/s]\u001b[A\n","Iteration:  86% 91/106 [01:21<00:13,  1.11it/s]\u001b[A\n","Iteration:  87% 92/106 [01:22<00:12,  1.11it/s]\u001b[A\n","Iteration:  88% 93/106 [01:23<00:11,  1.14it/s]\u001b[A\n","Iteration:  89% 94/106 [01:24<00:10,  1.10it/s]\u001b[A\n","Iteration:  90% 95/106 [01:24<00:08,  1.23it/s]\u001b[A\n","Iteration:  91% 96/106 [01:25<00:08,  1.18it/s]\u001b[A\n","Iteration:  92% 97/106 [01:27<00:08,  1.07it/s]\u001b[A\n","Iteration:  92% 98/106 [01:27<00:06,  1.15it/s]\u001b[A\n","Iteration:  93% 99/106 [01:28<00:06,  1.05it/s]\u001b[A\n","Iteration:  94% 100/106 [01:29<00:05,  1.03it/s]\u001b[A\n","Iteration:  95% 101/106 [01:30<00:04,  1.02it/s]\u001b[A\n","Iteration:  96% 102/106 [01:31<00:04,  1.01s/it]\u001b[A\n","Iteration:  97% 103/106 [01:32<00:02,  1.06it/s]\u001b[A\n","Iteration:  98% 104/106 [01:33<00:02,  1.00s/it]\u001b[A\n","Iteration:  99% 105/106 [01:34<00:01,  1.00s/it]\u001b[A\n","Iteration: 100% 106/106 [01:35<00:00,  1.11it/s]\n","[2022-07-30 10:55:37,068][__main__][DEBUG] - epoch is 8\n","[2022-07-30 10:55:37,069][__main__][DEBUG] - validation loss is tensor(0.1052, device='cuda:0')\n","Epoch:   9% 9/100 [52:54<8:55:56, 353.37s/it]\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/106 [00:00<01:15,  1.39it/s]\u001b[A\n","Iteration:   2% 2/106 [00:01<01:18,  1.32it/s]\u001b[A\n","Iteration:   3% 3/106 [00:02<01:21,  1.26it/s]\u001b[A\n","Iteration:   4% 4/106 [00:03<01:19,  1.28it/s]\u001b[A\n","Iteration:   5% 5/106 [00:03<01:20,  1.25it/s]\u001b[A\n","Iteration:   6% 6/106 [00:05<01:30,  1.11it/s]\u001b[A\n","Iteration:   7% 7/106 [00:05<01:24,  1.17it/s]\u001b[A\n","Iteration:   8% 8/106 [00:07<01:36,  1.02it/s]\u001b[A\n","Iteration:   8% 9/106 [00:08<01:47,  1.10s/it]\u001b[A\n","Iteration:   9% 10/106 [00:09<01:40,  1.04s/it]\u001b[A\n","Iteration:  10% 11/106 [00:10<01:32,  1.02it/s]\u001b[A\n","Iteration:  11% 12/106 [00:11<01:29,  1.05it/s]\u001b[A\n","Iteration:  12% 13/106 [00:12<01:53,  1.22s/it]\u001b[A\n","Iteration:  13% 14/106 [00:13<01:40,  1.10s/it]\u001b[A\n","Iteration:  14% 15/106 [00:14<01:31,  1.01s/it]\u001b[A\n","Iteration:  15% 16/106 [00:15<01:33,  1.04s/it]\u001b[A\n","Iteration:  16% 17/106 [00:16<01:25,  1.05it/s]\u001b[A\n","Iteration:  17% 18/106 [00:17<01:24,  1.04it/s]\u001b[A\n","Iteration:  18% 19/106 [00:18<01:18,  1.10it/s]\u001b[A\n","Iteration:  19% 20/106 [00:19<01:20,  1.07it/s]\u001b[A\n","Iteration:  20% 21/106 [00:20<01:18,  1.08it/s]\u001b[A\n","Iteration:  21% 22/106 [00:20<01:15,  1.12it/s]\u001b[A\n","Iteration:  22% 23/106 [00:21<01:12,  1.15it/s]\u001b[A\n","Iteration:  23% 24/106 [00:22<01:15,  1.09it/s]\u001b[A\n","Iteration:  24% 25/106 [00:23<01:16,  1.06it/s]\u001b[A\n","Iteration:  25% 26/106 [00:24<01:12,  1.11it/s]\u001b[A\n","Iteration:  25% 27/106 [00:25<01:05,  1.21it/s]\u001b[A\n","Iteration:  26% 28/106 [00:26<01:05,  1.19it/s]\u001b[A\n","Iteration:  27% 29/106 [00:27<01:19,  1.03s/it]\u001b[A\n","Iteration:  28% 30/106 [00:28<01:14,  1.02it/s]\u001b[A\n","Iteration:  29% 31/106 [00:28<01:05,  1.15it/s]\u001b[A\n","Iteration:  30% 32/106 [00:29<01:01,  1.20it/s]\u001b[A\n","Iteration:  31% 33/106 [00:30<01:04,  1.14it/s]\u001b[A\n","Iteration:  32% 34/106 [00:31<01:00,  1.18it/s]\u001b[A\n","Iteration:  33% 35/106 [00:32<00:56,  1.26it/s]\u001b[A\n","Iteration:  34% 36/106 [00:33<00:57,  1.21it/s]\u001b[A\n","Iteration:  35% 37/106 [00:33<00:57,  1.19it/s]\u001b[A\n","Iteration:  36% 38/106 [00:34<01:01,  1.11it/s]\u001b[A\n","Iteration:  37% 39/106 [00:35<01:00,  1.11it/s]\u001b[A\n","Iteration:  38% 40/106 [00:36<00:56,  1.16it/s]\u001b[A\n","Iteration:  39% 41/106 [00:37<00:51,  1.26it/s]\u001b[A\n","Iteration:  40% 42/106 [00:38<01:00,  1.06it/s]\u001b[A\n","Iteration:  41% 43/106 [00:39<00:54,  1.15it/s]\u001b[A\n","Iteration:  42% 44/106 [00:40<00:57,  1.08it/s]\u001b[A\n","Iteration:  42% 45/106 [00:41<00:58,  1.05it/s]\u001b[A\n","Iteration:  43% 46/106 [00:42<00:58,  1.02it/s]\u001b[A\n","Iteration:  44% 47/106 [00:43<00:54,  1.09it/s]\u001b[A\n","Iteration:  45% 48/106 [00:44<00:54,  1.06it/s]\u001b[A\n","Iteration:  46% 49/106 [00:45<00:58,  1.03s/it]\u001b[A\n","Iteration:  47% 50/106 [00:46<00:58,  1.05s/it]\u001b[A\n","Iteration:  48% 51/106 [00:47<00:52,  1.05it/s]\u001b[A\n","Iteration:  49% 52/106 [00:48<00:50,  1.07it/s]\u001b[A\n","Iteration:  50% 53/106 [00:49<01:03,  1.20s/it]\u001b[A\n","Iteration:  51% 54/106 [00:51<01:03,  1.21s/it]\u001b[A\n","Iteration:  52% 55/106 [00:52<01:01,  1.20s/it]\u001b[A\n","Iteration:  53% 56/106 [00:53<00:57,  1.15s/it]\u001b[A\n","Iteration:  54% 57/106 [00:54<00:53,  1.10s/it]\u001b[A\n","Iteration:  55% 58/106 [00:55<00:50,  1.04s/it]\u001b[A\n","Iteration:  56% 59/106 [00:56<00:45,  1.04it/s]\u001b[A\n","Iteration:  57% 60/106 [00:57<00:44,  1.02it/s]\u001b[A\n","Iteration:  58% 61/106 [00:57<00:41,  1.09it/s]\u001b[A\n","Iteration:  58% 62/106 [00:58<00:40,  1.08it/s]\u001b[A\n","Iteration:  59% 63/106 [00:59<00:39,  1.10it/s]\u001b[A\n","Iteration:  60% 64/106 [01:00<00:39,  1.07it/s]\u001b[A\n","Iteration:  61% 65/106 [01:01<00:37,  1.08it/s]\u001b[A\n","Iteration:  62% 66/106 [01:02<00:33,  1.18it/s]\u001b[A\n","Iteration:  63% 67/106 [01:03<00:32,  1.19it/s]\u001b[A\n","Iteration:  64% 68/106 [01:03<00:31,  1.22it/s]\u001b[A\n","Iteration:  65% 69/106 [01:04<00:31,  1.17it/s]\u001b[A\n","Iteration:  66% 70/106 [01:05<00:28,  1.28it/s]\u001b[A\n","Iteration:  67% 71/106 [01:06<00:28,  1.22it/s]\u001b[A\n","Iteration:  68% 72/106 [01:07<00:32,  1.05it/s]\u001b[A\n","Iteration:  69% 73/106 [01:08<00:29,  1.13it/s]\u001b[A\n","Iteration:  70% 74/106 [01:09<00:33,  1.04s/it]\u001b[A\n","Iteration:  71% 75/106 [01:10<00:28,  1.07it/s]\u001b[A\n","Iteration:  72% 76/106 [01:11<00:28,  1.06it/s]\u001b[A\n","Iteration:  73% 77/106 [01:12<00:28,  1.03it/s]\u001b[A\n","Iteration:  74% 78/106 [01:13<00:25,  1.11it/s]\u001b[A\n","Iteration:  75% 79/106 [01:14<00:26,  1.02it/s]\u001b[A\n","Iteration:  75% 80/106 [01:14<00:23,  1.09it/s]\u001b[A\n","Iteration:  76% 81/106 [01:15<00:20,  1.19it/s]\u001b[A\n","Iteration:  77% 82/106 [01:16<00:21,  1.11it/s]\u001b[A\n","Iteration:  78% 83/106 [01:17<00:19,  1.20it/s]\u001b[A\n","Iteration:  79% 84/106 [01:18<00:17,  1.27it/s]\u001b[A\n","Iteration:  80% 85/106 [01:18<00:15,  1.34it/s]\u001b[A\n","Iteration:  81% 86/106 [01:19<00:15,  1.33it/s]\u001b[A\n","Iteration:  82% 87/106 [01:20<00:14,  1.35it/s]\u001b[A\n","Iteration:  83% 88/106 [01:20<00:13,  1.34it/s]\u001b[A\n","Iteration:  84% 89/106 [01:21<00:12,  1.34it/s]\u001b[A\n","Iteration:  85% 90/106 [01:22<00:12,  1.26it/s]\u001b[A\n","Iteration:  86% 91/106 [01:23<00:11,  1.29it/s]\u001b[A\n","Iteration:  87% 92/106 [01:24<00:11,  1.25it/s]\u001b[A\n","Iteration:  88% 93/106 [01:24<00:10,  1.26it/s]\u001b[A\n","Iteration:  89% 94/106 [01:25<00:09,  1.28it/s]\u001b[A\n","Iteration:  90% 95/106 [01:26<00:08,  1.26it/s]\u001b[A\n","Iteration:  91% 96/106 [01:27<00:07,  1.31it/s]\u001b[A\n","Iteration:  92% 97/106 [01:28<00:08,  1.11it/s]\u001b[A\n","Iteration:  92% 98/106 [01:29<00:06,  1.18it/s]\u001b[A\n","Iteration:  93% 99/106 [01:30<00:06,  1.17it/s]\u001b[A\n","Iteration:  94% 100/106 [01:30<00:05,  1.16it/s]\u001b[A\n","Iteration:  95% 101/106 [01:31<00:04,  1.19it/s]\u001b[A\n","Iteration:  96% 102/106 [01:32<00:03,  1.15it/s]\u001b[A\n","Iteration:  97% 103/106 [01:33<00:02,  1.25it/s]\u001b[A\n","Iteration:  98% 104/106 [01:34<00:01,  1.08it/s]\u001b[A\n","Iteration:  99% 105/106 [01:35<00:00,  1.20it/s]\u001b[A\n","Iteration: 100% 106/106 [01:35<00:00,  1.11it/s]\n","[2022-07-30 11:01:30,746][__main__][DEBUG] - epoch is 9\n","[2022-07-30 11:01:30,747][__main__][DEBUG] - validation loss is tensor(0.1096, device='cuda:0')\n","Epoch:  10% 10/100 [58:48<8:50:12, 353.48s/it]\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/106 [00:00<01:17,  1.35it/s]\u001b[A\n","Iteration:   2% 2/106 [00:01<01:30,  1.15it/s]\u001b[A\n","Iteration:   3% 3/106 [00:02<01:23,  1.24it/s]\u001b[A\n","Iteration:   4% 4/106 [00:03<01:23,  1.22it/s]\u001b[A\n","Iteration:   5% 5/106 [00:04<01:21,  1.24it/s]\u001b[A\n","Iteration:   6% 6/106 [00:04<01:18,  1.28it/s]\u001b[A\n","Iteration:   7% 7/106 [00:05<01:22,  1.20it/s]\u001b[A\n","Iteration:   8% 8/106 [00:07<01:38,  1.00s/it]\u001b[A\n","Iteration:   8% 9/106 [00:08<01:57,  1.22s/it]\u001b[A\n","Iteration:   9% 10/106 [00:09<01:52,  1.17s/it]\u001b[A\n","Iteration:  10% 11/106 [00:11<02:03,  1.30s/it]\u001b[A\n","Iteration:  11% 12/106 [00:11<01:39,  1.06s/it]\u001b[A\n","Iteration:  12% 13/106 [00:12<01:37,  1.04s/it]\u001b[A\n","Iteration:  13% 14/106 [00:14<01:38,  1.07s/it]\u001b[A\n","Iteration:  14% 15/106 [00:14<01:28,  1.03it/s]\u001b[A\n","Iteration:  15% 16/106 [00:15<01:21,  1.10it/s]\u001b[A\n","Iteration:  16% 17/106 [00:16<01:15,  1.18it/s]\u001b[A\n","Iteration:  17% 18/106 [00:17<01:17,  1.14it/s]\u001b[A\n","Iteration:  18% 19/106 [00:18<01:23,  1.04it/s]\u001b[A\n","Iteration:  19% 20/106 [00:19<01:21,  1.05it/s]\u001b[A\n","Iteration:  20% 21/106 [00:20<01:17,  1.09it/s]\u001b[A\n","Iteration:  21% 22/106 [00:21<01:20,  1.05it/s]\u001b[A\n","Iteration:  22% 23/106 [00:22<01:17,  1.07it/s]\u001b[A\n","Iteration:  23% 24/106 [00:22<01:11,  1.14it/s]\u001b[A\n","Iteration:  24% 25/106 [00:23<01:03,  1.27it/s]\u001b[A\n","Iteration:  25% 26/106 [00:24<01:02,  1.28it/s]\u001b[A\n","Iteration:  25% 27/106 [00:24<00:58,  1.34it/s]\u001b[A\n","Iteration:  26% 28/106 [00:25<01:06,  1.17it/s]\u001b[A\n","Iteration:  27% 29/106 [00:26<01:08,  1.12it/s]\u001b[A\n","Iteration:  28% 30/106 [00:27<01:08,  1.11it/s]\u001b[A\n","Iteration:  29% 31/106 [00:28<01:02,  1.20it/s]\u001b[A\n","Iteration:  30% 32/106 [00:29<01:02,  1.19it/s]\u001b[A\n","Iteration:  31% 33/106 [00:30<00:57,  1.26it/s]\u001b[A\n","Iteration:  32% 34/106 [00:30<00:58,  1.24it/s]\u001b[A\n","Iteration:  33% 35/106 [00:31<01:00,  1.18it/s]\u001b[A\n","Iteration:  34% 36/106 [00:32<00:59,  1.17it/s]\u001b[A\n","Iteration:  35% 37/106 [00:33<01:02,  1.10it/s]\u001b[A\n","Iteration:  36% 38/106 [00:34<01:02,  1.09it/s]\u001b[A\n","Iteration:  37% 39/106 [00:35<01:06,  1.00it/s]\u001b[A\n","Iteration:  38% 40/106 [00:36<01:01,  1.07it/s]\u001b[A\n","Iteration:  39% 41/106 [00:37<00:59,  1.09it/s]\u001b[A\n","Iteration:  40% 42/106 [00:38<00:53,  1.20it/s]\u001b[A\n","Iteration:  41% 43/106 [00:38<00:51,  1.23it/s]\u001b[A\n","Iteration:  42% 44/106 [00:39<00:49,  1.25it/s]\u001b[A\n","Iteration:  42% 45/106 [00:40<00:52,  1.15it/s]\u001b[A\n","Iteration:  43% 46/106 [00:42<01:01,  1.03s/it]\u001b[A\n","Iteration:  44% 47/106 [00:43<01:01,  1.04s/it]\u001b[A\n","Iteration:  45% 48/106 [00:44<01:02,  1.08s/it]\u001b[A\n","Iteration:  46% 49/106 [00:45<00:56,  1.00it/s]\u001b[A\n","Iteration:  47% 50/106 [00:46<00:53,  1.04it/s]\u001b[A\n","Iteration:  48% 51/106 [00:46<00:49,  1.11it/s]\u001b[A\n","Iteration:  49% 52/106 [00:47<00:46,  1.15it/s]\u001b[A\n","Iteration:  50% 53/106 [00:48<00:50,  1.06it/s]\u001b[A\n","Iteration:  51% 54/106 [00:49<00:46,  1.13it/s]\u001b[A\n","Iteration:  52% 55/106 [00:50<00:45,  1.11it/s]\u001b[A\n","Iteration:  53% 56/106 [00:51<00:51,  1.03s/it]\u001b[A\n","Iteration:  54% 57/106 [00:52<00:44,  1.10it/s]\u001b[A\n","Iteration:  55% 58/106 [00:52<00:39,  1.23it/s]\u001b[A\n","Iteration:  56% 59/106 [00:53<00:35,  1.32it/s]\u001b[A\n","Iteration:  57% 60/106 [00:54<00:35,  1.28it/s]\u001b[A\n","Iteration:  58% 61/106 [00:55<00:33,  1.33it/s]\u001b[A\n","Iteration:  58% 62/106 [00:57<00:51,  1.18s/it]\u001b[A\n","Iteration:  59% 63/106 [00:58<00:46,  1.08s/it]\u001b[A\n","Iteration:  60% 64/106 [00:59<00:45,  1.09s/it]\u001b[A\n","Iteration:  61% 65/106 [01:00<00:42,  1.04s/it]\u001b[A\n","Iteration:  62% 66/106 [01:00<00:36,  1.10it/s]\u001b[A\n","Iteration:  63% 67/106 [01:01<00:38,  1.01it/s]\u001b[A\n","Iteration:  64% 68/106 [01:03<00:43,  1.15s/it]\u001b[A\n","Iteration:  65% 69/106 [01:04<00:41,  1.13s/it]\u001b[A\n","Iteration:  66% 70/106 [01:05<00:35,  1.01it/s]\u001b[A\n","Iteration:  67% 71/106 [01:06<00:35,  1.00s/it]\u001b[A\n","Iteration:  68% 72/106 [01:06<00:30,  1.12it/s]\u001b[A\n","Iteration:  69% 73/106 [01:07<00:28,  1.17it/s]\u001b[A\n","Iteration:  70% 74/106 [01:08<00:27,  1.14it/s]\u001b[A\n","Iteration:  71% 75/106 [01:09<00:25,  1.20it/s]\u001b[A\n","Iteration:  72% 76/106 [01:10<00:25,  1.18it/s]\u001b[A\n","Iteration:  73% 77/106 [01:11<00:24,  1.21it/s]\u001b[A\n","Iteration:  74% 78/106 [01:11<00:23,  1.21it/s]\u001b[A\n","Iteration:  75% 79/106 [01:12<00:25,  1.08it/s]\u001b[A\n","Iteration:  75% 80/106 [01:13<00:22,  1.17it/s]\u001b[A\n","Iteration:  76% 81/106 [01:14<00:20,  1.20it/s]\u001b[A\n","Iteration:  77% 82/106 [01:15<00:18,  1.27it/s]\u001b[A\n","Iteration:  78% 83/106 [01:16<00:19,  1.19it/s]\u001b[A\n","Iteration:  79% 84/106 [01:16<00:18,  1.22it/s]\u001b[A\n","Iteration:  80% 85/106 [01:17<00:17,  1.22it/s]\u001b[A\n","Iteration:  81% 86/106 [01:18<00:15,  1.28it/s]\u001b[A\n","Iteration:  82% 87/106 [01:19<00:17,  1.09it/s]\u001b[A\n","Iteration:  83% 88/106 [01:20<00:16,  1.11it/s]\u001b[A\n","Iteration:  84% 89/106 [01:21<00:14,  1.21it/s]\u001b[A\n","Iteration:  85% 90/106 [01:22<00:13,  1.16it/s]\u001b[A\n","Iteration:  86% 91/106 [01:22<00:12,  1.21it/s]\u001b[A\n","Iteration:  87% 92/106 [01:23<00:11,  1.23it/s]\u001b[A\n","Iteration:  88% 93/106 [01:24<00:09,  1.31it/s]\u001b[A\n","Iteration:  89% 94/106 [01:25<00:09,  1.24it/s]\u001b[A\n","Iteration:  90% 95/106 [01:26<00:09,  1.14it/s]\u001b[A\n","Iteration:  91% 96/106 [01:26<00:08,  1.25it/s]\u001b[A\n","Iteration:  92% 97/106 [01:27<00:06,  1.31it/s]\u001b[A\n","Iteration:  92% 98/106 [01:28<00:05,  1.33it/s]\u001b[A\n","Iteration:  93% 99/106 [01:29<00:05,  1.29it/s]\u001b[A\n","Iteration:  94% 100/106 [01:30<00:05,  1.09it/s]\u001b[A\n","Iteration:  95% 101/106 [01:31<00:05,  1.02s/it]\u001b[A\n","Iteration:  96% 102/106 [01:32<00:03,  1.03it/s]\u001b[A\n","Iteration:  97% 103/106 [01:33<00:02,  1.13it/s]\u001b[A\n","Iteration:  98% 104/106 [01:33<00:01,  1.21it/s]\u001b[A\n","Iteration:  99% 105/106 [01:34<00:00,  1.22it/s]\u001b[A\n","Iteration: 100% 106/106 [01:35<00:00,  1.11it/s]\n","[2022-07-30 11:07:25,588][__main__][DEBUG] - epoch is 10\n","[2022-07-30 11:07:25,589][__main__][DEBUG] - validation loss is tensor(0.1089, device='cuda:0')\n","Epoch:  11% 11/100 [1:04:43<8:44:55, 353.88s/it]\n","Iteration:   0% 0/106 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/106 [00:00<01:21,  1.29it/s]\u001b[A\n","Iteration:   2% 2/106 [00:01<01:45,  1.01s/it]\u001b[A\n","Iteration:   3% 3/106 [00:02<01:34,  1.09it/s]\u001b[A\n","Iteration:   4% 4/106 [00:03<01:21,  1.26it/s]\u001b[A\n","Iteration:   5% 5/106 [00:03<01:14,  1.36it/s]\u001b[A\n","Iteration:   6% 6/106 [00:04<01:14,  1.34it/s]\u001b[A\n","Iteration:   7% 7/106 [00:05<01:16,  1.30it/s]\u001b[A\n","Iteration:   8% 8/106 [00:06<01:09,  1.40it/s]\u001b[A\n","Iteration:   8% 9/106 [00:07<01:15,  1.28it/s]\u001b[A\n","Iteration:   9% 10/106 [00:08<01:34,  1.02it/s]\u001b[A\n","Iteration:  10% 11/106 [00:09<01:45,  1.12s/it]\u001b[A\n","Iteration:  11% 12/106 [00:10<01:39,  1.06s/it]\u001b[A\n","Iteration:  12% 13/106 [00:12<01:55,  1.24s/it]\u001b[A\n","Iteration:  13% 14/106 [00:13<01:44,  1.13s/it]\u001b[A\n","Iteration:  14% 15/106 [00:14<01:44,  1.15s/it]\u001b[A\n","Iteration:  15% 16/106 [00:15<01:34,  1.05s/it]\u001b[A\n","Iteration:  16% 17/106 [00:16<01:25,  1.05it/s]\u001b[A\n","Iteration:  17% 18/106 [00:16<01:18,  1.12it/s]\u001b[A\n","Iteration:  18% 19/106 [00:18<01:24,  1.03it/s]\u001b[A\n","Iteration:  19% 20/106 [00:18<01:17,  1.11it/s]\u001b[A\n","Iteration:  20% 21/106 [00:19<01:15,  1.13it/s]\u001b[A\n","Iteration:  21% 22/106 [00:20<01:15,  1.12it/s]\u001b[A\n","Iteration:  22% 23/106 [00:21<01:16,  1.08it/s]\u001b[A\n","Iteration:  23% 24/106 [00:22<01:17,  1.06it/s]\u001b[A\n","Iteration:  24% 25/106 [00:23<01:07,  1.20it/s]\u001b[A\n","Iteration:  25% 26/106 [00:24<01:10,  1.14it/s]\u001b[A\n","Iteration:  25% 27/106 [00:24<01:05,  1.21it/s]\u001b[A\n","Iteration:  26% 28/106 [00:26<01:13,  1.06it/s]\u001b[A\n","Iteration:  27% 29/106 [00:26<01:05,  1.17it/s]\u001b[A\n","Iteration:  28% 30/106 [00:27<01:06,  1.14it/s]\u001b[A\n","Iteration:  29% 31/106 [00:28<01:00,  1.24it/s]\u001b[A\n","Iteration:  30% 32/106 [00:29<01:00,  1.23it/s]\u001b[A\n","Iteration:  31% 33/106 [00:29<00:56,  1.30it/s]\u001b[A\n","Iteration:  32% 34/106 [00:30<00:52,  1.37it/s]\u001b[A\n","Iteration:  33% 35/106 [00:31<00:50,  1.42it/s]\u001b[A\n","Iteration:  34% 36/106 [00:31<00:52,  1.34it/s]\u001b[A\n","Iteration:  35% 37/106 [00:32<00:47,  1.46it/s]\u001b[A\n","Iteration:  36% 38/106 [00:33<00:51,  1.32it/s]\u001b[A\n","Iteration:  37% 39/106 [00:34<00:57,  1.17it/s]\u001b[A\n","Iteration:  38% 40/106 [00:35<00:56,  1.17it/s]\u001b[A\n","Iteration:  39% 41/106 [00:35<00:52,  1.24it/s]\u001b[A\n","Iteration:  40% 42/106 [00:36<00:49,  1.28it/s]\u001b[A\n","Iteration:  41% 43/106 [00:37<00:49,  1.27it/s]\u001b[A\n","Iteration:  42% 44/106 [00:38<00:51,  1.20it/s]\u001b[A\n","Iteration:  42% 45/106 [00:39<00:48,  1.26it/s]\u001b[A\n","Iteration:  43% 46/106 [00:39<00:48,  1.25it/s]\u001b[A\n","Iteration:  44% 47/106 [00:40<00:51,  1.15it/s]\u001b[A\n","Iteration:  45% 48/106 [00:41<00:51,  1.12it/s]\u001b[A\n","Iteration:  46% 49/106 [00:43<00:57,  1.01s/it]\u001b[A\n","Iteration:  47% 50/106 [00:44<00:57,  1.03s/it]\u001b[A\n","Iteration:  48% 51/106 [00:45<00:58,  1.07s/it]\u001b[A\n","Iteration:  49% 52/106 [00:46<00:50,  1.06it/s]\u001b[A\n","Iteration:  50% 53/106 [00:47<00:50,  1.05it/s]\u001b[A\n","Iteration:  51% 54/106 [00:48<00:51,  1.02it/s]\u001b[A\n","Iteration:  52% 55/106 [00:48<00:45,  1.12it/s]\u001b[A\n","Iteration:  53% 56/106 [00:49<00:43,  1.15it/s]\u001b[A\n","Iteration:  54% 57/106 [00:50<00:45,  1.07it/s]\u001b[A\n","Iteration:  55% 58/106 [00:51<00:41,  1.17it/s]\u001b[A\n","Iteration:  56% 59/106 [00:52<00:41,  1.12it/s]\u001b[A\n","Iteration:  57% 60/106 [00:53<00:42,  1.09it/s]\u001b[A\n","Iteration:  58% 61/106 [00:54<00:39,  1.13it/s]\u001b[A\n","Iteration:  58% 62/106 [00:54<00:36,  1.19it/s]\u001b[A\n","Iteration:  59% 63/106 [00:55<00:35,  1.20it/s]\u001b[A\n","Iteration:  60% 64/106 [00:56<00:37,  1.13it/s]\u001b[A\n","Iteration:  61% 65/106 [00:57<00:41,  1.00s/it]\u001b[A\n","Iteration:  62% 66/106 [00:58<00:40,  1.00s/it]\u001b[A\n","Iteration:  63% 67/106 [00:59<00:36,  1.07it/s]\u001b[A\n","Iteration:  64% 68/106 [01:00<00:34,  1.10it/s]\u001b[A\n","Iteration:  65% 69/106 [01:01<00:32,  1.15it/s]\u001b[A\n","Iteration:  66% 70/106 [01:02<00:33,  1.07it/s]\u001b[A\n","Iteration:  67% 71/106 [01:03<00:31,  1.11it/s]\u001b[A\n","Iteration:  68% 72/106 [01:03<00:27,  1.25it/s]\u001b[A\n","Iteration:  69% 73/106 [01:04<00:25,  1.27it/s]\u001b[A\n","Iteration:  70% 74/106 [01:05<00:23,  1.35it/s]\u001b[A\n","Iteration:  71% 75/106 [01:06<00:26,  1.18it/s]\u001b[A\n","Iteration:  72% 76/106 [01:07<00:24,  1.21it/s]\u001b[A\n","Iteration:  73% 77/106 [01:07<00:22,  1.31it/s]\u001b[A\n","Iteration:  74% 78/106 [01:08<00:20,  1.35it/s]\u001b[A\n","Iteration:  75% 79/106 [01:09<00:20,  1.33it/s]\u001b[A\n","Iteration:  75% 80/106 [01:10<00:21,  1.20it/s]\u001b[A\n","Iteration:  76% 81/106 [01:11<00:25,  1.01s/it]\u001b[A\n","Iteration:  77% 82/106 [01:12<00:23,  1.01it/s]\u001b[A\n","Iteration:  78% 83/106 [01:13<00:23,  1.02s/it]\u001b[A\n","Iteration:  79% 84/106 [01:15<00:26,  1.20s/it]\u001b[A\n","Iteration:  80% 85/106 [01:15<00:21,  1.02s/it]\u001b[A\n","Iteration:  81% 86/106 [01:16<00:18,  1.07it/s]\u001b[A\n","Iteration:  82% 87/106 [01:17<00:17,  1.11it/s]\u001b[A\n","Iteration:  83% 88/106 [01:18<00:16,  1.09it/s]\u001b[A\n","Iteration:  84% 89/106 [01:19<00:15,  1.10it/s]\u001b[A\n","Iteration:  85% 90/106 [01:20<00:14,  1.09it/s]\u001b[A\n","Iteration:  86% 91/106 [01:21<00:13,  1.10it/s]\u001b[A\n","Iteration:  87% 92/106 [01:21<00:12,  1.14it/s]\u001b[A\n","Iteration:  88% 93/106 [01:23<00:12,  1.06it/s]\u001b[A\n","Iteration:  89% 94/106 [01:23<00:10,  1.17it/s]\u001b[A\n","Iteration:  90% 95/106 [01:24<00:08,  1.23it/s]\u001b[A\n","Iteration:  91% 96/106 [01:25<00:08,  1.23it/s]\u001b[A\n","Iteration:  92% 97/106 [01:26<00:08,  1.04it/s]\u001b[A\n","Iteration:  92% 98/106 [01:27<00:08,  1.08s/it]\u001b[A\n","Iteration:  93% 99/106 [01:28<00:07,  1.00s/it]\u001b[A\n","Iteration:  94% 100/106 [01:29<00:05,  1.10it/s]\u001b[A\n","Iteration:  95% 101/106 [01:30<00:04,  1.19it/s]\u001b[A\n","Iteration:  96% 102/106 [01:32<00:04,  1.21s/it]\u001b[A\n","Iteration:  97% 103/106 [01:33<00:03,  1.15s/it]\u001b[A\n","Iteration:  98% 104/106 [01:34<00:02,  1.08s/it]\u001b[A\n","Iteration:  99% 105/106 [01:35<00:01,  1.09s/it]\u001b[A\n","Iteration: 100% 106/106 [01:35<00:00,  1.11it/s]\n"]}]},{"cell_type":"code","source":["!HYDRA_FULL_ERROR=1 python eval.py hydra.job.chdir=False hydra.verbose=True"],"metadata":{"id":"w7sdHHb0uFV-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659181522896,"user_tz":-540,"elapsed":1317997,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"7aba5895-6753-47ab-9578-aea215ee2e5e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using backend: pytorch\n","eval.py:20: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"config\", config_name='config')\n","/usr/local/lib/python3.7/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n","  warnings.warn(msg, UserWarning)\n","[2022-07-30 11:21:17,087][HYDRA] Hydra 1.2.0\n","[2022-07-30 11:21:17,087][HYDRA] ===========\n","[2022-07-30 11:21:17,088][HYDRA] Installed Hydra Plugins\n","[2022-07-30 11:21:17,088][HYDRA] ***********************\n","[2022-07-30 11:21:17,088][HYDRA] \tConfigSource:\n","[2022-07-30 11:21:17,088][HYDRA] \t-------------\n","[2022-07-30 11:21:17,088][HYDRA] \t\tFileConfigSource\n","[2022-07-30 11:21:17,088][HYDRA] \t\tImportlibResourcesConfigSource\n","[2022-07-30 11:21:17,088][HYDRA] \t\tStructuredConfigSource\n","[2022-07-30 11:21:17,088][HYDRA] \tCompletionPlugin:\n","[2022-07-30 11:21:17,088][HYDRA] \t-----------------\n","[2022-07-30 11:21:17,088][HYDRA] \t\tBashCompletion\n","[2022-07-30 11:21:17,088][HYDRA] \t\tFishCompletion\n","[2022-07-30 11:21:17,088][HYDRA] \t\tZshCompletion\n","[2022-07-30 11:21:17,088][HYDRA] \tLauncher:\n","[2022-07-30 11:21:17,088][HYDRA] \t---------\n","[2022-07-30 11:21:17,088][HYDRA] \t\tBasicLauncher\n","[2022-07-30 11:21:17,088][HYDRA] \tSweeper:\n","[2022-07-30 11:21:17,088][HYDRA] \t--------\n","[2022-07-30 11:21:17,088][HYDRA] \t\tBasicSweeper\n","[2022-07-30 11:21:17,088][HYDRA] \n","[2022-07-30 11:21:17,088][HYDRA] Config search path\n","[2022-07-30 11:21:17,089][HYDRA] ******************\n","[2022-07-30 11:21:17,248][HYDRA] | Provider | Search path                                                           |\n","[2022-07-30 11:21:17,248][HYDRA] ------------------------------------------------------------------------------------\n","[2022-07-30 11:21:17,248][HYDRA] | hydra    | pkg://hydra.conf                                                      |\n","[2022-07-30 11:21:17,248][HYDRA] | main     | file:///content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp/config |\n","[2022-07-30 11:21:17,248][HYDRA] | schema   | structured://                                                         |\n","[2022-07-30 11:21:17,248][HYDRA] ------------------------------------------------------------------------------------\n","[2022-07-30 11:21:17,341][HYDRA] \n","[2022-07-30 11:21:17,341][HYDRA] Defaults Tree\n","[2022-07-30 11:21:17,341][HYDRA] *************\n","[2022-07-30 11:21:17,342][HYDRA] <root>:\n","[2022-07-30 11:21:17,342][HYDRA]   hydra/config:\n","[2022-07-30 11:21:17,342][HYDRA]     hydra/output: default\n","[2022-07-30 11:21:17,342][HYDRA]     hydra/launcher: basic\n","[2022-07-30 11:21:17,342][HYDRA]     hydra/sweeper: basic\n","[2022-07-30 11:21:17,342][HYDRA]     hydra/help: default\n","[2022-07-30 11:21:17,342][HYDRA]     hydra/hydra_help: default\n","[2022-07-30 11:21:17,342][HYDRA]     hydra/hydra_logging: default\n","[2022-07-30 11:21:17,342][HYDRA]     hydra/job_logging: default\n","[2022-07-30 11:21:17,342][HYDRA]     hydra/callbacks: null\n","[2022-07-30 11:21:17,342][HYDRA]     hydra/env: default\n","[2022-07-30 11:21:17,343][HYDRA]     _self_\n","[2022-07-30 11:21:17,343][HYDRA]   config:\n","[2022-07-30 11:21:17,343][HYDRA]     data/FB15kET\n","[2022-07-30 11:21:17,343][HYDRA]     model/T5\n","[2022-07-30 11:21:17,343][HYDRA]     _self_\n","[2022-07-30 11:21:17,440][HYDRA] \n","[2022-07-30 11:21:17,440][HYDRA] Defaults List\n","[2022-07-30 11:21:17,440][HYDRA] *************\n","[2022-07-30 11:21:17,440][HYDRA] | Config path                 | Package             | _self_ | Parent       | \n","[2022-07-30 11:21:17,440][HYDRA] ------------------------------------------------------------------------------\n","[2022-07-30 11:21:17,440][HYDRA] | hydra/output/default        | hydra               | False  | hydra/config |\n","[2022-07-30 11:21:17,440][HYDRA] | hydra/launcher/basic        | hydra.launcher      | False  | hydra/config |\n","[2022-07-30 11:21:17,440][HYDRA] | hydra/sweeper/basic         | hydra.sweeper       | False  | hydra/config |\n","[2022-07-30 11:21:17,440][HYDRA] | hydra/help/default          | hydra.help          | False  | hydra/config |\n","[2022-07-30 11:21:17,440][HYDRA] | hydra/hydra_help/default    | hydra.hydra_help    | False  | hydra/config |\n","[2022-07-30 11:21:17,440][HYDRA] | hydra/hydra_logging/default | hydra.hydra_logging | False  | hydra/config |\n","[2022-07-30 11:21:17,440][HYDRA] | hydra/job_logging/default   | hydra.job_logging   | False  | hydra/config |\n","[2022-07-30 11:21:17,440][HYDRA] | hydra/env/default           | hydra.env           | False  | hydra/config |\n","[2022-07-30 11:21:17,440][HYDRA] | hydra/config                | hydra               | True   | <root>       |\n","[2022-07-30 11:21:17,440][HYDRA] | data/FB15kET                | data                | False  | config       |\n","[2022-07-30 11:21:17,440][HYDRA] | model/T5                    | model               | False  | config       |\n","[2022-07-30 11:21:17,440][HYDRA] | config                      |                     | True   | <root>       |\n","[2022-07-30 11:21:17,441][HYDRA] ------------------------------------------------------------------------------\n","[2022-07-30 11:21:17,591][HYDRA] Config\n","[2022-07-30 11:21:17,591][HYDRA] ******\n","[2022-07-30 11:21:17,596][HYDRA] data:\n","  name: FB15kET\n","  data_dir: data\n","  overwrite: false\n","  is_unigraph: false\n","  change_mid_to_name: true\n","  lowest_level: true\n","  remove_under_score: true\n","model:\n","  cuda: true\n","  debug: false\n","  save_dir: save\n","  name: T5\n","  train_dataset: ET_1_1_train.txt\n","  pretrained_model: t5-base\n","  append_another_bos: false\n","  append_sep_token: true\n","  is_in_edge: true\n","  is_out_edge: true\n","  max_epoch: 100\n","  train_batch_size: 8\n","  valid_batch_size: 8\n","  test_batch_size: 8\n","  num_workers: 0\n","  temperature: 0.5\n","  repetition_penalty: 1.5\n","  n_gpus: 1\n","  max_input_length: 256\n","  max_output_length: 128\n","  gradient_accumulation_steps: 1\n","  max_grad_norm: 1.0\n","  early_stopping: true\n","  optimizer:\n","    algorithm: Adam\n","    learning_rate: 0.0001\n","  valid:\n","    valid_dataset: ET_1_1_valid.txt\n","    valid_epoch: 1\n","    num_beams: 5\n","    length_penalty: 1.0\n","    max_output_length: 128\n","    clean_up_spaces: true\n","    save_outputs: true\n","    save_one_batch: true\n","  test:\n","    test_dataset: ET_1_1_test.txt\n","    save_outputs: true\n","preprocess:\n","  load_ET: false\n","  load_KG: true\n","  neighbor_sampling: true\n","  neighbor_num: 10\n","  num_layers: 1\n","\n","[2022-07-30 11:21:17,680][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): s3.amazonaws.com:443\n","2022-07-30 11:21:17,680 DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n","[2022-07-30 11:21:17,808][urllib3.connectionpool][DEBUG] - https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/t5-spiece.model HTTP/1.1\" 200 0\n","2022-07-30 11:21:17,808 DEBUG    https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/t5-spiece.model HTTP/1.1\" 200 0\n","[2022-07-30 11:21:17,810][transformers.tokenization_utils_base][INFO] - loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n","2022-07-30 11:21:17,810 INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/t5-spiece.model from cache at /root/.cache/torch/transformers/68f1b8dbca4350743bb54b8c4169fd38cbabaad564f85a9239337a8d0342af9f.9995af32582a1a7062cb3173c118cb7b4636fa03feb967340f20fc37406f021f\n","[2022-07-30 11:21:26,925][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): s3.amazonaws.com:443\n","2022-07-30 11:21:26,925 DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n","[2022-07-30 11:21:27,049][urllib3.connectionpool][DEBUG] - https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/t5-base-config.json HTTP/1.1\" 200 0\n","2022-07-30 11:21:27,049 DEBUG    https://s3.amazonaws.com:443 \"HEAD /models.huggingface.co/bert/t5-base-config.json HTTP/1.1\" 200 0\n","[2022-07-30 11:21:27,051][transformers.configuration_utils][INFO] - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json from cache at /root/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b\n","2022-07-30 11:21:27,051 INFO     loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json from cache at /root/.cache/torch/transformers/40578967d1f029acb6162b36db9d8b4307063e885990ccd297c2c5be1cf1b3d7.2995d650f5eba18c8baa4146e210d32d56165e90d374281741fc78b872cd6c9b\n","[2022-07-30 11:21:27,052][transformers.configuration_utils][INFO] - Model config T5Config {\n","  \"architectures\": [\n","    \"T5WithLMHeadModel\"\n","  ],\n","  \"d_ff\": 3072,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"vocab_size\": 32128\n","}\n","\n","2022-07-30 11:21:27,052 INFO     Model config T5Config {\n","  \"architectures\": [\n","    \"T5WithLMHeadModel\"\n","  ],\n","  \"d_ff\": 3072,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"vocab_size\": 32128\n","}\n","\n","[2022-07-30 11:21:27,054][urllib3.connectionpool][DEBUG] - Starting new HTTPS connection (1): cdn.huggingface.co:443\n","2022-07-30 11:21:27,054 DEBUG    Starting new HTTPS connection (1): cdn.huggingface.co:443\n","[2022-07-30 11:21:27,180][urllib3.connectionpool][DEBUG] - https://cdn.huggingface.co:443 \"HEAD /t5-base-pytorch_model.bin HTTP/1.1\" 200 0\n","2022-07-30 11:21:27,180 DEBUG    https://cdn.huggingface.co:443 \"HEAD /t5-base-pytorch_model.bin HTTP/1.1\" 200 0\n","[2022-07-30 11:21:27,182][transformers.modeling_utils][INFO] - loading weights file https://cdn.huggingface.co/t5-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa\n","2022-07-30 11:21:27,182 INFO     loading weights file https://cdn.huggingface.co/t5-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/f6f2fde9fa7611f4eff74620de9cbe734e7a717b5b143bd283cae4c2d6022990.54f906ff53bd09195cfc183a29cadc81b7705f07fcdb796d24163cb632b6bdfa\n","[2022-07-30 11:21:34,060][transformers.modeling_utils][INFO] - All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n","\n","2022-07-30 11:21:34,060 INFO     All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n","\n","[2022-07-30 11:21:34,061][transformers.modeling_utils][WARNING] - Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","2022-07-30 11:21:34,061 WARNING  Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[2022-07-30 11:21:38,216][root][DEBUG] - Parameter model.shared.weight: torch.Size([32128, 768]), require_grad=True\n","2022-07-30 11:21:38,216 DEBUG    Parameter model.shared.weight: torch.Size([32128, 768]), require_grad=True\n","[2022-07-30 11:21:38,217][root][DEBUG] - Parameter model.encoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,217 DEBUG    Parameter model.encoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,217][root][DEBUG] - Parameter model.encoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,217 DEBUG    Parameter model.encoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,218][root][DEBUG] - Parameter model.encoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,218 DEBUG    Parameter model.encoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,218][root][DEBUG] - Parameter model.encoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,218 DEBUG    Parameter model.encoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,218][root][DEBUG] - Parameter model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 12]), require_grad=True\n","2022-07-30 11:21:38,218 DEBUG    Parameter model.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 12]), require_grad=True\n","[2022-07-30 11:21:38,219][root][DEBUG] - Parameter model.encoder.block.0.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,219 DEBUG    Parameter model.encoder.block.0.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,219][root][DEBUG] - Parameter model.encoder.block.0.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,219 DEBUG    Parameter model.encoder.block.0.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,219][root][DEBUG] - Parameter model.encoder.block.0.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,219 DEBUG    Parameter model.encoder.block.0.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,219][root][DEBUG] - Parameter model.encoder.block.0.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,219 DEBUG    Parameter model.encoder.block.0.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,220][root][DEBUG] - Parameter model.encoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,220 DEBUG    Parameter model.encoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,220][root][DEBUG] - Parameter model.encoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,220 DEBUG    Parameter model.encoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,220][root][DEBUG] - Parameter model.encoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,220 DEBUG    Parameter model.encoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,221][root][DEBUG] - Parameter model.encoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,221 DEBUG    Parameter model.encoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,221][root][DEBUG] - Parameter model.encoder.block.1.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,221 DEBUG    Parameter model.encoder.block.1.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,221][root][DEBUG] - Parameter model.encoder.block.1.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,221 DEBUG    Parameter model.encoder.block.1.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,221][root][DEBUG] - Parameter model.encoder.block.1.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,221 DEBUG    Parameter model.encoder.block.1.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,222][root][DEBUG] - Parameter model.encoder.block.1.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,222 DEBUG    Parameter model.encoder.block.1.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,222][root][DEBUG] - Parameter model.encoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,222 DEBUG    Parameter model.encoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,222][root][DEBUG] - Parameter model.encoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,222 DEBUG    Parameter model.encoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,223][root][DEBUG] - Parameter model.encoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,223 DEBUG    Parameter model.encoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,223][root][DEBUG] - Parameter model.encoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,223 DEBUG    Parameter model.encoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,223][root][DEBUG] - Parameter model.encoder.block.2.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,223 DEBUG    Parameter model.encoder.block.2.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,224][root][DEBUG] - Parameter model.encoder.block.2.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,224 DEBUG    Parameter model.encoder.block.2.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,224][root][DEBUG] - Parameter model.encoder.block.2.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,224 DEBUG    Parameter model.encoder.block.2.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,224][root][DEBUG] - Parameter model.encoder.block.2.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,224 DEBUG    Parameter model.encoder.block.2.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,224][root][DEBUG] - Parameter model.encoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,224 DEBUG    Parameter model.encoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,224][root][DEBUG] - Parameter model.encoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,224 DEBUG    Parameter model.encoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,225][root][DEBUG] - Parameter model.encoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,225 DEBUG    Parameter model.encoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,225][root][DEBUG] - Parameter model.encoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,225 DEBUG    Parameter model.encoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,225][root][DEBUG] - Parameter model.encoder.block.3.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,225 DEBUG    Parameter model.encoder.block.3.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,226][root][DEBUG] - Parameter model.encoder.block.3.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,226 DEBUG    Parameter model.encoder.block.3.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,226][root][DEBUG] - Parameter model.encoder.block.3.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,226 DEBUG    Parameter model.encoder.block.3.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,226][root][DEBUG] - Parameter model.encoder.block.3.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,226 DEBUG    Parameter model.encoder.block.3.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,227][root][DEBUG] - Parameter model.encoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,227 DEBUG    Parameter model.encoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,227][root][DEBUG] - Parameter model.encoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,227 DEBUG    Parameter model.encoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,227][root][DEBUG] - Parameter model.encoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,227 DEBUG    Parameter model.encoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,227][root][DEBUG] - Parameter model.encoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,227 DEBUG    Parameter model.encoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,228][root][DEBUG] - Parameter model.encoder.block.4.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,228 DEBUG    Parameter model.encoder.block.4.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,228][root][DEBUG] - Parameter model.encoder.block.4.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,228 DEBUG    Parameter model.encoder.block.4.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,228][root][DEBUG] - Parameter model.encoder.block.4.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,228 DEBUG    Parameter model.encoder.block.4.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,228][root][DEBUG] - Parameter model.encoder.block.4.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,228 DEBUG    Parameter model.encoder.block.4.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,229][root][DEBUG] - Parameter model.encoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,229 DEBUG    Parameter model.encoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,229][root][DEBUG] - Parameter model.encoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,229 DEBUG    Parameter model.encoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,229][root][DEBUG] - Parameter model.encoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,229 DEBUG    Parameter model.encoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,229][root][DEBUG] - Parameter model.encoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,229 DEBUG    Parameter model.encoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,229][root][DEBUG] - Parameter model.encoder.block.5.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,229 DEBUG    Parameter model.encoder.block.5.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,230][root][DEBUG] - Parameter model.encoder.block.5.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,230 DEBUG    Parameter model.encoder.block.5.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,230][root][DEBUG] - Parameter model.encoder.block.5.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,230 DEBUG    Parameter model.encoder.block.5.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,230][root][DEBUG] - Parameter model.encoder.block.5.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,230 DEBUG    Parameter model.encoder.block.5.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,230][root][DEBUG] - Parameter model.encoder.block.6.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,230 DEBUG    Parameter model.encoder.block.6.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,231][root][DEBUG] - Parameter model.encoder.block.6.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,231 DEBUG    Parameter model.encoder.block.6.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,231][root][DEBUG] - Parameter model.encoder.block.6.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,231 DEBUG    Parameter model.encoder.block.6.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,231][root][DEBUG] - Parameter model.encoder.block.6.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,231 DEBUG    Parameter model.encoder.block.6.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,231][root][DEBUG] - Parameter model.encoder.block.6.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,231 DEBUG    Parameter model.encoder.block.6.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,231][root][DEBUG] - Parameter model.encoder.block.6.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,231 DEBUG    Parameter model.encoder.block.6.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,232][root][DEBUG] - Parameter model.encoder.block.6.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,232 DEBUG    Parameter model.encoder.block.6.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,232][root][DEBUG] - Parameter model.encoder.block.6.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,232 DEBUG    Parameter model.encoder.block.6.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,232][root][DEBUG] - Parameter model.encoder.block.7.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,232 DEBUG    Parameter model.encoder.block.7.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,232][root][DEBUG] - Parameter model.encoder.block.7.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,232 DEBUG    Parameter model.encoder.block.7.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,233][root][DEBUG] - Parameter model.encoder.block.7.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,233 DEBUG    Parameter model.encoder.block.7.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,233][root][DEBUG] - Parameter model.encoder.block.7.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,233 DEBUG    Parameter model.encoder.block.7.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,256][root][DEBUG] - Parameter model.encoder.block.7.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,256 DEBUG    Parameter model.encoder.block.7.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,257][root][DEBUG] - Parameter model.encoder.block.7.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,257 DEBUG    Parameter model.encoder.block.7.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,257][root][DEBUG] - Parameter model.encoder.block.7.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,257 DEBUG    Parameter model.encoder.block.7.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,257][root][DEBUG] - Parameter model.encoder.block.7.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,257 DEBUG    Parameter model.encoder.block.7.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,258][root][DEBUG] - Parameter model.encoder.block.8.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,258 DEBUG    Parameter model.encoder.block.8.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,258][root][DEBUG] - Parameter model.encoder.block.8.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,258 DEBUG    Parameter model.encoder.block.8.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,259][root][DEBUG] - Parameter model.encoder.block.8.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,259 DEBUG    Parameter model.encoder.block.8.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,259][root][DEBUG] - Parameter model.encoder.block.8.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,259 DEBUG    Parameter model.encoder.block.8.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,259][root][DEBUG] - Parameter model.encoder.block.8.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,259 DEBUG    Parameter model.encoder.block.8.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,260][root][DEBUG] - Parameter model.encoder.block.8.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,260 DEBUG    Parameter model.encoder.block.8.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,260][root][DEBUG] - Parameter model.encoder.block.8.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,260 DEBUG    Parameter model.encoder.block.8.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,260][root][DEBUG] - Parameter model.encoder.block.8.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,260 DEBUG    Parameter model.encoder.block.8.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,261][root][DEBUG] - Parameter model.encoder.block.9.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,261 DEBUG    Parameter model.encoder.block.9.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,261][root][DEBUG] - Parameter model.encoder.block.9.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,261 DEBUG    Parameter model.encoder.block.9.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,261][root][DEBUG] - Parameter model.encoder.block.9.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,261 DEBUG    Parameter model.encoder.block.9.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,262][root][DEBUG] - Parameter model.encoder.block.9.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,262 DEBUG    Parameter model.encoder.block.9.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,262][root][DEBUG] - Parameter model.encoder.block.9.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,262 DEBUG    Parameter model.encoder.block.9.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,262][root][DEBUG] - Parameter model.encoder.block.9.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,262 DEBUG    Parameter model.encoder.block.9.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,263][root][DEBUG] - Parameter model.encoder.block.9.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,263 DEBUG    Parameter model.encoder.block.9.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,263][root][DEBUG] - Parameter model.encoder.block.9.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,263 DEBUG    Parameter model.encoder.block.9.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,263][root][DEBUG] - Parameter model.encoder.block.10.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,263 DEBUG    Parameter model.encoder.block.10.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,264][root][DEBUG] - Parameter model.encoder.block.10.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,264 DEBUG    Parameter model.encoder.block.10.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,264][root][DEBUG] - Parameter model.encoder.block.10.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,264 DEBUG    Parameter model.encoder.block.10.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,264][root][DEBUG] - Parameter model.encoder.block.10.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,264 DEBUG    Parameter model.encoder.block.10.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,265][root][DEBUG] - Parameter model.encoder.block.10.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,265 DEBUG    Parameter model.encoder.block.10.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,265][root][DEBUG] - Parameter model.encoder.block.10.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,265 DEBUG    Parameter model.encoder.block.10.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,265][root][DEBUG] - Parameter model.encoder.block.10.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,265 DEBUG    Parameter model.encoder.block.10.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,266][root][DEBUG] - Parameter model.encoder.block.10.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,266 DEBUG    Parameter model.encoder.block.10.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,266][root][DEBUG] - Parameter model.encoder.block.11.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,266 DEBUG    Parameter model.encoder.block.11.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,267][root][DEBUG] - Parameter model.encoder.block.11.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,267 DEBUG    Parameter model.encoder.block.11.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,267][root][DEBUG] - Parameter model.encoder.block.11.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,267 DEBUG    Parameter model.encoder.block.11.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,267][root][DEBUG] - Parameter model.encoder.block.11.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,267 DEBUG    Parameter model.encoder.block.11.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,268][root][DEBUG] - Parameter model.encoder.block.11.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,268 DEBUG    Parameter model.encoder.block.11.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,268][root][DEBUG] - Parameter model.encoder.block.11.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,268 DEBUG    Parameter model.encoder.block.11.layer.1.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,268][root][DEBUG] - Parameter model.encoder.block.11.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,268 DEBUG    Parameter model.encoder.block.11.layer.1.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,269][root][DEBUG] - Parameter model.encoder.block.11.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,269 DEBUG    Parameter model.encoder.block.11.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,269][root][DEBUG] - Parameter model.encoder.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,269 DEBUG    Parameter model.encoder.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,270][root][DEBUG] - Parameter model.decoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,270 DEBUG    Parameter model.decoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,270][root][DEBUG] - Parameter model.decoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,270 DEBUG    Parameter model.decoder.block.0.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,270][root][DEBUG] - Parameter model.decoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,270 DEBUG    Parameter model.decoder.block.0.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,271][root][DEBUG] - Parameter model.decoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,271 DEBUG    Parameter model.decoder.block.0.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,271][root][DEBUG] - Parameter model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 12]), require_grad=True\n","2022-07-30 11:21:38,271 DEBUG    Parameter model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: torch.Size([32, 12]), require_grad=True\n","[2022-07-30 11:21:38,271][root][DEBUG] - Parameter model.decoder.block.0.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,271 DEBUG    Parameter model.decoder.block.0.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,272][root][DEBUG] - Parameter model.decoder.block.0.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,272 DEBUG    Parameter model.decoder.block.0.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,272][root][DEBUG] - Parameter model.decoder.block.0.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,272 DEBUG    Parameter model.decoder.block.0.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,273][root][DEBUG] - Parameter model.decoder.block.0.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,273 DEBUG    Parameter model.decoder.block.0.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,273][root][DEBUG] - Parameter model.decoder.block.0.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,273 DEBUG    Parameter model.decoder.block.0.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,273][root][DEBUG] - Parameter model.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight: torch.Size([32, 12]), require_grad=True\n","2022-07-30 11:21:38,273 DEBUG    Parameter model.decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight: torch.Size([32, 12]), require_grad=True\n","[2022-07-30 11:21:38,274][root][DEBUG] - Parameter model.decoder.block.0.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,274 DEBUG    Parameter model.decoder.block.0.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,274][root][DEBUG] - Parameter model.decoder.block.0.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,274 DEBUG    Parameter model.decoder.block.0.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,274][root][DEBUG] - Parameter model.decoder.block.0.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,274 DEBUG    Parameter model.decoder.block.0.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,274][root][DEBUG] - Parameter model.decoder.block.0.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,274 DEBUG    Parameter model.decoder.block.0.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,275][root][DEBUG] - Parameter model.decoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,275 DEBUG    Parameter model.decoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,275][root][DEBUG] - Parameter model.decoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,275 DEBUG    Parameter model.decoder.block.1.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,276][root][DEBUG] - Parameter model.decoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,276 DEBUG    Parameter model.decoder.block.1.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,276][root][DEBUG] - Parameter model.decoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,276 DEBUG    Parameter model.decoder.block.1.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,276][root][DEBUG] - Parameter model.decoder.block.1.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,276 DEBUG    Parameter model.decoder.block.1.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,277][root][DEBUG] - Parameter model.decoder.block.1.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,277 DEBUG    Parameter model.decoder.block.1.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,277][root][DEBUG] - Parameter model.decoder.block.1.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,277 DEBUG    Parameter model.decoder.block.1.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,277][root][DEBUG] - Parameter model.decoder.block.1.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,277 DEBUG    Parameter model.decoder.block.1.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,278][root][DEBUG] - Parameter model.decoder.block.1.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,278 DEBUG    Parameter model.decoder.block.1.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,278][root][DEBUG] - Parameter model.decoder.block.1.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,278 DEBUG    Parameter model.decoder.block.1.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,278][root][DEBUG] - Parameter model.decoder.block.1.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,278 DEBUG    Parameter model.decoder.block.1.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,278][root][DEBUG] - Parameter model.decoder.block.1.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,278 DEBUG    Parameter model.decoder.block.1.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,279][root][DEBUG] - Parameter model.decoder.block.1.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,279 DEBUG    Parameter model.decoder.block.1.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,279][root][DEBUG] - Parameter model.decoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,279 DEBUG    Parameter model.decoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,279][root][DEBUG] - Parameter model.decoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,279 DEBUG    Parameter model.decoder.block.2.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,280][root][DEBUG] - Parameter model.decoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,280 DEBUG    Parameter model.decoder.block.2.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,280][root][DEBUG] - Parameter model.decoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,280 DEBUG    Parameter model.decoder.block.2.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,280][root][DEBUG] - Parameter model.decoder.block.2.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,280 DEBUG    Parameter model.decoder.block.2.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,281][root][DEBUG] - Parameter model.decoder.block.2.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,281 DEBUG    Parameter model.decoder.block.2.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,281][root][DEBUG] - Parameter model.decoder.block.2.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,281 DEBUG    Parameter model.decoder.block.2.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,281][root][DEBUG] - Parameter model.decoder.block.2.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,281 DEBUG    Parameter model.decoder.block.2.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,282][root][DEBUG] - Parameter model.decoder.block.2.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,282 DEBUG    Parameter model.decoder.block.2.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,282][root][DEBUG] - Parameter model.decoder.block.2.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,282 DEBUG    Parameter model.decoder.block.2.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,282][root][DEBUG] - Parameter model.decoder.block.2.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,282 DEBUG    Parameter model.decoder.block.2.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,282][root][DEBUG] - Parameter model.decoder.block.2.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,282 DEBUG    Parameter model.decoder.block.2.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,283][root][DEBUG] - Parameter model.decoder.block.2.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,283 DEBUG    Parameter model.decoder.block.2.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,283][root][DEBUG] - Parameter model.decoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,283 DEBUG    Parameter model.decoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,283][root][DEBUG] - Parameter model.decoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,283 DEBUG    Parameter model.decoder.block.3.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,284][root][DEBUG] - Parameter model.decoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,284 DEBUG    Parameter model.decoder.block.3.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,284][root][DEBUG] - Parameter model.decoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,284 DEBUG    Parameter model.decoder.block.3.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,284][root][DEBUG] - Parameter model.decoder.block.3.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,284 DEBUG    Parameter model.decoder.block.3.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,285][root][DEBUG] - Parameter model.decoder.block.3.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,285 DEBUG    Parameter model.decoder.block.3.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,285][root][DEBUG] - Parameter model.decoder.block.3.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,285 DEBUG    Parameter model.decoder.block.3.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,285][root][DEBUG] - Parameter model.decoder.block.3.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,285 DEBUG    Parameter model.decoder.block.3.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,286][root][DEBUG] - Parameter model.decoder.block.3.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,286 DEBUG    Parameter model.decoder.block.3.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,286][root][DEBUG] - Parameter model.decoder.block.3.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,286 DEBUG    Parameter model.decoder.block.3.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,286][root][DEBUG] - Parameter model.decoder.block.3.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,286 DEBUG    Parameter model.decoder.block.3.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,287][root][DEBUG] - Parameter model.decoder.block.3.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,287 DEBUG    Parameter model.decoder.block.3.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,287][root][DEBUG] - Parameter model.decoder.block.3.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,287 DEBUG    Parameter model.decoder.block.3.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,287][root][DEBUG] - Parameter model.decoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,287 DEBUG    Parameter model.decoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,288][root][DEBUG] - Parameter model.decoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,288 DEBUG    Parameter model.decoder.block.4.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,288][root][DEBUG] - Parameter model.decoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,288 DEBUG    Parameter model.decoder.block.4.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,288][root][DEBUG] - Parameter model.decoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,288 DEBUG    Parameter model.decoder.block.4.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,288][root][DEBUG] - Parameter model.decoder.block.4.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,288 DEBUG    Parameter model.decoder.block.4.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,289][root][DEBUG] - Parameter model.decoder.block.4.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,289 DEBUG    Parameter model.decoder.block.4.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,289][root][DEBUG] - Parameter model.decoder.block.4.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,289 DEBUG    Parameter model.decoder.block.4.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,289][root][DEBUG] - Parameter model.decoder.block.4.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,289 DEBUG    Parameter model.decoder.block.4.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,290][root][DEBUG] - Parameter model.decoder.block.4.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,290 DEBUG    Parameter model.decoder.block.4.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,290][root][DEBUG] - Parameter model.decoder.block.4.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,290 DEBUG    Parameter model.decoder.block.4.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,290][root][DEBUG] - Parameter model.decoder.block.4.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,290 DEBUG    Parameter model.decoder.block.4.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,291][root][DEBUG] - Parameter model.decoder.block.4.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,291 DEBUG    Parameter model.decoder.block.4.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,291][root][DEBUG] - Parameter model.decoder.block.4.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,291 DEBUG    Parameter model.decoder.block.4.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,291][root][DEBUG] - Parameter model.decoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,291 DEBUG    Parameter model.decoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,292][root][DEBUG] - Parameter model.decoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,292 DEBUG    Parameter model.decoder.block.5.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,292][root][DEBUG] - Parameter model.decoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,292 DEBUG    Parameter model.decoder.block.5.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,292][root][DEBUG] - Parameter model.decoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,292 DEBUG    Parameter model.decoder.block.5.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,293][root][DEBUG] - Parameter model.decoder.block.5.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,293 DEBUG    Parameter model.decoder.block.5.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,293][root][DEBUG] - Parameter model.decoder.block.5.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,293 DEBUG    Parameter model.decoder.block.5.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,293][root][DEBUG] - Parameter model.decoder.block.5.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,293 DEBUG    Parameter model.decoder.block.5.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,294][root][DEBUG] - Parameter model.decoder.block.5.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,294 DEBUG    Parameter model.decoder.block.5.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,294][root][DEBUG] - Parameter model.decoder.block.5.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,294 DEBUG    Parameter model.decoder.block.5.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,294][root][DEBUG] - Parameter model.decoder.block.5.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,294 DEBUG    Parameter model.decoder.block.5.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,295][root][DEBUG] - Parameter model.decoder.block.5.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,295 DEBUG    Parameter model.decoder.block.5.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,295][root][DEBUG] - Parameter model.decoder.block.5.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,295 DEBUG    Parameter model.decoder.block.5.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,295][root][DEBUG] - Parameter model.decoder.block.5.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,295 DEBUG    Parameter model.decoder.block.5.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,296][root][DEBUG] - Parameter model.decoder.block.6.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,296 DEBUG    Parameter model.decoder.block.6.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,296][root][DEBUG] - Parameter model.decoder.block.6.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,296 DEBUG    Parameter model.decoder.block.6.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,296][root][DEBUG] - Parameter model.decoder.block.6.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,296 DEBUG    Parameter model.decoder.block.6.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,296][root][DEBUG] - Parameter model.decoder.block.6.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,296 DEBUG    Parameter model.decoder.block.6.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,297][root][DEBUG] - Parameter model.decoder.block.6.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,297 DEBUG    Parameter model.decoder.block.6.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,297][root][DEBUG] - Parameter model.decoder.block.6.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,297 DEBUG    Parameter model.decoder.block.6.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,297][root][DEBUG] - Parameter model.decoder.block.6.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,297 DEBUG    Parameter model.decoder.block.6.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,298][root][DEBUG] - Parameter model.decoder.block.6.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,298 DEBUG    Parameter model.decoder.block.6.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,298][root][DEBUG] - Parameter model.decoder.block.6.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,298 DEBUG    Parameter model.decoder.block.6.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,298][root][DEBUG] - Parameter model.decoder.block.6.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,298 DEBUG    Parameter model.decoder.block.6.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,299][root][DEBUG] - Parameter model.decoder.block.6.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,299 DEBUG    Parameter model.decoder.block.6.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,299][root][DEBUG] - Parameter model.decoder.block.6.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,299 DEBUG    Parameter model.decoder.block.6.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,299][root][DEBUG] - Parameter model.decoder.block.6.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,299 DEBUG    Parameter model.decoder.block.6.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,300][root][DEBUG] - Parameter model.decoder.block.7.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,300 DEBUG    Parameter model.decoder.block.7.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,300][root][DEBUG] - Parameter model.decoder.block.7.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,300 DEBUG    Parameter model.decoder.block.7.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,300][root][DEBUG] - Parameter model.decoder.block.7.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,300 DEBUG    Parameter model.decoder.block.7.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,301][root][DEBUG] - Parameter model.decoder.block.7.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,301 DEBUG    Parameter model.decoder.block.7.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,301][root][DEBUG] - Parameter model.decoder.block.7.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,301 DEBUG    Parameter model.decoder.block.7.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,301][root][DEBUG] - Parameter model.decoder.block.7.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,301 DEBUG    Parameter model.decoder.block.7.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,301][root][DEBUG] - Parameter model.decoder.block.7.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,301 DEBUG    Parameter model.decoder.block.7.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,302][root][DEBUG] - Parameter model.decoder.block.7.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,302 DEBUG    Parameter model.decoder.block.7.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,302][root][DEBUG] - Parameter model.decoder.block.7.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,302 DEBUG    Parameter model.decoder.block.7.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,302][root][DEBUG] - Parameter model.decoder.block.7.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,302 DEBUG    Parameter model.decoder.block.7.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,303][root][DEBUG] - Parameter model.decoder.block.7.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,303 DEBUG    Parameter model.decoder.block.7.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,303][root][DEBUG] - Parameter model.decoder.block.7.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,303 DEBUG    Parameter model.decoder.block.7.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,303][root][DEBUG] - Parameter model.decoder.block.7.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,303 DEBUG    Parameter model.decoder.block.7.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,304][root][DEBUG] - Parameter model.decoder.block.8.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,304 DEBUG    Parameter model.decoder.block.8.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,304][root][DEBUG] - Parameter model.decoder.block.8.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,304 DEBUG    Parameter model.decoder.block.8.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,304][root][DEBUG] - Parameter model.decoder.block.8.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,304 DEBUG    Parameter model.decoder.block.8.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,305][root][DEBUG] - Parameter model.decoder.block.8.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,305 DEBUG    Parameter model.decoder.block.8.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,305][root][DEBUG] - Parameter model.decoder.block.8.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,305 DEBUG    Parameter model.decoder.block.8.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,305][root][DEBUG] - Parameter model.decoder.block.8.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,305 DEBUG    Parameter model.decoder.block.8.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,306][root][DEBUG] - Parameter model.decoder.block.8.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,306 DEBUG    Parameter model.decoder.block.8.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,306][root][DEBUG] - Parameter model.decoder.block.8.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,306 DEBUG    Parameter model.decoder.block.8.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,306][root][DEBUG] - Parameter model.decoder.block.8.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,306 DEBUG    Parameter model.decoder.block.8.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,307][root][DEBUG] - Parameter model.decoder.block.8.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,307 DEBUG    Parameter model.decoder.block.8.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,307][root][DEBUG] - Parameter model.decoder.block.8.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,307 DEBUG    Parameter model.decoder.block.8.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,307][root][DEBUG] - Parameter model.decoder.block.8.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,307 DEBUG    Parameter model.decoder.block.8.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,307][root][DEBUG] - Parameter model.decoder.block.8.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,307 DEBUG    Parameter model.decoder.block.8.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,308][root][DEBUG] - Parameter model.decoder.block.9.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,308 DEBUG    Parameter model.decoder.block.9.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,308][root][DEBUG] - Parameter model.decoder.block.9.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,308 DEBUG    Parameter model.decoder.block.9.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,308][root][DEBUG] - Parameter model.decoder.block.9.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,308 DEBUG    Parameter model.decoder.block.9.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,309][root][DEBUG] - Parameter model.decoder.block.9.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,309 DEBUG    Parameter model.decoder.block.9.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,309][root][DEBUG] - Parameter model.decoder.block.9.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,309 DEBUG    Parameter model.decoder.block.9.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,309][root][DEBUG] - Parameter model.decoder.block.9.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,309 DEBUG    Parameter model.decoder.block.9.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,310][root][DEBUG] - Parameter model.decoder.block.9.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,310 DEBUG    Parameter model.decoder.block.9.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,310][root][DEBUG] - Parameter model.decoder.block.9.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,310 DEBUG    Parameter model.decoder.block.9.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,310][root][DEBUG] - Parameter model.decoder.block.9.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,310 DEBUG    Parameter model.decoder.block.9.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,310][root][DEBUG] - Parameter model.decoder.block.9.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,310 DEBUG    Parameter model.decoder.block.9.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,311][root][DEBUG] - Parameter model.decoder.block.9.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,311 DEBUG    Parameter model.decoder.block.9.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,311][root][DEBUG] - Parameter model.decoder.block.9.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,311 DEBUG    Parameter model.decoder.block.9.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,312][root][DEBUG] - Parameter model.decoder.block.9.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,312 DEBUG    Parameter model.decoder.block.9.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,312][root][DEBUG] - Parameter model.decoder.block.10.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,312 DEBUG    Parameter model.decoder.block.10.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,312][root][DEBUG] - Parameter model.decoder.block.10.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,312 DEBUG    Parameter model.decoder.block.10.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,312][root][DEBUG] - Parameter model.decoder.block.10.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,312 DEBUG    Parameter model.decoder.block.10.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,313][root][DEBUG] - Parameter model.decoder.block.10.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,313 DEBUG    Parameter model.decoder.block.10.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,313][root][DEBUG] - Parameter model.decoder.block.10.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,313 DEBUG    Parameter model.decoder.block.10.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,313][root][DEBUG] - Parameter model.decoder.block.10.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,313 DEBUG    Parameter model.decoder.block.10.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,314][root][DEBUG] - Parameter model.decoder.block.10.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,314 DEBUG    Parameter model.decoder.block.10.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,314][root][DEBUG] - Parameter model.decoder.block.10.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,314 DEBUG    Parameter model.decoder.block.10.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,314][root][DEBUG] - Parameter model.decoder.block.10.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,314 DEBUG    Parameter model.decoder.block.10.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,315][root][DEBUG] - Parameter model.decoder.block.10.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,315 DEBUG    Parameter model.decoder.block.10.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,315][root][DEBUG] - Parameter model.decoder.block.10.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,315 DEBUG    Parameter model.decoder.block.10.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,315][root][DEBUG] - Parameter model.decoder.block.10.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,315 DEBUG    Parameter model.decoder.block.10.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,316][root][DEBUG] - Parameter model.decoder.block.10.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,316 DEBUG    Parameter model.decoder.block.10.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,316][root][DEBUG] - Parameter model.decoder.block.11.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,316 DEBUG    Parameter model.decoder.block.11.layer.0.SelfAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,316][root][DEBUG] - Parameter model.decoder.block.11.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,316 DEBUG    Parameter model.decoder.block.11.layer.0.SelfAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,317][root][DEBUG] - Parameter model.decoder.block.11.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,317 DEBUG    Parameter model.decoder.block.11.layer.0.SelfAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,317][root][DEBUG] - Parameter model.decoder.block.11.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,317 DEBUG    Parameter model.decoder.block.11.layer.0.SelfAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,317][root][DEBUG] - Parameter model.decoder.block.11.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,317 DEBUG    Parameter model.decoder.block.11.layer.0.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,317][root][DEBUG] - Parameter model.decoder.block.11.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,317 DEBUG    Parameter model.decoder.block.11.layer.1.EncDecAttention.q.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,318][root][DEBUG] - Parameter model.decoder.block.11.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,318 DEBUG    Parameter model.decoder.block.11.layer.1.EncDecAttention.k.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,318][root][DEBUG] - Parameter model.decoder.block.11.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,318 DEBUG    Parameter model.decoder.block.11.layer.1.EncDecAttention.v.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,319][root][DEBUG] - Parameter model.decoder.block.11.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-30 11:21:38,319 DEBUG    Parameter model.decoder.block.11.layer.1.EncDecAttention.o.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-30 11:21:38,319][root][DEBUG] - Parameter model.decoder.block.11.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,319 DEBUG    Parameter model.decoder.block.11.layer.1.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,319][root][DEBUG] - Parameter model.decoder.block.11.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-30 11:21:38,319 DEBUG    Parameter model.decoder.block.11.layer.2.DenseReluDense.wi.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-30 11:21:38,320][root][DEBUG] - Parameter model.decoder.block.11.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-30 11:21:38,320 DEBUG    Parameter model.decoder.block.11.layer.2.DenseReluDense.wo.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-30 11:21:38,320][root][DEBUG] - Parameter model.decoder.block.11.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,320 DEBUG    Parameter model.decoder.block.11.layer.2.layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-30 11:21:38,320][root][DEBUG] - Parameter model.decoder.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-30 11:21:38,320 DEBUG    Parameter model.decoder.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","Iteration: 100% 573/573 [21:23<00:00,  2.24s/it]\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","[2022-07-30 11:43:05,686][root][INFO] - test data set : ET_1_1_test.txt\n","2022-07-30 11:43:05,686 INFO     test data set : ET_1_1_test.txt\n","[2022-07-30 11:43:05,686][root][INFO] - N-grams: 1-1.5303127770974508e-05, 2-1.1509996415398945e-06, 3-7.349498707128138e-07, 4-1.06056620955e-312\n","2022-07-30 11:43:05,686 INFO     N-grams: 1-1.5303127770974508e-05, 2-1.1509996415398945e-06, 3-7.349498707128138e-07, 4-1.06056620955e-312\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"xiRHm4nruPw9"},"execution_count":null,"outputs":[]}]}