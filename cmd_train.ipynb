{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cmd_train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1Mrwlgkjv2G7LOLpOtCehzWxaTRu9sXPA","authorship_tag":"ABX9TyMEs/DfhGRTESZkd/lhX3ai"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"dymDs0Po0aES","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658556413044,"user_tz":-540,"elapsed":349,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"c5f8bc91-3418-4ddc-8894-fd5e893018d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Jul 23 06:04:42 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp"],"metadata":{"id":"EfCxDoEXuzgg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658556416920,"user_tz":-540,"elapsed":338,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"c8d88fd2-f4ba-49cb-e112-cef6d85e6146"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp\n"]}]},{"cell_type":"code","source":["!python -m pip install -r requirements.txt"],"metadata":{"id":"mshVPsrku0kd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658556916476,"user_tz":-540,"elapsed":498159,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"9b7e9602-98ec-4bee-d865-d259b0156360"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.8.0\n","  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n","\u001b[K     |████████████████████████████████| 735.5 MB 13 kB/s \n","\u001b[?25hCollecting transformers==3.0.0\n","  Downloading transformers-3.0.0-py3-none-any.whl (754 kB)\n","\u001b[K     |████████████████████████████████| 754 kB 76.3 MB/s \n","\u001b[?25hCollecting torch_scatter==2.0.4\n","  Downloading torch_scatter-2.0.4.tar.gz (20 kB)\n","Collecting dgl==0.5.3\n","  Downloading dgl-0.5.3-cp37-cp37m-manylinux1_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 70.3 MB/s \n","\u001b[?25hCollecting hydra==2.4\n","  Downloading Hydra-2.4.tar.gz (80 kB)\n","\u001b[K     |████████████████████████████████| 80 kB 10.7 MB/s \n","\u001b[?25hCollecting omegaconf==2.2.2\n","  Downloading omegaconf-2.2.2-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->-r requirements.txt (line 2)) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->-r requirements.txt (line 2)) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0->-r requirements.txt (line 3)) (3.7.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0->-r requirements.txt (line 3)) (4.64.0)\n","Collecting tokenizers==0.8.0-rc4\n","  Downloading tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 55.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0->-r requirements.txt (line 3)) (2022.6.2)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 57.9 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 74.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0->-r requirements.txt (line 3)) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0->-r requirements.txt (line 3)) (2.23.0)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl==0.5.3->-r requirements.txt (line 5)) (2.6.3)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl==0.5.3->-r requirements.txt (line 5)) (1.7.3)\n","Collecting PyYAML>=5.1.0\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 50.3 MB/s \n","\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[K     |████████████████████████████████| 117 kB 74.6 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0->-r requirements.txt (line 3)) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0->-r requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0->-r requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0->-r requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0->-r requirements.txt (line 3)) (3.0.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0->-r requirements.txt (line 3)) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0->-r requirements.txt (line 3)) (1.1.0)\n","Building wheels for collected packages: torch-scatter, hydra, antlr4-python3-runtime, sacremoses\n","  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-scatter: filename=torch_scatter-2.0.4-cp37-cp37m-linux_x86_64.whl size=14800844 sha256=b84d85b938cd650b3539b9a3f0c91edf1f22b3d6d9aa725f9258e12bb5ad97bd\n","  Stored in directory: /root/.cache/pip/wheels/4a/3d/01/e408ecc11389070cbacee91b70c978bf3557130e9aaa18a95b\n","  Building wheel for hydra (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hydra: filename=Hydra-2.4-cp37-cp37m-linux_x86_64.whl size=219227 sha256=c7126ddaa87d4b90664c9372e0896d5e918aa3bfd0d9a2617010155ebfcb2550\n","  Stored in directory: /root/.cache/pip/wheels/f5/65/98/3603b340344bd22dfe1c809365013fc4d425d83d89c3e2cd17\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=df9a58b0101c7d1d40b97930bf4748ec6e0281f1665f5008c50322031ae50079\n","  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=7308a8891a5dfcf13a7ab0943411a65ff00b1b691821f86906777c273d5668e5\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built torch-scatter hydra antlr4-python3-runtime sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, PyYAML, antlr4-python3-runtime, transformers, torch-scatter, torch, omegaconf, hydra, dgl\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.0+cu113\n","    Uninstalling torch-1.12.0+cu113:\n","      Successfully uninstalled torch-1.12.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.8.0 which is incompatible.\n","torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.8.0 which is incompatible.\n","torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n","Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.9.3 dgl-0.5.3 hydra-2.4 omegaconf-2.2.2 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.8.0rc4 torch-1.8.0 torch-scatter-2.0.4 transformers-3.0.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8LmQt_jcvSZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658557060326,"user_tz":-540,"elapsed":143869,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"5a893b45-7cd7-43f1-edc5-9ab43d2ea162"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.9.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n","\u001b[K     |█████████████                   | 834.1 MB 69.3 MB/s eta 0:00:18tcmalloc: large alloc 1147494400 bytes == 0x39706000 @  0x7f3f8c73b615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████▌               | 1055.7 MB 108.7 MB/s eta 0:00:10tcmalloc: large alloc 1434370048 bytes == 0x7dd5c000 @  0x7f3f8c73b615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |█████████████████████           | 1336.2 MB 102.1 MB/s eta 0:00:07tcmalloc: large alloc 1792966656 bytes == 0x2b8e000 @  0x7f3f8c73b615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |██████████████████████████▌     | 1691.1 MB 1.2 MB/s eta 0:05:03tcmalloc: large alloc 2241208320 bytes == 0x6d976000 @  0x7f3f8c73b615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████████████████████| 2041.3 MB 1.1 MB/s eta 0:00:01tcmalloc: large alloc 2041348096 bytes == 0xf32d8000 @  0x7f3f8c73a1e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n","tcmalloc: large alloc 2551685120 bytes == 0x1e1260000 @  0x7f3f8c73b615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n","\u001b[K     |████████████████████████████████| 2041.3 MB 7.1 kB/s \n","\u001b[?25hCollecting torchvision==0.10.0+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)\n","\u001b[K     |████████████████████████████████| 23.2 MB 234 kB/s \n","\u001b[?25hCollecting torchaudio===0.9.0\n","  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 6.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu111) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (1.21.6)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (7.1.2)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.8.0\n","    Uninstalling torch-1.8.0:\n","      Successfully uninstalled torch-1.8.0\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.13.0+cu113\n","    Uninstalling torchvision-0.13.0+cu113:\n","      Successfully uninstalled torchvision-0.13.0+cu113\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 0.12.0+cu113\n","    Uninstalling torchaudio-0.12.0+cu113:\n","      Successfully uninstalled torchaudio-0.12.0+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.9.0+cu111 which is incompatible.\u001b[0m\n","Successfully installed torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111\n"]}],"source":["!python -m pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","source":["!python -m pip uninstall torch-scatter -y"],"metadata":{"id":"JJqgY88ou7Qp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658557061027,"user_tz":-540,"elapsed":717,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"e3482f5a-0440-4545-a1c4-4a5edbcc4de3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch-scatter 2.0.4\n","Uninstalling torch-scatter-2.0.4:\n","  Successfully uninstalled torch-scatter-2.0.4\n"]}]},{"cell_type":"code","source":["!python -m pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.1+cu111.html"],"metadata":{"id":"7c7V5B7yc9Cc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658557065857,"user_tz":-540,"elapsed":4839,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"9cf92d82-e7fc-4966-8198-d1c608da8440"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.9.1+cu111.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-1.9.0%2Bcu111/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (10.4 MB)\n","\u001b[K     |████████████████████████████████| 10.4 MB 8.8 MB/s \n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.0.9\n"]}]},{"cell_type":"code","source":["!python -c \"import torch_scatter; print(torch_scatter.__version__)\""],"metadata":{"id":"l2wm30u-c-i0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658557066955,"user_tz":-540,"elapsed":1108,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"a30a05e6-345c-4a25-d320-e9d31aa5b571"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.9\n"]}]},{"cell_type":"code","source":["!pip install hydra-core --upgrade"],"metadata":{"id":"GgPMOQSL4HVZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658557070790,"user_tz":-540,"elapsed":3839,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"2d147e88-4d97-4370-89d7-0cf92dc0567e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting hydra-core\n","  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n","\u001b[K     |████████████████████████████████| 151 kB 7.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from hydra-core) (21.3)\n","Requirement already satisfied: omegaconf~=2.2 in /usr/local/lib/python3.7/dist-packages (from hydra-core) (2.2.2)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core) (5.8.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.7/dist-packages (from hydra-core) (4.9.3)\n","Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf~=2.2->hydra-core) (6.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->hydra-core) (3.0.9)\n","Installing collected packages: hydra-core\n","Successfully installed hydra-core-1.2.0\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp/data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FT5vluqcNpey","executionInfo":{"status":"ok","timestamp":1658391076700,"user_tz":-540,"elapsed":430,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"509c51db-1dbe-4933-9f2e-c92aba3546bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp/data\n"]}]},{"cell_type":"code","source":["!python preprocess.py --dataset FB15kET"],"metadata":{"id":"PZROa2w4NUfm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python preprocess.py --dataset YAGO43kET"],"metadata":{"id":"mDB7erFzN4St"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd /content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp"],"metadata":{"id":"hsnRA4_pUSzk","executionInfo":{"status":"ok","timestamp":1658565367623,"user_tz":-540,"elapsed":13,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"aea0c14f-422b-41cc-def0-d21a7cf8461d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp\n"]}]},{"cell_type":"code","source":["!HYDRA_FULL_ERROR=1 python run.py hydra.job.chdir=False hydra.verbose=True"],"metadata":{"id":"586hjT34u-VB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9fd61a94-31ac-453b-b657-b2fa2cd0d879","executionInfo":{"status":"ok","timestamp":1658566206727,"user_tz":-540,"elapsed":837348,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using backend: pytorch\n","run.py:16: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"config\", config_name='config')\n","/usr/local/lib/python3.7/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n","  warnings.warn(msg, UserWarning)\n","[2022-07-23 08:34:03,568][HYDRA] Hydra 1.2.0\n","[2022-07-23 08:34:03,568][HYDRA] ===========\n","[2022-07-23 08:34:03,568][HYDRA] Installed Hydra Plugins\n","[2022-07-23 08:34:03,568][HYDRA] ***********************\n","[2022-07-23 08:34:03,568][HYDRA] \tConfigSource:\n","[2022-07-23 08:34:03,568][HYDRA] \t-------------\n","[2022-07-23 08:34:03,568][HYDRA] \t\tFileConfigSource\n","[2022-07-23 08:34:03,568][HYDRA] \t\tImportlibResourcesConfigSource\n","[2022-07-23 08:34:03,568][HYDRA] \t\tStructuredConfigSource\n","[2022-07-23 08:34:03,568][HYDRA] \tCompletionPlugin:\n","[2022-07-23 08:34:03,568][HYDRA] \t-----------------\n","[2022-07-23 08:34:03,568][HYDRA] \t\tBashCompletion\n","[2022-07-23 08:34:03,568][HYDRA] \t\tFishCompletion\n","[2022-07-23 08:34:03,568][HYDRA] \t\tZshCompletion\n","[2022-07-23 08:34:03,568][HYDRA] \tLauncher:\n","[2022-07-23 08:34:03,569][HYDRA] \t---------\n","[2022-07-23 08:34:03,569][HYDRA] \t\tBasicLauncher\n","[2022-07-23 08:34:03,569][HYDRA] \tSweeper:\n","[2022-07-23 08:34:03,569][HYDRA] \t--------\n","[2022-07-23 08:34:03,569][HYDRA] \t\tBasicSweeper\n","[2022-07-23 08:34:03,569][HYDRA] \n","[2022-07-23 08:34:03,569][HYDRA] Config search path\n","[2022-07-23 08:34:03,569][HYDRA] ******************\n","[2022-07-23 08:34:03,708][HYDRA] | Provider | Search path                                                           |\n","[2022-07-23 08:34:03,708][HYDRA] ------------------------------------------------------------------------------------\n","[2022-07-23 08:34:03,708][HYDRA] | hydra    | pkg://hydra.conf                                                      |\n","[2022-07-23 08:34:03,709][HYDRA] | main     | file:///content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp/config |\n","[2022-07-23 08:34:03,709][HYDRA] | schema   | structured://                                                         |\n","[2022-07-23 08:34:03,709][HYDRA] ------------------------------------------------------------------------------------\n","[2022-07-23 08:34:03,785][HYDRA] \n","[2022-07-23 08:34:03,785][HYDRA] Defaults Tree\n","[2022-07-23 08:34:03,785][HYDRA] *************\n","[2022-07-23 08:34:03,785][HYDRA] <root>:\n","[2022-07-23 08:34:03,785][HYDRA]   hydra/config:\n","[2022-07-23 08:34:03,785][HYDRA]     hydra/output: default\n","[2022-07-23 08:34:03,785][HYDRA]     hydra/launcher: basic\n","[2022-07-23 08:34:03,785][HYDRA]     hydra/sweeper: basic\n","[2022-07-23 08:34:03,785][HYDRA]     hydra/help: default\n","[2022-07-23 08:34:03,785][HYDRA]     hydra/hydra_help: default\n","[2022-07-23 08:34:03,785][HYDRA]     hydra/hydra_logging: default\n","[2022-07-23 08:34:03,785][HYDRA]     hydra/job_logging: default\n","[2022-07-23 08:34:03,785][HYDRA]     hydra/callbacks: null\n","[2022-07-23 08:34:03,785][HYDRA]     hydra/env: default\n","[2022-07-23 08:34:03,786][HYDRA]     _self_\n","[2022-07-23 08:34:03,786][HYDRA]   config:\n","[2022-07-23 08:34:03,786][HYDRA]     data/YAGO43kET\n","[2022-07-23 08:34:03,786][HYDRA]     model/JointGT\n","[2022-07-23 08:34:03,786][HYDRA]     _self_\n","[2022-07-23 08:34:03,862][HYDRA] \n","[2022-07-23 08:34:03,862][HYDRA] Defaults List\n","[2022-07-23 08:34:03,862][HYDRA] *************\n","[2022-07-23 08:34:03,862][HYDRA] | Config path                 | Package             | _self_ | Parent       | \n","[2022-07-23 08:34:03,862][HYDRA] ------------------------------------------------------------------------------\n","[2022-07-23 08:34:03,862][HYDRA] | hydra/output/default        | hydra               | False  | hydra/config |\n","[2022-07-23 08:34:03,862][HYDRA] | hydra/launcher/basic        | hydra.launcher      | False  | hydra/config |\n","[2022-07-23 08:34:03,862][HYDRA] | hydra/sweeper/basic         | hydra.sweeper       | False  | hydra/config |\n","[2022-07-23 08:34:03,862][HYDRA] | hydra/help/default          | hydra.help          | False  | hydra/config |\n","[2022-07-23 08:34:03,862][HYDRA] | hydra/hydra_help/default    | hydra.hydra_help    | False  | hydra/config |\n","[2022-07-23 08:34:03,863][HYDRA] | hydra/hydra_logging/default | hydra.hydra_logging | False  | hydra/config |\n","[2022-07-23 08:34:03,863][HYDRA] | hydra/job_logging/default   | hydra.job_logging   | False  | hydra/config |\n","[2022-07-23 08:34:03,863][HYDRA] | hydra/env/default           | hydra.env           | False  | hydra/config |\n","[2022-07-23 08:34:03,863][HYDRA] | hydra/config                | hydra               | True   | <root>       |\n","[2022-07-23 08:34:03,863][HYDRA] | data/YAGO43kET              | data                | False  | config       |\n","[2022-07-23 08:34:03,863][HYDRA] | model/JointGT               | model               | False  | config       |\n","[2022-07-23 08:34:03,863][HYDRA] | config                      |                     | True   | <root>       |\n","[2022-07-23 08:34:03,863][HYDRA] ------------------------------------------------------------------------------\n","[2022-07-23 08:34:03,995][HYDRA] Config\n","[2022-07-23 08:34:03,995][HYDRA] ******\n","[2022-07-23 08:34:03,999][HYDRA] data:\n","  name: YAGO43kET\n","  data_dir: data\n","  overwrite: false\n","  is_unigraph: false\n","  lowest_level: false\n","  remove_under_score: true\n","model:\n","  cuda: true\n","  debug: false\n","  save_dir: save\n","  name: JointGT\n","  tokenizer_path: pretrain_model/jointgt_bart\n","  model_path: pretrain_model/jointgt_bart\n","  train_dataset: ET_1_1_train.txt\n","  pretrained_model: Bart\n","  append_another_bos: false\n","  is_in_edge: true\n","  is_out_edge: true\n","  max_node_length: 50\n","  max_edge_length: 60\n","  max_epoch: 100\n","  train_batch_size: 8\n","  valid_batch_size: 8\n","  test_batch_size: 8\n","  num_workers: 4\n","  temperature: 0.5\n","  n_gpus: 1\n","  max_input_length: 256\n","  max_output_length: 128\n","  gradient_accumulation_steps: 1\n","  max_grad_norm: 1.0\n","  early_stopping: true\n","  optimizer:\n","    algorithm: Adam\n","    learning_rate: 1.0e-06\n","  valid:\n","    valid_dataset: ET_1_1_valid.txt\n","    valid_epoch: 1\n","    num_beams: 5\n","    length_penalty: 1.0\n","    max_output_length: 128\n","    clean_up_spaces: true\n","    save_outputs: true\n","    save_one_batch: true\n","  test:\n","    test_dataset: ET_1_1_test.txt\n","    save_outputs: true\n","preprocess:\n","  load_ET: false\n","  load_KG: true\n","  neighbor_sampling: true\n","  neighbor_num: 10\n","  num_layers: 1\n","\n","[2022-07-23 08:34:04,067][transformers.tokenization_utils_base][INFO] - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.\n","[2022-07-23 08:34:04,069][transformers.tokenization_utils_base][INFO] - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.\n","[2022-07-23 08:34:04,069][transformers.tokenization_utils_base][INFO] - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.\n","[2022-07-23 08:34:04,069][transformers.tokenization_utils_base][INFO] - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.\n","[2022-07-23 08:34:04,070][transformers.tokenization_utils_base][INFO] - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.\n","[2022-07-23 08:34:04,070][transformers.tokenization_utils_base][INFO] - loading file pretrain_model/jointgt_bart/vocab.json\n","[2022-07-23 08:34:04,070][transformers.tokenization_utils_base][INFO] - loading file pretrain_model/jointgt_bart/merges.txt\n","[2022-07-23 08:34:04,071][transformers.tokenization_utils_base][INFO] - loading file None\n","[2022-07-23 08:34:04,071][transformers.tokenization_utils_base][INFO] - loading file None\n","[2022-07-23 08:34:04,071][transformers.tokenization_utils_base][INFO] - loading file None\n","[2022-07-23 08:34:04,071][transformers.tokenization_utils_base][INFO] - loading file None\n","tcmalloc: large alloc 7651123200 bytes == 0xceea000 @  0x7fdfa7452b6b 0x7fdfa7472379 0x7fdef02e126e 0x7fdef02e29e2 0x7fdef165de19 0x7fdef165eb67 0x7fdef1a3b059 0x7fdef219fe6a 0x7fdef2182f8e 0x7fdef1d87cd5 0x7fdef1a3f0c0 0x7fdef2311e64 0x7fdef216fec4 0x7fdef2186287 0x7fdef1de1441 0x7fdf94d50d25 0x593784 0x548c51 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x4bc98a 0x532b86 0x594a96 0x515600 0x549e0e 0x593fce 0x5118f8\n","tcmalloc: large alloc 7651123200 bytes == 0x7fdc37f52000 @  0x7fdfa7452b6b 0x7fdfa7472379 0x7fdef02e126e 0x7fdef02e29e2 0x7fdef165de19 0x7fdef165eb67 0x7fdef1a3b059 0x7fdef219fe6a 0x7fdef2182f8e 0x7fdef1d87cd5 0x7fdef1a3f0c0 0x7fdef2311e64 0x7fdef216fec4 0x7fdef2186287 0x7fdef1de1441 0x7fdf94d50d25 0x593784 0x548c51 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x4bc98a 0x532b86 0x594a96 0x515600 0x549e0e 0x593fce 0x5118f8\n","[2022-07-23 08:34:09,634][transformers.configuration_utils][INFO] - loading configuration file pretrain_model/jointgt_bart/config.json\n","[2022-07-23 08:34:09,634][transformers.configuration_utils][INFO] - Model config BartConfig {\n","  \"activation_dropout\": 0.1,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"MyBartPretrain\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"d_model\": 768,\n","  \"decoder_attention_heads\": 12,\n","  \"decoder_ffn_dim\": 3072,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 12,\n","  \"encoder_ffn_dim\": 3072,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 6,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"length_penalty\": 1.0,\n","      \"max_length\": 128,\n","      \"min_length\": 12,\n","      \"num_beams\": 4\n","    },\n","    \"summarization_cnn\": {\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"num_beams\": 4\n","    },\n","    \"summarization_xsum\": {\n","      \"length_penalty\": 1.0,\n","      \"max_length\": 62,\n","      \"min_length\": 11,\n","      \"num_beams\": 6\n","    }\n","  },\n","  \"vocab_size\": 50265\n","}\n","\n","[2022-07-23 08:34:09,635][transformers.modeling_utils][INFO] - loading weights file pretrain_model/jointgt_bart/pytorch_model.bin\n","[2022-07-23 08:34:17,018][transformers.modeling_utils][INFO] - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.\n","\n","[2022-07-23 08:34:17,018][transformers.modeling_utils][INFO] - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at pretrain_model/jointgt_bart.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.\n","[2022-07-23 08:34:21,317][__main__][DEBUG] - Parameter model.model.shared.weight: torch.Size([50265, 768]), require_grad=True\n","[2022-07-23 08:34:21,318][__main__][DEBUG] - Parameter model.model.encoder.embed_positions.weight: torch.Size([1026, 768]), require_grad=True\n","[2022-07-23 08:34:21,318][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,318][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,319][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,319][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,319][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,319][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,319][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,320][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,320][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,320][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,320][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,320][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,320][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,321][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,321][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,321][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,321][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,321][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,322][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,322][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,322][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,322][__main__][DEBUG] - Parameter model.model.encoder.layers.0.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,322][__main__][DEBUG] - Parameter model.model.encoder.layers.0.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,323][__main__][DEBUG] - Parameter model.model.encoder.layers.0.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,323][__main__][DEBUG] - Parameter model.model.encoder.layers.0.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,323][__main__][DEBUG] - Parameter model.model.encoder.layers.0.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,323][__main__][DEBUG] - Parameter model.model.encoder.layers.0.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,323][__main__][DEBUG] - Parameter model.model.encoder.layers.0.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,324][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,324][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,324][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,324][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,324][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,324][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,325][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,325][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,325][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,325][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,325][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,325][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,326][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,326][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,326][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,326][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,326][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,327][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,327][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,327][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,327][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,327][__main__][DEBUG] - Parameter model.model.encoder.layers.1.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,328][__main__][DEBUG] - Parameter model.model.encoder.layers.1.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,328][__main__][DEBUG] - Parameter model.model.encoder.layers.1.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,328][__main__][DEBUG] - Parameter model.model.encoder.layers.1.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,328][__main__][DEBUG] - Parameter model.model.encoder.layers.1.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,328][__main__][DEBUG] - Parameter model.model.encoder.layers.1.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,329][__main__][DEBUG] - Parameter model.model.encoder.layers.1.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,329][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,329][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,329][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,329][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,329][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,330][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,330][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,330][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,330][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,330][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,331][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,331][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,331][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,331][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,331][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,331][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,332][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,332][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,332][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,332][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,332][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,333][__main__][DEBUG] - Parameter model.model.encoder.layers.2.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,333][__main__][DEBUG] - Parameter model.model.encoder.layers.2.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,333][__main__][DEBUG] - Parameter model.model.encoder.layers.2.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,333][__main__][DEBUG] - Parameter model.model.encoder.layers.2.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,333][__main__][DEBUG] - Parameter model.model.encoder.layers.2.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,333][__main__][DEBUG] - Parameter model.model.encoder.layers.2.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,334][__main__][DEBUG] - Parameter model.model.encoder.layers.2.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,334][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,334][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,334][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,334][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,335][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,335][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,335][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,335][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,335][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,335][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,336][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,336][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,336][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,336][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,336][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,337][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,337][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,337][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,337][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,337][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,338][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,338][__main__][DEBUG] - Parameter model.model.encoder.layers.3.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,338][__main__][DEBUG] - Parameter model.model.encoder.layers.3.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,338][__main__][DEBUG] - Parameter model.model.encoder.layers.3.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,338][__main__][DEBUG] - Parameter model.model.encoder.layers.3.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,339][__main__][DEBUG] - Parameter model.model.encoder.layers.3.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,339][__main__][DEBUG] - Parameter model.model.encoder.layers.3.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,339][__main__][DEBUG] - Parameter model.model.encoder.layers.3.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,339][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,339][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,339][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,340][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,340][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,340][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,340][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,345][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,345][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,345][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,345][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,346][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,346][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,346][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,346][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,346][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,346][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,347][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,347][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,347][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,347][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,347][__main__][DEBUG] - Parameter model.model.encoder.layers.4.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,348][__main__][DEBUG] - Parameter model.model.encoder.layers.4.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,348][__main__][DEBUG] - Parameter model.model.encoder.layers.4.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,348][__main__][DEBUG] - Parameter model.model.encoder.layers.4.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,348][__main__][DEBUG] - Parameter model.model.encoder.layers.4.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,349][__main__][DEBUG] - Parameter model.model.encoder.layers.4.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,349][__main__][DEBUG] - Parameter model.model.encoder.layers.4.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,349][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,349][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,349][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,350][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,350][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,350][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,350][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,350][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,350][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,351][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,351][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,351][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,351][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,351][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,351][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,352][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,352][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,352][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,352][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,352][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,353][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,353][__main__][DEBUG] - Parameter model.model.encoder.layers.5.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,353][__main__][DEBUG] - Parameter model.model.encoder.layers.5.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,353][__main__][DEBUG] - Parameter model.model.encoder.layers.5.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,353][__main__][DEBUG] - Parameter model.model.encoder.layers.5.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,353][__main__][DEBUG] - Parameter model.model.encoder.layers.5.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,354][__main__][DEBUG] - Parameter model.model.encoder.layers.5.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,354][__main__][DEBUG] - Parameter model.model.encoder.layers.5.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,354][__main__][DEBUG] - Parameter model.model.encoder.layernorm_embedding.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,354][__main__][DEBUG] - Parameter model.model.encoder.layernorm_embedding.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,354][__main__][DEBUG] - Parameter model.model.decoder.embed_positions.weight: torch.Size([1026, 768]), require_grad=True\n","[2022-07-23 08:34:21,355][__main__][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,355][__main__][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,355][__main__][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,355][__main__][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,355][__main__][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,355][__main__][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,356][__main__][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,356][__main__][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,356][__main__][DEBUG] - Parameter model.model.decoder.layers.0.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,356][__main__][DEBUG] - Parameter model.model.decoder.layers.0.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,356][__main__][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,357][__main__][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,357][__main__][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,357][__main__][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,357][__main__][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,357][__main__][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,357][__main__][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,358][__main__][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,358][__main__][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,358][__main__][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,358][__main__][DEBUG] - Parameter model.model.decoder.layers.0.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,358][__main__][DEBUG] - Parameter model.model.decoder.layers.0.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,359][__main__][DEBUG] - Parameter model.model.decoder.layers.0.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,359][__main__][DEBUG] - Parameter model.model.decoder.layers.0.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,359][__main__][DEBUG] - Parameter model.model.decoder.layers.0.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,359][__main__][DEBUG] - Parameter model.model.decoder.layers.0.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,359][__main__][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,360][__main__][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,360][__main__][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,360][__main__][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,360][__main__][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,360][__main__][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,360][__main__][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,361][__main__][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,361][__main__][DEBUG] - Parameter model.model.decoder.layers.1.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,361][__main__][DEBUG] - Parameter model.model.decoder.layers.1.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,361][__main__][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,361][__main__][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,362][__main__][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,362][__main__][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,362][__main__][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,362][__main__][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,362][__main__][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,362][__main__][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,363][__main__][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,363][__main__][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,363][__main__][DEBUG] - Parameter model.model.decoder.layers.1.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,363][__main__][DEBUG] - Parameter model.model.decoder.layers.1.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,363][__main__][DEBUG] - Parameter model.model.decoder.layers.1.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,364][__main__][DEBUG] - Parameter model.model.decoder.layers.1.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,364][__main__][DEBUG] - Parameter model.model.decoder.layers.1.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,364][__main__][DEBUG] - Parameter model.model.decoder.layers.1.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,364][__main__][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,364][__main__][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,365][__main__][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,365][__main__][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,365][__main__][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,365][__main__][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,365][__main__][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,366][__main__][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,366][__main__][DEBUG] - Parameter model.model.decoder.layers.2.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,366][__main__][DEBUG] - Parameter model.model.decoder.layers.2.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,366][__main__][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,366][__main__][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,366][__main__][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,367][__main__][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,367][__main__][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,367][__main__][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,367][__main__][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,367][__main__][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,368][__main__][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,368][__main__][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,368][__main__][DEBUG] - Parameter model.model.decoder.layers.2.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,368][__main__][DEBUG] - Parameter model.model.decoder.layers.2.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,368][__main__][DEBUG] - Parameter model.model.decoder.layers.2.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,368][__main__][DEBUG] - Parameter model.model.decoder.layers.2.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,369][__main__][DEBUG] - Parameter model.model.decoder.layers.2.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,369][__main__][DEBUG] - Parameter model.model.decoder.layers.2.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,369][__main__][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,369][__main__][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,369][__main__][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,370][__main__][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,370][__main__][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,370][__main__][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,370][__main__][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,370][__main__][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,370][__main__][DEBUG] - Parameter model.model.decoder.layers.3.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,371][__main__][DEBUG] - Parameter model.model.decoder.layers.3.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,371][__main__][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,450][__main__][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,450][__main__][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,451][__main__][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,451][__main__][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,451][__main__][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,451][__main__][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,452][__main__][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,452][__main__][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,452][__main__][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,452][__main__][DEBUG] - Parameter model.model.decoder.layers.3.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,452][__main__][DEBUG] - Parameter model.model.decoder.layers.3.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,452][__main__][DEBUG] - Parameter model.model.decoder.layers.3.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,452][__main__][DEBUG] - Parameter model.model.decoder.layers.3.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,453][__main__][DEBUG] - Parameter model.model.decoder.layers.3.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,453][__main__][DEBUG] - Parameter model.model.decoder.layers.3.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,453][__main__][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,453][__main__][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,453][__main__][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,453][__main__][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,453][__main__][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,453][__main__][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,453][__main__][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,454][__main__][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,454][__main__][DEBUG] - Parameter model.model.decoder.layers.4.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,454][__main__][DEBUG] - Parameter model.model.decoder.layers.4.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,454][__main__][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,454][__main__][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,454][__main__][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,454][__main__][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,455][__main__][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,455][__main__][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,455][__main__][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,455][__main__][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,455][__main__][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,455][__main__][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,455][__main__][DEBUG] - Parameter model.model.decoder.layers.4.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,456][__main__][DEBUG] - Parameter model.model.decoder.layers.4.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,456][__main__][DEBUG] - Parameter model.model.decoder.layers.4.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,456][__main__][DEBUG] - Parameter model.model.decoder.layers.4.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,456][__main__][DEBUG] - Parameter model.model.decoder.layers.4.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,456][__main__][DEBUG] - Parameter model.model.decoder.layers.4.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,456][__main__][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,456][__main__][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,456][__main__][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,456][__main__][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,456][__main__][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,457][__main__][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,457][__main__][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,457][__main__][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,457][__main__][DEBUG] - Parameter model.model.decoder.layers.5.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,457][__main__][DEBUG] - Parameter model.model.decoder.layers.5.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,457][__main__][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,457][__main__][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,457][__main__][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,457][__main__][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,457][__main__][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,458][__main__][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,458][__main__][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:34:21,458][__main__][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,458][__main__][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,458][__main__][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,458][__main__][DEBUG] - Parameter model.model.decoder.layers.5.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:34:21,458][__main__][DEBUG] - Parameter model.model.decoder.layers.5.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:34:21,458][__main__][DEBUG] - Parameter model.model.decoder.layers.5.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:34:21,458][__main__][DEBUG] - Parameter model.model.decoder.layers.5.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,458][__main__][DEBUG] - Parameter model.model.decoder.layers.5.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,459][__main__][DEBUG] - Parameter model.model.decoder.layers.5.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,459][__main__][DEBUG] - Parameter model.model.decoder.layernorm_embedding.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:34:21,459][__main__][DEBUG] - Parameter model.model.decoder.layernorm_embedding.bias: torch.Size([768]), require_grad=True\n","Epoch:   0% 0/100 [00:00<?, ?it/s][2022-07-23 08:34:21,461][__main__][DEBUG] - Starting training!\n","\n","Iteration:   0% 0/422 [00:00<?, ?it/s]\u001b[AThe current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","[2022-07-23 08:34:22,633][__main__][DEBUG] - batch 0: tensor([[    0,   646, 44143,  ...,     1,     1,     1],\n","        [    0,   646, 44143,  ...,     1,     1,     1],\n","        [    0,   646, 44143,  ..., 29015,   742,     2],\n","        ...,\n","        [    0,   646, 44143,  ...,     1,     1,     1],\n","        [    0,   646, 44143,  ...,     1,     1,     1],\n","        [    0,   646, 44143,  ..., 29015,   742,     2]], device='cuda:0') \n","[2022-07-23 08:34:22,635][__main__][DEBUG] - batch 1: tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0') \n","[2022-07-23 08:34:22,637][__main__][DEBUG] - batch 2: tensor([[    0,  2136,  4135,  ...,     1,     1,     1],\n","        [    0, 47764,   636,  ...,     1,     1,     1],\n","        [    0, 47764,   636,  ...,     1,     1,     1],\n","        ...,\n","        [    0,  2136,  4135,  ...,     1,     1,     1],\n","        [    0,  2136,  4135,  ...,     1,     1,     1],\n","        [    0, 47764,   636,  ...,     1,     1,     1]], device='cuda:0') \n","[2022-07-23 08:34:22,639][__main__][DEBUG] - batch 3: tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0') \n","[2022-07-23 08:34:22,641][__main__][DEBUG] - batch 4: tensor([[50, 50, 50,  ..., 50, 50, 50],\n","        [50, 50, 50,  ..., 50, 50, 50],\n","        [50, 50, 50,  ..., 50, 50, 50],\n","        ...,\n","        [50, 50, 50,  ..., 50, 50, 50],\n","        [50, 50, 50,  ..., 50, 50, 50],\n","        [50, 50, 50,  ..., 50, 50, 50]], device='cuda:0') \n","[2022-07-23 08:34:22,643][__main__][DEBUG] - batch 5: tensor([[60, 60, 60,  ..., 60, 60, 60],\n","        [60, 60, 60,  ..., 60, 60, 60],\n","        [60, 60, 60,  ..., 60, 60, 60],\n","        ...,\n","        [60, 60, 60,  ..., 60, 60, 60],\n","        [60, 60, 60,  ..., 60, 60, 60],\n","        [60, 60, 60,  ..., 60, 60, 60]], device='cuda:0') \n","[2022-07-23 08:34:22,644][__main__][DEBUG] - batch 6: tensor([[ 3],\n","        [ 2],\n","        [ 9],\n","        [ 2],\n","        [ 4],\n","        [ 7],\n","        [ 3],\n","        [70]], device='cuda:0') \n","[2022-07-23 08:34:22,645][__main__][DEBUG] - batch 7: tensor([[1],\n","        [1],\n","        [1],\n","        [1],\n","        [1],\n","        [2],\n","        [1],\n","        [2]], device='cuda:0') \n","[2022-07-23 08:34:22,654][__main__][DEBUG] - batch 8: tensor([[[60,  0,  1,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         ...,\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60]],\n","\n","        [[60, 60, 60,  ..., 60, 60, 60],\n","         [ 0, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         ...,\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60]],\n","\n","        [[60, 60,  2,  ..., 60, 60, 60],\n","         [60, 60,  3,  ..., 60, 60, 60],\n","         [60, 10, 60,  ..., 60, 60, 60],\n","         ...,\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60]],\n","\n","        ...,\n","\n","        [[60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         ...,\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60]],\n","\n","        [[60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [ 1,  0, 60,  ..., 60, 60, 60],\n","         ...,\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60]],\n","\n","        [[60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         ...,\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60],\n","         [60, 60, 60,  ..., 60, 60, 60]]], device='cuda:0') \n","\n","Iteration:   0% 1/422 [00:01<11:57,  1.70s/it]\u001b[A\n","Iteration:   0% 2/422 [00:02<06:49,  1.02it/s]\u001b[A\n","Iteration:   1% 3/422 [00:02<05:10,  1.35it/s]\u001b[A\n","Iteration:   1% 4/422 [00:03<04:22,  1.59it/s]\u001b[A\n","Iteration:   1% 5/422 [00:03<03:57,  1.76it/s]\u001b[A\n","Iteration:   1% 6/422 [00:04<03:42,  1.87it/s]\u001b[A\n","Iteration:   2% 7/422 [00:04<03:31,  1.96it/s]\u001b[A\n","Iteration:   2% 8/422 [00:04<03:24,  2.03it/s]\u001b[A\n","Iteration:   2% 9/422 [00:05<03:19,  2.07it/s]\u001b[A\n","Iteration:   2% 10/422 [00:05<03:16,  2.10it/s]\u001b[A\n","Iteration:   3% 11/422 [00:06<03:14,  2.12it/s]\u001b[A\n","Iteration:   3% 12/422 [00:06<03:12,  2.13it/s]\u001b[A\n","Iteration:   3% 13/422 [00:07<03:10,  2.15it/s]\u001b[A\n","Iteration:   3% 14/422 [00:07<03:09,  2.15it/s]\u001b[A\n","Iteration:   4% 15/422 [00:08<03:09,  2.15it/s]\u001b[A\n","Iteration:   4% 16/422 [00:08<03:08,  2.16it/s]\u001b[A\n","Iteration:   4% 17/422 [00:09<03:07,  2.16it/s]\u001b[A\n","Iteration:   4% 18/422 [00:09<03:06,  2.16it/s]\u001b[A\n","Iteration:   5% 19/422 [00:10<03:06,  2.16it/s]\u001b[A\n","Iteration:   5% 20/422 [00:10<03:05,  2.16it/s]\u001b[A\n","Iteration:   5% 21/422 [00:10<03:05,  2.17it/s]\u001b[A\n","Iteration:   5% 22/422 [00:11<03:04,  2.17it/s]\u001b[A\n","Iteration:   5% 23/422 [00:11<03:04,  2.17it/s]\u001b[A\n","Iteration:   6% 24/422 [00:12<03:03,  2.17it/s]\u001b[A\n","Iteration:   6% 25/422 [00:12<03:03,  2.17it/s]\u001b[A\n","Iteration:   6% 26/422 [00:13<03:02,  2.17it/s]\u001b[A\n","Iteration:   6% 27/422 [00:13<03:02,  2.17it/s]\u001b[A\n","Iteration:   7% 28/422 [00:14<03:01,  2.17it/s]\u001b[A\n","Iteration:   7% 29/422 [00:14<03:01,  2.17it/s]\u001b[A\n","Iteration:   7% 30/422 [00:15<03:00,  2.17it/s]\u001b[A\n","Iteration:   7% 31/422 [00:15<03:00,  2.17it/s]\u001b[A\n","Iteration:   8% 32/422 [00:16<02:59,  2.17it/s]\u001b[A\n","Iteration:   8% 33/422 [00:16<02:59,  2.17it/s]\u001b[A\n","Iteration:   8% 34/422 [00:16<02:59,  2.16it/s]\u001b[A\n","Iteration:   8% 35/422 [00:17<02:58,  2.16it/s]\u001b[A\n","Iteration:   9% 36/422 [00:17<02:58,  2.17it/s]\u001b[A\n","Iteration:   9% 37/422 [00:18<02:57,  2.16it/s]\u001b[A\n","Iteration:   9% 38/422 [00:18<02:57,  2.16it/s]\u001b[A\n","Iteration:   9% 39/422 [00:19<02:56,  2.17it/s]\u001b[A\n","Iteration:   9% 40/422 [00:19<02:56,  2.17it/s]\u001b[A\n","Iteration:  10% 41/422 [00:20<02:55,  2.16it/s]\u001b[A\n","Iteration:  10% 42/422 [00:20<02:55,  2.16it/s]\u001b[A\n","Iteration:  10% 43/422 [00:21<02:54,  2.17it/s]\u001b[A\n","Iteration:  10% 44/422 [00:21<02:54,  2.17it/s]\u001b[A\n","Iteration:  11% 45/422 [00:22<02:54,  2.17it/s]\u001b[A\n","Iteration:  11% 46/422 [00:22<02:53,  2.17it/s]\u001b[A\n","Iteration:  11% 47/422 [00:22<02:53,  2.16it/s]\u001b[A\n","Iteration:  11% 48/422 [00:23<02:52,  2.16it/s]\u001b[A\n","Iteration:  12% 49/422 [00:23<02:52,  2.16it/s]\u001b[A\n","Iteration:  12% 50/422 [00:24<02:51,  2.17it/s]\u001b[A\n","Iteration:  12% 51/422 [00:24<02:51,  2.17it/s]\u001b[A\n","Iteration:  12% 52/422 [00:25<02:50,  2.17it/s]\u001b[A\n","Iteration:  13% 53/422 [00:25<02:50,  2.17it/s]\u001b[A\n","Iteration:  13% 54/422 [00:26<02:49,  2.17it/s]\u001b[A\n","Iteration:  13% 55/422 [00:26<02:49,  2.17it/s]\u001b[A\n","Iteration:  13% 56/422 [00:27<02:49,  2.16it/s]\u001b[A\n","Iteration:  14% 57/422 [00:27<02:48,  2.17it/s]\u001b[A\n","Iteration:  14% 58/422 [00:28<02:47,  2.17it/s]\u001b[A\n","Iteration:  14% 59/422 [00:28<02:47,  2.16it/s]\u001b[A\n","Iteration:  14% 60/422 [00:28<02:47,  2.16it/s]\u001b[A\n","Iteration:  14% 61/422 [00:29<02:46,  2.16it/s]\u001b[A\n","Iteration:  15% 62/422 [00:29<02:46,  2.17it/s]\u001b[A\n","Iteration:  15% 63/422 [00:30<02:45,  2.17it/s]\u001b[A\n","Iteration:  15% 64/422 [00:30<02:45,  2.17it/s]\u001b[A\n","Iteration:  15% 65/422 [00:31<02:44,  2.17it/s]\u001b[A\n","Iteration:  16% 66/422 [00:31<02:44,  2.17it/s]\u001b[A\n","Iteration:  16% 67/422 [00:32<02:43,  2.17it/s]\u001b[A\n","Iteration:  16% 68/422 [00:32<02:43,  2.16it/s]\u001b[A\n","Iteration:  16% 69/422 [00:33<02:43,  2.16it/s]\u001b[A\n","Iteration:  17% 70/422 [00:33<02:42,  2.16it/s]\u001b[A\n","Iteration:  17% 71/422 [00:34<02:42,  2.17it/s]\u001b[A\n","Iteration:  17% 72/422 [00:34<02:41,  2.16it/s]\u001b[A\n","Iteration:  17% 73/422 [00:34<02:41,  2.16it/s]\u001b[A\n","Iteration:  18% 74/422 [00:35<02:41,  2.16it/s]\u001b[A\n","Iteration:  18% 75/422 [00:35<02:40,  2.16it/s]\u001b[A\n","Iteration:  18% 76/422 [00:36<02:40,  2.16it/s]\u001b[A\n","Iteration:  18% 77/422 [00:36<02:39,  2.16it/s]\u001b[A\n","Iteration:  18% 78/422 [00:37<02:39,  2.16it/s]\u001b[A\n","Iteration:  19% 79/422 [00:37<02:38,  2.16it/s]\u001b[A\n","Iteration:  19% 80/422 [00:38<02:38,  2.16it/s]\u001b[A\n","Iteration:  19% 81/422 [00:38<02:38,  2.15it/s]\u001b[A\n","Iteration:  19% 82/422 [00:39<02:37,  2.16it/s]\u001b[A\n","Iteration:  20% 83/422 [00:39<02:36,  2.16it/s]\u001b[A\n","Iteration:  20% 84/422 [00:40<02:36,  2.16it/s]\u001b[A\n","Iteration:  20% 85/422 [00:40<02:35,  2.16it/s]\u001b[A\n","Iteration:  20% 86/422 [00:40<02:35,  2.17it/s]\u001b[A\n","Iteration:  21% 87/422 [00:41<02:34,  2.17it/s]\u001b[A\n","Iteration:  21% 88/422 [00:41<02:34,  2.17it/s]\u001b[A\n","Iteration:  21% 89/422 [00:42<02:33,  2.17it/s]\u001b[A\n","Iteration:  21% 90/422 [00:42<02:33,  2.17it/s]\u001b[A\n","Iteration:  22% 91/422 [00:43<02:32,  2.17it/s]\u001b[A\n","Iteration:  22% 92/422 [00:43<02:32,  2.16it/s]\u001b[A\n","Iteration:  22% 93/422 [00:44<02:32,  2.16it/s]\u001b[A\n","Iteration:  22% 94/422 [00:44<02:31,  2.16it/s]\u001b[A\n","Iteration:  23% 95/422 [00:45<02:31,  2.17it/s]\u001b[A\n","Iteration:  23% 96/422 [00:45<02:30,  2.17it/s]\u001b[A\n","Iteration:  23% 97/422 [00:46<02:29,  2.17it/s]\u001b[A\n","Iteration:  23% 98/422 [00:46<02:29,  2.17it/s]\u001b[A\n","Iteration:  23% 99/422 [00:46<02:29,  2.17it/s]\u001b[A\n","Iteration:  24% 100/422 [00:47<02:28,  2.16it/s]\u001b[A\n","Iteration:  24% 101/422 [00:47<02:28,  2.16it/s]\u001b[A\n","Iteration:  24% 102/422 [00:48<02:28,  2.16it/s]\u001b[A\n","Iteration:  24% 103/422 [00:48<02:27,  2.16it/s]\u001b[A\n","Iteration:  25% 104/422 [00:49<02:27,  2.16it/s]\u001b[A\n","Iteration:  25% 105/422 [00:49<02:26,  2.16it/s]\u001b[A\n","Iteration:  25% 106/422 [00:50<02:26,  2.16it/s]\u001b[A\n","Iteration:  25% 107/422 [00:50<02:26,  2.16it/s]\u001b[A\n","Iteration:  26% 108/422 [00:51<02:25,  2.16it/s]\u001b[A\n","Iteration:  26% 109/422 [00:51<02:24,  2.16it/s]\u001b[A\n","Iteration:  26% 110/422 [00:52<02:24,  2.15it/s]\u001b[A\n","Iteration:  26% 111/422 [00:52<02:24,  2.16it/s]\u001b[A\n","Iteration:  27% 112/422 [00:52<02:23,  2.16it/s]\u001b[A\n","Iteration:  27% 113/422 [00:53<02:23,  2.16it/s]\u001b[A\n","Iteration:  27% 114/422 [00:53<02:22,  2.15it/s]\u001b[A\n","Iteration:  27% 115/422 [00:54<02:22,  2.16it/s]\u001b[A\n","Iteration:  27% 116/422 [00:54<02:21,  2.16it/s]\u001b[A\n","Iteration:  28% 117/422 [00:55<02:21,  2.16it/s]\u001b[A\n","Iteration:  28% 118/422 [00:55<02:20,  2.16it/s]\u001b[A\n","Iteration:  28% 119/422 [00:56<02:20,  2.16it/s]\u001b[A\n","Iteration:  28% 120/422 [00:56<02:21,  2.14it/s]\u001b[A\n","Iteration:  29% 121/422 [00:57<02:20,  2.15it/s]\u001b[A\n","Iteration:  29% 122/422 [00:57<02:19,  2.16it/s]\u001b[A\n","Iteration:  29% 123/422 [00:58<02:18,  2.16it/s]\u001b[A\n","Iteration:  29% 124/422 [00:58<02:17,  2.16it/s]\u001b[A\n","Iteration:  30% 125/422 [00:59<02:17,  2.16it/s]\u001b[A\n","Iteration:  30% 126/422 [00:59<02:16,  2.16it/s]\u001b[A\n","Iteration:  30% 127/422 [00:59<02:16,  2.17it/s]\u001b[A\n","Iteration:  30% 128/422 [01:00<02:15,  2.17it/s]\u001b[A\n","Iteration:  31% 129/422 [01:00<02:15,  2.17it/s]\u001b[A\n","Iteration:  31% 130/422 [01:01<02:14,  2.16it/s]\u001b[A\n","Iteration:  31% 131/422 [01:01<02:14,  2.17it/s]\u001b[A\n","Iteration:  31% 132/422 [01:02<02:14,  2.16it/s]\u001b[A\n","Iteration:  32% 133/422 [01:02<02:13,  2.16it/s]\u001b[A\n","Iteration:  32% 134/422 [01:03<02:12,  2.17it/s]\u001b[A\n","Iteration:  32% 135/422 [01:03<02:12,  2.17it/s]\u001b[A\n","Iteration:  32% 136/422 [01:04<02:12,  2.16it/s]\u001b[A\n","Iteration:  32% 137/422 [01:04<02:11,  2.16it/s]\u001b[A\n","Iteration:  33% 138/422 [01:05<02:11,  2.17it/s]\u001b[A\n","Iteration:  33% 139/422 [01:05<02:10,  2.17it/s]\u001b[A\n","Iteration:  33% 140/422 [01:05<02:10,  2.16it/s]\u001b[A\n","Iteration:  33% 141/422 [01:06<02:09,  2.16it/s]\u001b[A\n","Iteration:  34% 142/422 [01:06<02:09,  2.16it/s]\u001b[A\n","Iteration:  34% 143/422 [01:07<02:09,  2.16it/s]\u001b[A\n","Iteration:  34% 144/422 [01:07<02:08,  2.16it/s]\u001b[A\n","Iteration:  34% 145/422 [01:08<02:08,  2.16it/s]\u001b[A\n","Iteration:  35% 146/422 [01:08<02:07,  2.16it/s]\u001b[A\n","Iteration:  35% 147/422 [01:09<02:07,  2.16it/s]\u001b[A\n","Iteration:  35% 148/422 [01:09<02:06,  2.16it/s]\u001b[A\n","Iteration:  35% 149/422 [01:10<02:06,  2.16it/s]\u001b[A\n","Iteration:  36% 150/422 [01:10<02:05,  2.16it/s]\u001b[A\n","Iteration:  36% 151/422 [01:11<02:05,  2.16it/s]\u001b[A\n","Iteration:  36% 152/422 [01:11<02:04,  2.16it/s]\u001b[A\n","Iteration:  36% 153/422 [01:11<02:04,  2.16it/s]\u001b[A\n","Iteration:  36% 154/422 [01:12<02:04,  2.16it/s]\u001b[A\n","Iteration:  37% 155/422 [01:12<02:03,  2.16it/s]\u001b[A\n","Iteration:  37% 156/422 [01:13<02:02,  2.16it/s]\u001b[A\n","Iteration:  37% 157/422 [01:13<02:02,  2.16it/s]\u001b[A\n","Iteration:  37% 158/422 [01:14<02:01,  2.17it/s]\u001b[A\n","Iteration:  38% 159/422 [01:14<02:01,  2.17it/s]\u001b[A\n","Iteration:  38% 160/422 [01:15<02:00,  2.17it/s]\u001b[A\n","Iteration:  38% 161/422 [01:15<02:00,  2.17it/s]\u001b[A\n","Iteration:  38% 162/422 [01:16<01:59,  2.17it/s]\u001b[A\n","Iteration:  39% 163/422 [01:16<01:59,  2.17it/s]\u001b[A\n","Iteration:  39% 164/422 [01:17<01:59,  2.17it/s]\u001b[A\n","Iteration:  39% 165/422 [01:17<01:58,  2.17it/s]\u001b[A\n","Iteration:  39% 166/422 [01:17<01:58,  2.17it/s]\u001b[A\n","Iteration:  40% 167/422 [01:18<01:57,  2.17it/s]\u001b[A\n","Iteration:  40% 168/422 [01:18<01:57,  2.17it/s]\u001b[A\n","Iteration:  40% 169/422 [01:19<01:56,  2.16it/s]\u001b[A\n","Iteration:  40% 170/422 [01:19<01:56,  2.16it/s]\u001b[A\n","Iteration:  41% 171/422 [01:20<01:56,  2.15it/s]\u001b[A\n","Iteration:  41% 172/422 [01:20<01:56,  2.14it/s]\u001b[A\n","Iteration:  41% 173/422 [01:21<01:56,  2.14it/s]\u001b[A\n","Iteration:  41% 174/422 [01:21<01:55,  2.14it/s]\u001b[A\n","Iteration:  41% 175/422 [01:22<01:55,  2.14it/s]\u001b[A\n","Iteration:  42% 176/422 [01:22<01:54,  2.14it/s]\u001b[A\n","Iteration:  42% 177/422 [01:23<01:54,  2.15it/s]\u001b[A\n","Iteration:  42% 178/422 [01:23<01:53,  2.15it/s]\u001b[A\n","Iteration:  42% 179/422 [01:24<01:52,  2.15it/s]\u001b[A\n","Iteration:  43% 180/422 [01:24<01:52,  2.16it/s]\u001b[A\n","Iteration:  43% 181/422 [01:24<01:51,  2.16it/s]\u001b[A\n","Iteration:  43% 182/422 [01:25<01:51,  2.16it/s]\u001b[A\n","Iteration:  43% 183/422 [01:25<01:50,  2.16it/s]\u001b[A\n","Iteration:  44% 184/422 [01:26<01:50,  2.16it/s]\u001b[A\n","Iteration:  44% 185/422 [01:26<01:49,  2.16it/s]\u001b[A\n","Iteration:  44% 186/422 [01:27<01:49,  2.16it/s]\u001b[A\n","Iteration:  44% 187/422 [01:27<01:48,  2.16it/s]\u001b[A\n","Iteration:  45% 188/422 [01:28<01:48,  2.16it/s]\u001b[A\n","Iteration:  45% 189/422 [01:28<01:48,  2.15it/s]\u001b[A\n","Iteration:  45% 190/422 [01:29<01:47,  2.16it/s]\u001b[A\n","Iteration:  45% 191/422 [01:29<01:46,  2.16it/s]\u001b[A\n","Iteration:  45% 192/422 [01:30<01:46,  2.16it/s]\u001b[A\n","Iteration:  46% 193/422 [01:30<01:45,  2.16it/s]\u001b[A\n","Iteration:  46% 194/422 [01:30<01:45,  2.16it/s]\u001b[A\n","Iteration:  46% 195/422 [01:31<01:45,  2.16it/s]\u001b[A\n","Iteration:  46% 196/422 [01:31<01:44,  2.16it/s]\u001b[A\n","Iteration:  47% 197/422 [01:32<01:43,  2.16it/s]\u001b[A\n","Iteration:  47% 198/422 [01:32<01:43,  2.16it/s]\u001b[A\n","Iteration:  47% 199/422 [01:33<01:42,  2.17it/s]\u001b[A\n","Iteration:  47% 200/422 [01:33<01:42,  2.17it/s]\u001b[A\n","Iteration:  48% 201/422 [01:34<01:41,  2.17it/s]\u001b[A\n","Iteration:  48% 202/422 [01:34<01:41,  2.17it/s]\u001b[A\n","Iteration:  48% 203/422 [01:35<01:41,  2.17it/s]\u001b[A\n","Iteration:  48% 204/422 [01:35<01:40,  2.17it/s]\u001b[A\n","Iteration:  49% 205/422 [01:36<01:40,  2.17it/s]\u001b[A\n","Iteration:  49% 206/422 [01:36<01:39,  2.17it/s]\u001b[A\n","Iteration:  49% 207/422 [01:36<01:39,  2.17it/s]\u001b[A\n","Iteration:  49% 208/422 [01:37<01:38,  2.17it/s]\u001b[A\n","Iteration:  50% 209/422 [01:37<01:38,  2.16it/s]\u001b[A\n","Iteration:  50% 210/422 [01:38<01:38,  2.16it/s]\u001b[A\n","Iteration:  50% 211/422 [01:38<01:37,  2.16it/s]\u001b[A\n","Iteration:  50% 212/422 [01:39<01:37,  2.16it/s]\u001b[A\n","Iteration:  50% 213/422 [01:39<01:36,  2.16it/s]\u001b[A\n","Iteration:  51% 214/422 [01:40<01:36,  2.16it/s]\u001b[A\n","Iteration:  51% 215/422 [01:40<01:35,  2.16it/s]\u001b[A\n","Iteration:  51% 216/422 [01:41<01:35,  2.16it/s]\u001b[A\n","Iteration:  51% 217/422 [01:41<01:34,  2.16it/s]\u001b[A\n","Iteration:  52% 218/422 [01:42<01:34,  2.16it/s]\u001b[A\n","Iteration:  52% 219/422 [01:42<01:33,  2.16it/s]\u001b[A\n","Iteration:  52% 220/422 [01:42<01:33,  2.16it/s]\u001b[A\n","Iteration:  52% 221/422 [01:43<01:32,  2.17it/s]\u001b[A\n","Iteration:  53% 222/422 [01:43<01:32,  2.16it/s]\u001b[A\n","Iteration:  53% 223/422 [01:44<01:32,  2.16it/s]\u001b[A\n","Iteration:  53% 224/422 [01:44<01:31,  2.16it/s]\u001b[A\n","Iteration:  53% 225/422 [01:45<01:31,  2.16it/s]\u001b[A\n","Iteration:  54% 226/422 [01:45<01:31,  2.15it/s]\u001b[A\n","Iteration:  54% 227/422 [01:46<01:30,  2.15it/s]\u001b[A\n","Iteration:  54% 228/422 [01:46<01:29,  2.16it/s]\u001b[A\n","Iteration:  54% 229/422 [01:47<01:29,  2.16it/s]\u001b[A\n","Iteration:  55% 230/422 [01:47<01:28,  2.16it/s]\u001b[A\n","Iteration:  55% 231/422 [01:48<01:28,  2.17it/s]\u001b[A\n","Iteration:  55% 232/422 [01:48<01:27,  2.17it/s]\u001b[A\n","Iteration:  55% 233/422 [01:48<01:27,  2.17it/s]\u001b[A\n","Iteration:  55% 234/422 [01:49<01:26,  2.17it/s]\u001b[A\n","Iteration:  56% 235/422 [01:49<01:26,  2.16it/s]\u001b[A\n","Iteration:  56% 236/422 [01:50<01:26,  2.16it/s]\u001b[A\n","Iteration:  56% 237/422 [01:50<01:25,  2.17it/s]\u001b[A\n","Iteration:  56% 238/422 [01:51<01:24,  2.17it/s]\u001b[A\n","Iteration:  57% 239/422 [01:51<01:24,  2.17it/s]\u001b[A\n","Iteration:  57% 240/422 [01:52<01:24,  2.17it/s]\u001b[A\n","Iteration:  57% 241/422 [01:52<01:23,  2.17it/s]\u001b[A\n","Iteration:  57% 242/422 [01:53<01:22,  2.17it/s]\u001b[A\n","Iteration:  58% 243/422 [01:53<01:22,  2.17it/s]\u001b[A\n","Iteration:  58% 244/422 [01:54<01:22,  2.17it/s]\u001b[A\n","Iteration:  58% 245/422 [01:54<01:21,  2.17it/s]\u001b[A\n","Iteration:  58% 246/422 [01:54<01:21,  2.17it/s]\u001b[A\n","Iteration:  59% 247/422 [01:55<01:20,  2.17it/s]\u001b[A\n","Iteration:  59% 248/422 [01:55<01:20,  2.17it/s]\u001b[A\n","Iteration:  59% 249/422 [01:56<01:19,  2.17it/s]\u001b[A\n","Iteration:  59% 250/422 [01:56<01:19,  2.17it/s]\u001b[A\n","Iteration:  59% 251/422 [01:57<01:18,  2.17it/s]\u001b[A\n","Iteration:  60% 252/422 [01:57<01:18,  2.17it/s]\u001b[A\n","Iteration:  60% 253/422 [01:58<01:17,  2.17it/s]\u001b[A\n","Iteration:  60% 254/422 [01:58<01:17,  2.17it/s]\u001b[A\n","Iteration:  60% 255/422 [01:59<01:17,  2.16it/s]\u001b[A\n","Iteration:  61% 256/422 [01:59<01:16,  2.16it/s]\u001b[A\n","Iteration:  61% 257/422 [02:00<01:16,  2.16it/s]\u001b[A\n","Iteration:  61% 258/422 [02:00<01:15,  2.16it/s]\u001b[A\n","Iteration:  61% 259/422 [02:00<01:15,  2.16it/s]\u001b[A\n","Iteration:  62% 260/422 [02:01<01:14,  2.16it/s]\u001b[A\n","Iteration:  62% 261/422 [02:01<01:14,  2.16it/s]\u001b[A\n","Iteration:  62% 262/422 [02:02<01:14,  2.16it/s]\u001b[A\n","Iteration:  62% 263/422 [02:02<01:13,  2.16it/s]\u001b[A\n","Iteration:  63% 264/422 [02:03<01:13,  2.16it/s]\u001b[A\n","Iteration:  63% 265/422 [02:03<01:12,  2.16it/s]\u001b[A\n","Iteration:  63% 266/422 [02:04<01:12,  2.16it/s]\u001b[A\n","Iteration:  63% 267/422 [02:04<01:11,  2.16it/s]\u001b[A\n","Iteration:  64% 268/422 [02:05<01:11,  2.16it/s]\u001b[A\n","Iteration:  64% 269/422 [02:05<01:10,  2.16it/s]\u001b[A\n","Iteration:  64% 270/422 [02:06<01:10,  2.16it/s]\u001b[A\n","Iteration:  64% 271/422 [02:06<01:09,  2.16it/s]\u001b[A\n","Iteration:  64% 272/422 [02:07<01:09,  2.16it/s]\u001b[A\n","Iteration:  65% 273/422 [02:07<01:08,  2.16it/s]\u001b[A\n","Iteration:  65% 274/422 [02:07<01:08,  2.16it/s]\u001b[A\n","Iteration:  65% 275/422 [02:08<01:07,  2.16it/s]\u001b[A\n","Iteration:  65% 276/422 [02:08<01:07,  2.16it/s]\u001b[A\n","Iteration:  66% 277/422 [02:09<01:06,  2.17it/s]\u001b[A\n","Iteration:  66% 278/422 [02:09<01:06,  2.16it/s]\u001b[A\n","Iteration:  66% 279/422 [02:10<01:06,  2.16it/s]\u001b[A\n","Iteration:  66% 280/422 [02:10<01:05,  2.17it/s]\u001b[A\n","Iteration:  67% 281/422 [02:11<01:05,  2.17it/s]\u001b[A\n","Iteration:  67% 282/422 [02:11<01:04,  2.17it/s]\u001b[A\n","Iteration:  67% 283/422 [02:12<01:04,  2.16it/s]\u001b[A\n","Iteration:  67% 284/422 [02:12<01:03,  2.16it/s]\u001b[A\n","Iteration:  68% 285/422 [02:13<01:03,  2.17it/s]\u001b[A\n","Iteration:  68% 286/422 [02:13<01:02,  2.17it/s]\u001b[A\n","Iteration:  68% 287/422 [02:13<01:02,  2.17it/s]\u001b[A\n","Iteration:  68% 288/422 [02:14<01:01,  2.17it/s]\u001b[A\n","Iteration:  68% 289/422 [02:14<01:01,  2.17it/s]\u001b[A\n","Iteration:  69% 290/422 [02:15<01:00,  2.17it/s]\u001b[A\n","Iteration:  69% 291/422 [02:15<01:00,  2.17it/s]\u001b[A\n","Iteration:  69% 292/422 [02:16<00:59,  2.17it/s]\u001b[A\n","Iteration:  69% 293/422 [02:16<00:59,  2.17it/s]\u001b[A\n","Iteration:  70% 294/422 [02:17<00:58,  2.17it/s]\u001b[A\n","Iteration:  70% 295/422 [02:17<00:58,  2.17it/s]\u001b[A\n","Iteration:  70% 296/422 [02:18<00:58,  2.16it/s]\u001b[A\n","Iteration:  70% 297/422 [02:18<00:57,  2.16it/s]\u001b[A\n","Iteration:  71% 298/422 [02:19<00:57,  2.16it/s]\u001b[A\n","Iteration:  71% 299/422 [02:19<00:56,  2.17it/s]\u001b[A\n","Iteration:  71% 300/422 [02:19<00:56,  2.17it/s]\u001b[A\n","Iteration:  71% 301/422 [02:20<00:55,  2.16it/s]\u001b[A\n","Iteration:  72% 302/422 [02:20<00:55,  2.16it/s]\u001b[A\n","Iteration:  72% 303/422 [02:21<00:54,  2.17it/s]\u001b[A\n","Iteration:  72% 304/422 [02:21<00:54,  2.17it/s]\u001b[A\n","Iteration:  72% 305/422 [02:22<00:53,  2.17it/s]\u001b[A\n","Iteration:  73% 306/422 [02:22<00:53,  2.17it/s]\u001b[A\n","Iteration:  73% 307/422 [02:23<00:53,  2.17it/s]\u001b[A\n","Iteration:  73% 308/422 [02:23<00:52,  2.17it/s]\u001b[A\n","Iteration:  73% 309/422 [02:24<00:52,  2.16it/s]\u001b[A\n","Iteration:  73% 310/422 [02:24<00:51,  2.16it/s]\u001b[A\n","Iteration:  74% 311/422 [02:25<00:51,  2.16it/s]\u001b[A\n","Iteration:  74% 312/422 [02:25<00:50,  2.17it/s]\u001b[A\n","Iteration:  74% 313/422 [02:25<00:50,  2.17it/s]\u001b[A\n","Iteration:  74% 314/422 [02:26<00:49,  2.17it/s]\u001b[A\n","Iteration:  75% 315/422 [02:26<00:49,  2.17it/s]\u001b[A\n","Iteration:  75% 316/422 [02:27<00:48,  2.17it/s]\u001b[A\n","Iteration:  75% 317/422 [02:27<00:48,  2.17it/s]\u001b[A\n","Iteration:  75% 318/422 [02:28<00:47,  2.17it/s]\u001b[A\n","Iteration:  76% 319/422 [02:28<00:47,  2.17it/s]\u001b[A\n","Iteration:  76% 320/422 [02:29<00:47,  2.17it/s]\u001b[A\n","Iteration:  76% 321/422 [02:29<00:46,  2.17it/s]\u001b[A\n","Iteration:  76% 322/422 [02:30<00:46,  2.15it/s]\u001b[A\n","Iteration:  77% 323/422 [02:30<00:45,  2.15it/s]\u001b[A\n","Iteration:  77% 324/422 [02:31<00:45,  2.16it/s]\u001b[A\n","Iteration:  77% 325/422 [02:31<00:44,  2.16it/s]\u001b[A\n","Iteration:  77% 326/422 [02:31<00:44,  2.16it/s]\u001b[A\n","Iteration:  77% 327/422 [02:32<00:43,  2.16it/s]\u001b[A\n","Iteration:  78% 328/422 [02:32<00:43,  2.16it/s]\u001b[A\n","Iteration:  78% 329/422 [02:33<00:43,  2.16it/s]\u001b[A\n","Iteration:  78% 330/422 [02:33<00:42,  2.16it/s]\u001b[A\n","Iteration:  78% 331/422 [02:34<00:42,  2.16it/s]\u001b[A\n","Iteration:  79% 332/422 [02:34<00:41,  2.15it/s]\u001b[A\n","Iteration:  79% 333/422 [02:35<00:41,  2.16it/s]\u001b[A\n","Iteration:  79% 334/422 [02:35<00:40,  2.16it/s]\u001b[A\n","Iteration:  79% 335/422 [02:36<00:40,  2.16it/s]\u001b[A\n","Iteration:  80% 336/422 [02:36<00:39,  2.16it/s]\u001b[A\n","Iteration:  80% 337/422 [02:37<00:39,  2.16it/s]\u001b[A\n","Iteration:  80% 338/422 [02:37<00:38,  2.16it/s]\u001b[A\n","Iteration:  80% 339/422 [02:37<00:38,  2.16it/s]\u001b[A\n","Iteration:  81% 340/422 [02:38<00:38,  2.16it/s]\u001b[A\n","Iteration:  81% 341/422 [02:38<00:37,  2.16it/s]\u001b[A\n","Iteration:  81% 342/422 [02:39<00:37,  2.16it/s]\u001b[A\n","Iteration:  81% 343/422 [02:39<00:36,  2.16it/s]\u001b[A\n","Iteration:  82% 344/422 [02:40<00:36,  2.16it/s]\u001b[A\n","Iteration:  82% 345/422 [02:40<00:35,  2.16it/s]\u001b[A\n","Iteration:  82% 346/422 [02:41<00:35,  2.16it/s]\u001b[A\n","Iteration:  82% 347/422 [02:41<00:34,  2.16it/s]\u001b[A\n","Iteration:  82% 348/422 [02:42<00:34,  2.16it/s]\u001b[A\n","Iteration:  83% 349/422 [02:42<00:33,  2.16it/s]\u001b[A\n","Iteration:  83% 350/422 [02:43<00:33,  2.16it/s]\u001b[A\n","Iteration:  83% 351/422 [02:43<00:32,  2.17it/s]\u001b[A\n","Iteration:  83% 352/422 [02:43<00:32,  2.15it/s]\u001b[A\n","Iteration:  84% 353/422 [02:44<00:32,  2.16it/s]\u001b[A\n","Iteration:  84% 354/422 [02:44<00:31,  2.16it/s]\u001b[A\n","Iteration:  84% 355/422 [02:45<00:31,  2.16it/s]\u001b[A\n","Iteration:  84% 356/422 [02:45<00:30,  2.16it/s]\u001b[A\n","Iteration:  85% 357/422 [02:46<00:30,  2.16it/s]\u001b[A\n","Iteration:  85% 358/422 [02:46<00:29,  2.16it/s]\u001b[A\n","Iteration:  85% 359/422 [02:47<00:29,  2.17it/s]\u001b[A\n","Iteration:  85% 360/422 [02:47<00:28,  2.17it/s]\u001b[A\n","Iteration:  86% 361/422 [02:48<00:28,  2.16it/s]\u001b[A\n","Iteration:  86% 362/422 [02:48<00:27,  2.16it/s]\u001b[A\n","Iteration:  86% 363/422 [02:49<00:28,  2.09it/s]\u001b[A\n","Iteration:  86% 364/422 [02:49<00:28,  2.07it/s]\u001b[A\n","Iteration:  86% 365/422 [02:50<00:27,  2.06it/s]\u001b[A\n","Iteration:  87% 366/422 [02:50<00:27,  2.07it/s]\u001b[A\n","Iteration:  87% 367/422 [02:51<00:27,  2.01it/s]\u001b[A\n","Iteration:  87% 368/422 [02:54<01:20,  1.49s/it]\u001b[A\n","Iteration:  87% 369/422 [02:55<01:02,  1.18s/it]\u001b[A\n","Iteration:  88% 370/422 [02:55<00:50,  1.04it/s]\u001b[A\n","Iteration:  88% 371/422 [02:56<00:41,  1.23it/s]\u001b[A\n","Iteration:  88% 372/422 [02:56<00:35,  1.41it/s]\u001b[A\n","Iteration:  88% 373/422 [02:57<00:31,  1.58it/s]\u001b[A\n","Iteration:  89% 374/422 [02:57<00:27,  1.72it/s]\u001b[A\n","Iteration:  89% 375/422 [02:58<00:25,  1.84it/s]\u001b[A\n","Iteration:  89% 376/422 [02:58<00:23,  1.92it/s]\u001b[A\n","Iteration:  89% 377/422 [02:59<00:22,  1.99it/s]\u001b[A\n","Iteration:  90% 378/422 [02:59<00:21,  2.04it/s]\u001b[A\n","Iteration:  90% 379/422 [02:59<00:20,  2.08it/s]\u001b[A\n","Iteration:  90% 380/422 [03:00<00:19,  2.10it/s]\u001b[A\n","Iteration:  90% 381/422 [03:00<00:19,  2.12it/s]\u001b[A\n","Iteration:  91% 382/422 [03:01<00:18,  2.13it/s]\u001b[A\n","Iteration:  91% 383/422 [03:01<00:18,  2.14it/s]\u001b[A\n","Iteration:  91% 384/422 [03:02<00:17,  2.15it/s]\u001b[A\n","Iteration:  91% 385/422 [03:02<00:17,  2.15it/s]\u001b[A\n","Iteration:  91% 386/422 [03:03<00:16,  2.16it/s]\u001b[A\n","Iteration:  92% 387/422 [03:03<00:16,  2.16it/s]\u001b[A\n","Iteration:  92% 388/422 [03:04<00:15,  2.16it/s]\u001b[A\n","Iteration:  92% 389/422 [03:04<00:15,  2.16it/s]\u001b[A\n","Iteration:  92% 390/422 [03:05<00:14,  2.16it/s]\u001b[A\n","Iteration:  93% 391/422 [03:05<00:14,  2.16it/s]\u001b[A\n","Iteration:  93% 392/422 [03:05<00:13,  2.16it/s]\u001b[A\n","Iteration:  93% 393/422 [03:06<00:13,  2.16it/s]\u001b[A\n","Iteration:  93% 394/422 [03:06<00:12,  2.16it/s]\u001b[A\n","Iteration:  94% 395/422 [03:07<00:12,  2.16it/s]\u001b[A\n","Iteration:  94% 396/422 [03:07<00:12,  2.16it/s]\u001b[A\n","Iteration:  94% 397/422 [03:08<00:11,  2.16it/s]\u001b[A\n","Iteration:  94% 398/422 [03:08<00:11,  2.16it/s]\u001b[A\n","Iteration:  95% 399/422 [03:09<00:10,  2.16it/s]\u001b[A\n","Iteration:  95% 400/422 [03:09<00:10,  2.16it/s]\u001b[A\n","Iteration:  95% 401/422 [03:10<00:09,  2.16it/s]\u001b[A\n","Iteration:  95% 402/422 [03:10<00:09,  2.17it/s]\u001b[A\n","Iteration:  95% 403/422 [03:11<00:08,  2.16it/s]\u001b[A\n","Iteration:  96% 404/422 [03:11<00:08,  2.16it/s]\u001b[A\n","Iteration:  96% 405/422 [03:12<00:07,  2.16it/s]\u001b[A\n","Iteration:  96% 406/422 [03:12<00:07,  2.16it/s]\u001b[A\n","Iteration:  96% 407/422 [03:12<00:06,  2.16it/s]\u001b[A\n","Iteration:  97% 408/422 [03:13<00:06,  2.16it/s]\u001b[A\n","Iteration:  97% 409/422 [03:13<00:06,  2.17it/s]\u001b[A\n","Iteration:  97% 410/422 [03:14<00:05,  2.17it/s]\u001b[A\n","Iteration:  97% 411/422 [03:14<00:05,  2.16it/s]\u001b[A\n","Iteration:  98% 412/422 [03:15<00:04,  2.16it/s]\u001b[A\n","Iteration:  98% 413/422 [03:15<00:04,  2.16it/s]\u001b[A\n","Iteration:  98% 414/422 [03:16<00:03,  2.17it/s]\u001b[A\n","Iteration:  98% 415/422 [03:16<00:03,  2.17it/s]\u001b[A\n","Iteration:  99% 416/422 [03:17<00:02,  2.17it/s]\u001b[A\n","Iteration:  99% 417/422 [03:17<00:02,  2.17it/s]\u001b[A\n","Iteration:  99% 418/422 [03:18<00:01,  2.17it/s]\u001b[A\n","Iteration:  99% 419/422 [03:18<00:01,  2.17it/s]\u001b[A\n","Iteration: 100% 420/422 [03:18<00:00,  2.17it/s]\u001b[A\n","Iteration: 100% 421/422 [03:19<00:00,  2.17it/s]\u001b[A\n","Iteration: 100% 422/422 [03:20<00:00,  2.11it/s]\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n","To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n","  return torch.floor_divide(self, other)\n","[2022-07-23 08:41:06,389][__main__][DEBUG] - epoch is 0\n","[2022-07-23 08:41:06,391][__main__][DEBUG] - validation loss is tensor(2.4279, device='cuda:0')\n","Epoch:   1% 1/100 [06:47<11:12:38, 407.66s/it]\n","Iteration:   0% 0/422 [00:00<?, ?it/s]\u001b[AThe current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","\n","Iteration:   0% 1/422 [00:01<10:49,  1.54s/it]\u001b[A\n","Iteration:   0% 2/422 [00:02<06:22,  1.10it/s]\u001b[A\n","Iteration:   1% 3/422 [00:02<04:55,  1.42it/s]\u001b[A\n","Iteration:   1% 4/422 [00:02<04:14,  1.64it/s]\u001b[A\n","Iteration:   1% 5/422 [00:03<03:52,  1.80it/s]\u001b[A\n","Iteration:   1% 6/422 [00:03<03:38,  1.90it/s]\u001b[A\n","Iteration:   2% 7/422 [00:04<03:29,  1.98it/s]\u001b[A\n","Iteration:   2% 8/422 [00:04<03:22,  2.04it/s]\u001b[A\n","Iteration:   2% 9/422 [00:05<03:18,  2.08it/s]\u001b[A\n","Iteration:   2% 10/422 [00:05<03:16,  2.09it/s]\u001b[A\n","Iteration:   3% 11/422 [00:06<03:16,  2.09it/s]\u001b[A\n","Iteration:   3% 12/422 [00:06<03:16,  2.08it/s]\u001b[A\n","Iteration:   3% 13/422 [00:07<03:16,  2.08it/s]\u001b[A\n","Iteration:   3% 14/422 [00:07<03:14,  2.10it/s]\u001b[A\n","Iteration:   4% 15/422 [00:08<03:14,  2.09it/s]\u001b[A\n","Iteration:   4% 16/422 [00:08<03:13,  2.10it/s]\u001b[A\n","Iteration:   4% 17/422 [00:09<03:11,  2.11it/s]\u001b[A\n","Iteration:   4% 18/422 [00:09<03:11,  2.11it/s]\u001b[A\n","Iteration:   5% 19/422 [00:09<03:10,  2.11it/s]\u001b[A\n","Iteration:   5% 20/422 [00:10<03:11,  2.10it/s]\u001b[A\n","Iteration:   5% 21/422 [00:10<03:10,  2.11it/s]\u001b[A\n","Iteration:   5% 22/422 [00:11<03:09,  2.11it/s]\u001b[A\n","Iteration:   5% 23/422 [00:11<03:07,  2.12it/s]\u001b[A\n","Iteration:   6% 24/422 [00:12<03:06,  2.14it/s]\u001b[A\n","Iteration:   6% 25/422 [00:12<03:05,  2.14it/s]\u001b[A\n","Iteration:   6% 26/422 [00:13<03:03,  2.15it/s]\u001b[A\n","Iteration:   6% 27/422 [00:13<03:03,  2.16it/s]\u001b[A\n","Iteration:   7% 28/422 [00:14<03:02,  2.16it/s]\u001b[A\n","Iteration:   7% 29/422 [00:14<03:02,  2.16it/s]\u001b[A\n","Iteration:   7% 30/422 [00:15<03:01,  2.16it/s]\u001b[A\n","Iteration:   7% 31/422 [00:15<03:00,  2.16it/s]\u001b[A\n","Iteration:   8% 32/422 [00:16<03:00,  2.17it/s]\u001b[A\n","Iteration:   8% 33/422 [00:16<02:59,  2.17it/s]\u001b[A\n","Iteration:   8% 34/422 [00:16<02:58,  2.17it/s]\u001b[A\n","Iteration:   8% 35/422 [00:17<02:58,  2.17it/s]\u001b[A\n","Iteration:   9% 36/422 [00:17<02:58,  2.16it/s]\u001b[A\n","Iteration:   9% 37/422 [00:18<02:57,  2.16it/s]\u001b[A\n","Iteration:   9% 38/422 [00:18<02:57,  2.16it/s]\u001b[A\n","Iteration:   9% 39/422 [00:19<02:57,  2.16it/s]\u001b[A\n","Iteration:   9% 40/422 [00:19<02:56,  2.16it/s]\u001b[A\n","Iteration:  10% 41/422 [00:20<02:56,  2.16it/s]\u001b[A\n","Iteration:  10% 42/422 [00:20<02:55,  2.16it/s]\u001b[A\n","Iteration:  10% 43/422 [00:21<02:55,  2.16it/s]\u001b[A\n","Iteration:  10% 44/422 [00:21<02:54,  2.17it/s]\u001b[A\n","Iteration:  11% 45/422 [00:22<02:53,  2.17it/s]\u001b[A\n","Iteration:  11% 46/422 [00:22<02:53,  2.17it/s]\u001b[A\n","Iteration:  11% 47/422 [00:22<02:52,  2.17it/s]\u001b[A\n","Iteration:  11% 48/422 [00:23<02:52,  2.17it/s]\u001b[A\n","Iteration:  12% 49/422 [00:23<02:52,  2.17it/s]\u001b[A\n","Iteration:  12% 50/422 [00:24<02:51,  2.17it/s]\u001b[A\n","Iteration:  12% 51/422 [00:24<02:51,  2.17it/s]\u001b[A\n","Iteration:  12% 52/422 [00:25<02:50,  2.17it/s]\u001b[A\n","Iteration:  13% 53/422 [00:25<02:50,  2.17it/s]\u001b[A\n","Iteration:  13% 54/422 [00:26<02:49,  2.17it/s]\u001b[A\n","Iteration:  13% 55/422 [00:26<02:49,  2.17it/s]\u001b[A\n","Iteration:  13% 56/422 [00:27<02:48,  2.17it/s]\u001b[A\n","Iteration:  14% 57/422 [00:27<02:48,  2.17it/s]\u001b[A\n","Iteration:  14% 58/422 [00:28<02:47,  2.17it/s]\u001b[A\n","Iteration:  14% 59/422 [00:28<02:47,  2.17it/s]\u001b[A\n","Iteration:  14% 60/422 [00:28<02:46,  2.17it/s]\u001b[A\n","Iteration:  14% 61/422 [00:29<02:46,  2.17it/s]\u001b[A\n","Iteration:  15% 62/422 [00:29<02:46,  2.16it/s]\u001b[A\n","Iteration:  15% 63/422 [00:30<02:46,  2.16it/s]\u001b[A\n","Iteration:  15% 64/422 [00:30<02:45,  2.16it/s]\u001b[A\n","Iteration:  15% 65/422 [00:31<02:45,  2.15it/s]\u001b[A\n","Iteration:  16% 66/422 [00:31<02:46,  2.14it/s]\u001b[A\n","Iteration:  16% 67/422 [00:32<02:44,  2.15it/s]\u001b[A\n","Iteration:  16% 68/422 [00:32<02:44,  2.16it/s]\u001b[A\n","Iteration:  16% 69/422 [00:33<02:43,  2.16it/s]\u001b[A\n","Iteration:  17% 70/422 [00:33<02:43,  2.16it/s]\u001b[A\n","Iteration:  17% 71/422 [00:34<02:42,  2.16it/s]\u001b[A\n","Iteration:  17% 72/422 [00:34<02:41,  2.16it/s]\u001b[A\n","Iteration:  17% 73/422 [00:34<02:41,  2.16it/s]\u001b[A\n","Iteration:  18% 74/422 [00:35<02:40,  2.16it/s]\u001b[A\n","Iteration:  18% 75/422 [00:35<02:40,  2.16it/s]\u001b[A\n","Iteration:  18% 76/422 [00:36<02:39,  2.17it/s]\u001b[A\n","Iteration:  18% 77/422 [00:36<02:39,  2.17it/s]\u001b[A\n","Iteration:  18% 78/422 [00:37<02:38,  2.17it/s]\u001b[A\n","Iteration:  19% 79/422 [00:37<02:38,  2.17it/s]\u001b[A\n","Iteration:  19% 80/422 [00:38<02:37,  2.17it/s]\u001b[A\n","Iteration:  19% 81/422 [00:38<02:37,  2.17it/s]\u001b[A\n","Iteration:  19% 82/422 [00:39<02:36,  2.17it/s]\u001b[A\n","Iteration:  20% 83/422 [00:39<02:36,  2.17it/s]\u001b[A\n","Iteration:  20% 84/422 [00:40<02:35,  2.17it/s]\u001b[A\n","Iteration:  20% 85/422 [00:40<02:35,  2.17it/s]\u001b[A\n","Iteration:  20% 86/422 [00:40<02:34,  2.17it/s]\u001b[A\n","Iteration:  21% 87/422 [00:41<02:34,  2.17it/s]\u001b[A\n","Iteration:  21% 88/422 [00:41<02:34,  2.17it/s]\u001b[A\n","Iteration:  21% 89/422 [00:42<02:33,  2.17it/s]\u001b[A\n","Iteration:  21% 90/422 [00:42<02:33,  2.17it/s]\u001b[A\n","Iteration:  22% 91/422 [00:43<02:32,  2.17it/s]\u001b[A\n","Iteration:  22% 92/422 [00:43<02:32,  2.17it/s]\u001b[A\n","Iteration:  22% 93/422 [00:44<02:31,  2.17it/s]\u001b[A\n","Iteration:  22% 94/422 [00:44<02:31,  2.17it/s]\u001b[A\n","Iteration:  23% 95/422 [00:45<02:30,  2.17it/s]\u001b[A\n","Iteration:  23% 96/422 [00:45<02:30,  2.17it/s]\u001b[A\n","Iteration:  23% 97/422 [00:46<02:29,  2.17it/s]\u001b[A\n","Iteration:  23% 98/422 [00:46<02:29,  2.17it/s]\u001b[A\n","Iteration:  23% 99/422 [00:46<02:28,  2.17it/s]\u001b[A\n","Iteration:  24% 100/422 [00:47<02:28,  2.17it/s]\u001b[A\n","Iteration:  24% 101/422 [00:47<02:28,  2.17it/s]\u001b[A\n","Iteration:  24% 102/422 [00:48<02:27,  2.17it/s]\u001b[A\n","Iteration:  24% 103/422 [00:48<02:27,  2.17it/s]\u001b[A\n","Iteration:  25% 104/422 [00:49<02:26,  2.17it/s]\u001b[A\n","Iteration:  25% 105/422 [00:49<02:26,  2.17it/s]\u001b[A\n","Iteration:  25% 106/422 [00:50<02:25,  2.17it/s]\u001b[A\n","Iteration:  25% 107/422 [00:50<02:25,  2.16it/s]\u001b[A\n","Iteration:  26% 108/422 [00:51<02:24,  2.17it/s]\u001b[A\n","Iteration:  26% 109/422 [00:51<02:24,  2.17it/s]\u001b[A\n","Iteration:  26% 110/422 [00:52<02:23,  2.17it/s]\u001b[A\n","Iteration:  26% 111/422 [00:52<02:23,  2.17it/s]\u001b[A\n","Iteration:  27% 112/422 [00:52<02:22,  2.17it/s]\u001b[A\n","Iteration:  27% 113/422 [00:53<02:22,  2.17it/s]\u001b[A\n","Iteration:  27% 114/422 [00:53<02:22,  2.17it/s]\u001b[A\n","Iteration:  27% 115/422 [00:54<02:21,  2.17it/s]\u001b[A\n","Iteration:  27% 116/422 [00:54<02:21,  2.17it/s]\u001b[A\n","Iteration:  28% 117/422 [00:55<02:20,  2.17it/s]\u001b[A\n","Iteration:  28% 118/422 [00:55<02:20,  2.17it/s]\u001b[A\n","Iteration:  28% 119/422 [00:56<02:19,  2.17it/s]\u001b[A\n","Iteration:  28% 120/422 [00:56<02:19,  2.17it/s]\u001b[A\n","Iteration:  29% 121/422 [00:57<02:18,  2.17it/s]\u001b[A\n","Iteration:  29% 122/422 [00:57<02:18,  2.17it/s]\u001b[A\n","Iteration:  29% 123/422 [00:58<02:17,  2.17it/s]\u001b[A\n","Iteration:  29% 124/422 [00:58<02:17,  2.17it/s]\u001b[A\n","Iteration:  30% 125/422 [00:58<02:17,  2.17it/s]\u001b[A\n","Iteration:  30% 126/422 [00:59<02:16,  2.17it/s]\u001b[A\n","Iteration:  30% 127/422 [00:59<02:16,  2.17it/s]\u001b[A\n","Iteration:  30% 128/422 [01:00<02:15,  2.17it/s]\u001b[A\n","Iteration:  31% 129/422 [01:00<02:15,  2.17it/s]\u001b[A\n","Iteration:  31% 130/422 [01:01<02:14,  2.17it/s]\u001b[A\n","Iteration:  31% 131/422 [01:01<02:14,  2.16it/s]\u001b[A\n","Iteration:  31% 132/422 [01:02<02:13,  2.16it/s]\u001b[A\n","Iteration:  32% 133/422 [01:02<02:13,  2.16it/s]\u001b[A\n","Iteration:  32% 134/422 [01:03<02:13,  2.16it/s]\u001b[A\n","Iteration:  32% 135/422 [01:03<02:12,  2.16it/s]\u001b[A\n","Iteration:  32% 136/422 [01:04<02:12,  2.15it/s]\u001b[A\n","Iteration:  32% 137/422 [01:04<02:12,  2.15it/s]\u001b[A\n","Iteration:  33% 138/422 [01:04<02:11,  2.16it/s]\u001b[A\n","Iteration:  33% 139/422 [01:05<02:10,  2.16it/s]\u001b[A\n","Iteration:  33% 140/422 [01:05<02:10,  2.17it/s]\u001b[A\n","Iteration:  33% 141/422 [01:06<02:09,  2.16it/s]\u001b[A\n","Iteration:  34% 142/422 [01:06<02:09,  2.16it/s]\u001b[A\n","Iteration:  34% 143/422 [01:07<02:08,  2.17it/s]\u001b[A\n","Iteration:  34% 144/422 [01:07<02:08,  2.17it/s]\u001b[A\n","Iteration:  34% 145/422 [01:08<02:07,  2.17it/s]\u001b[A\n","Iteration:  35% 146/422 [01:08<02:07,  2.17it/s]\u001b[A\n","Iteration:  35% 147/422 [01:09<02:06,  2.17it/s]\u001b[A\n","Iteration:  35% 148/422 [01:09<02:06,  2.17it/s]\u001b[A\n","Iteration:  35% 149/422 [01:10<02:06,  2.16it/s]\u001b[A\n","Iteration:  36% 150/422 [01:10<02:05,  2.16it/s]\u001b[A\n","Iteration:  36% 151/422 [01:10<02:05,  2.16it/s]\u001b[A\n","Iteration:  36% 152/422 [01:11<02:04,  2.17it/s]\u001b[A\n","Iteration:  36% 153/422 [01:11<02:04,  2.16it/s]\u001b[A\n","Iteration:  36% 154/422 [01:12<02:04,  2.16it/s]\u001b[A\n","Iteration:  37% 155/422 [01:12<02:03,  2.16it/s]\u001b[A\n","Iteration:  37% 156/422 [01:13<02:02,  2.16it/s]\u001b[A\n","Iteration:  37% 157/422 [01:13<02:02,  2.16it/s]\u001b[A\n","Iteration:  37% 158/422 [01:14<02:02,  2.16it/s]\u001b[A\n","Iteration:  38% 159/422 [01:14<02:01,  2.16it/s]\u001b[A\n","Iteration:  38% 160/422 [01:15<02:01,  2.16it/s]\u001b[A\n","Iteration:  38% 161/422 [01:15<02:00,  2.16it/s]\u001b[A\n","Iteration:  38% 162/422 [01:16<02:00,  2.16it/s]\u001b[A\n","Iteration:  39% 163/422 [01:16<01:59,  2.17it/s]\u001b[A\n","Iteration:  39% 164/422 [01:16<01:59,  2.16it/s]\u001b[A\n","Iteration:  39% 165/422 [01:17<01:58,  2.16it/s]\u001b[A\n","Iteration:  39% 166/422 [01:17<01:58,  2.16it/s]\u001b[A\n","Iteration:  40% 167/422 [01:18<01:57,  2.16it/s]\u001b[A\n","Iteration:  40% 168/422 [01:18<01:57,  2.16it/s]\u001b[A\n","Iteration:  40% 169/422 [01:19<01:56,  2.16it/s]\u001b[A\n","Iteration:  40% 170/422 [01:19<01:56,  2.17it/s]\u001b[A\n","Iteration:  41% 171/422 [01:20<01:55,  2.17it/s]\u001b[A\n","Iteration:  41% 172/422 [01:20<01:55,  2.16it/s]\u001b[A\n","Iteration:  41% 173/422 [01:21<01:55,  2.16it/s]\u001b[A\n","Iteration:  41% 174/422 [01:21<01:54,  2.16it/s]\u001b[A\n","Iteration:  41% 175/422 [01:22<01:54,  2.17it/s]\u001b[A\n","Iteration:  42% 176/422 [01:22<01:53,  2.17it/s]\u001b[A\n","Iteration:  42% 177/422 [01:23<01:53,  2.17it/s]\u001b[A\n","Iteration:  42% 178/422 [01:23<01:52,  2.17it/s]\u001b[A\n","Iteration:  42% 179/422 [01:23<01:52,  2.17it/s]\u001b[A\n","Iteration:  43% 180/422 [01:24<01:51,  2.17it/s]\u001b[A\n","Iteration:  43% 181/422 [01:24<01:51,  2.17it/s]\u001b[A\n","Iteration:  43% 182/422 [01:25<01:50,  2.17it/s]\u001b[A\n","Iteration:  43% 183/422 [01:25<01:50,  2.16it/s]\u001b[A\n","Iteration:  44% 184/422 [01:26<01:50,  2.16it/s]\u001b[A\n","Iteration:  44% 185/422 [01:26<01:50,  2.15it/s]\u001b[A\n","Iteration:  44% 186/422 [01:27<01:49,  2.15it/s]\u001b[A\n","Iteration:  44% 187/422 [01:27<01:49,  2.14it/s]\u001b[A\n","Iteration:  45% 188/422 [01:28<01:49,  2.15it/s]\u001b[A\n","Iteration:  45% 189/422 [01:28<01:48,  2.15it/s]\u001b[A\n","Iteration:  45% 190/422 [01:31<05:03,  1.31s/it]\u001b[A\n","Iteration:  45% 191/422 [01:32<04:03,  1.05s/it]\u001b[A\n","Iteration:  45% 192/422 [01:32<03:21,  1.14it/s]\u001b[A\n","Iteration:  46% 193/422 [01:33<02:51,  1.33it/s]\u001b[A\n","Iteration:  46% 194/422 [01:33<02:31,  1.51it/s]\u001b[A\n","Iteration:  46% 195/422 [01:34<02:16,  1.66it/s]\u001b[A\n","Iteration:  46% 196/422 [01:34<02:06,  1.79it/s]\u001b[A\n","Iteration:  47% 197/422 [01:35<01:59,  1.89it/s]\u001b[A\n","Iteration:  47% 198/422 [01:35<01:54,  1.96it/s]\u001b[A\n","Iteration:  47% 199/422 [01:35<01:50,  2.02it/s]\u001b[A\n","Iteration:  47% 200/422 [01:36<01:47,  2.06it/s]\u001b[A\n","Iteration:  48% 201/422 [01:36<01:45,  2.09it/s]\u001b[A\n","Iteration:  48% 202/422 [01:37<01:44,  2.11it/s]\u001b[A\n","Iteration:  48% 203/422 [01:37<01:42,  2.13it/s]\u001b[A\n","Iteration:  48% 204/422 [01:38<01:41,  2.14it/s]\u001b[A\n","Iteration:  49% 205/422 [01:38<01:41,  2.15it/s]\u001b[A\n","Iteration:  49% 206/422 [01:39<01:40,  2.15it/s]\u001b[A\n","Iteration:  49% 207/422 [01:39<01:39,  2.16it/s]\u001b[A\n","Iteration:  49% 208/422 [01:40<01:39,  2.16it/s]\u001b[A\n","Iteration:  50% 209/422 [01:40<01:38,  2.16it/s]\u001b[A\n","Iteration:  50% 210/422 [01:41<01:38,  2.16it/s]\u001b[A\n","Iteration:  50% 211/422 [01:41<01:37,  2.16it/s]\u001b[A\n","Iteration:  50% 212/422 [01:41<01:37,  2.16it/s]\u001b[A\n","Iteration:  50% 213/422 [01:42<01:36,  2.17it/s]\u001b[A\n","Iteration:  51% 214/422 [01:42<01:35,  2.17it/s]\u001b[A\n","Iteration:  51% 215/422 [01:43<01:35,  2.17it/s]\u001b[A\n","Iteration:  51% 216/422 [01:43<01:35,  2.17it/s]\u001b[A\n","Iteration:  51% 217/422 [01:44<01:34,  2.17it/s]\u001b[A\n","Iteration:  52% 218/422 [01:44<01:34,  2.17it/s]\u001b[A\n","Iteration:  52% 219/422 [01:45<01:33,  2.17it/s]\u001b[A\n","Iteration:  52% 220/422 [01:45<01:33,  2.16it/s]\u001b[A\n","Iteration:  52% 221/422 [01:46<01:33,  2.16it/s]\u001b[A\n","Iteration:  53% 222/422 [01:46<01:32,  2.16it/s]\u001b[A\n","Iteration:  53% 223/422 [01:47<01:31,  2.16it/s]\u001b[A\n","Iteration:  53% 224/422 [01:47<01:31,  2.17it/s]\u001b[A\n","Iteration:  53% 225/422 [01:47<01:30,  2.17it/s]\u001b[A\n","Iteration:  54% 226/422 [01:48<01:30,  2.17it/s]\u001b[A\n","Iteration:  54% 227/422 [01:48<01:29,  2.17it/s]\u001b[A\n","Iteration:  54% 228/422 [01:49<01:29,  2.17it/s]\u001b[A\n","Iteration:  54% 229/422 [01:49<01:29,  2.17it/s]\u001b[A\n","Iteration:  55% 230/422 [01:50<01:28,  2.17it/s]\u001b[A\n","Iteration:  55% 231/422 [01:50<01:28,  2.16it/s]\u001b[A\n","Iteration:  55% 232/422 [01:51<01:28,  2.16it/s]\u001b[A\n","Iteration:  55% 233/422 [01:51<01:27,  2.16it/s]\u001b[A\n","Iteration:  55% 234/422 [01:52<01:26,  2.16it/s]\u001b[A\n","Iteration:  56% 235/422 [01:52<01:26,  2.16it/s]\u001b[A\n","Iteration:  56% 236/422 [01:53<01:25,  2.16it/s]\u001b[A\n","Iteration:  56% 237/422 [01:53<01:25,  2.16it/s]\u001b[A\n","Iteration:  56% 238/422 [01:53<01:24,  2.17it/s]\u001b[A\n","Iteration:  57% 239/422 [01:54<01:24,  2.16it/s]\u001b[A\n","Iteration:  57% 240/422 [01:54<01:24,  2.16it/s]\u001b[A\n","Iteration:  57% 241/422 [01:55<01:23,  2.16it/s]\u001b[A\n","Iteration:  57% 242/422 [01:55<01:23,  2.16it/s]\u001b[A\n","Iteration:  58% 243/422 [01:56<01:22,  2.16it/s]\u001b[A\n","Iteration:  58% 244/422 [01:56<01:22,  2.17it/s]\u001b[A\n","Iteration:  58% 245/422 [01:57<01:21,  2.17it/s]\u001b[A\n","Iteration:  58% 246/422 [01:57<01:21,  2.17it/s]\u001b[A\n","Iteration:  59% 247/422 [01:58<01:20,  2.17it/s]\u001b[A\n","Iteration:  59% 248/422 [01:58<01:20,  2.17it/s]\u001b[A\n","Iteration:  59% 249/422 [01:59<01:19,  2.16it/s]\u001b[A\n","Iteration:  59% 250/422 [01:59<01:19,  2.16it/s]\u001b[A\n","Iteration:  59% 251/422 [02:00<01:19,  2.15it/s]\u001b[A\n","Iteration:  60% 252/422 [02:00<01:18,  2.15it/s]\u001b[A\n","Iteration:  60% 253/422 [02:00<01:18,  2.15it/s]\u001b[A\n","Iteration:  60% 254/422 [02:01<01:18,  2.15it/s]\u001b[A\n","Iteration:  60% 255/422 [02:01<01:17,  2.15it/s]\u001b[A\n","Iteration:  61% 256/422 [02:02<01:17,  2.16it/s]\u001b[A\n","Iteration:  61% 257/422 [02:02<01:16,  2.16it/s]\u001b[A\n","Iteration:  61% 258/422 [02:03<01:15,  2.16it/s]\u001b[A\n","Iteration:  61% 259/422 [02:03<01:15,  2.16it/s]\u001b[A\n","Iteration:  62% 260/422 [02:04<01:14,  2.16it/s]\u001b[A\n","Iteration:  62% 261/422 [02:04<01:14,  2.16it/s]\u001b[A\n","Iteration:  62% 262/422 [02:05<01:13,  2.16it/s]\u001b[A\n","Iteration:  62% 263/422 [02:05<01:13,  2.16it/s]\u001b[A\n","Iteration:  63% 264/422 [02:06<01:13,  2.16it/s]\u001b[A\n","Iteration:  63% 265/422 [02:06<01:12,  2.16it/s]\u001b[A\n","Iteration:  63% 266/422 [02:06<01:12,  2.17it/s]\u001b[A\n","Iteration:  63% 267/422 [02:07<01:11,  2.16it/s]\u001b[A\n","Iteration:  64% 268/422 [02:07<01:11,  2.16it/s]\u001b[A\n","Iteration:  64% 269/422 [02:08<01:10,  2.16it/s]\u001b[A\n","Iteration:  64% 270/422 [02:08<01:10,  2.16it/s]\u001b[A\n","Iteration:  64% 271/422 [02:09<01:09,  2.16it/s]\u001b[A\n","Iteration:  64% 272/422 [02:09<01:09,  2.16it/s]\u001b[A\n","Iteration:  65% 273/422 [02:10<01:08,  2.16it/s]\u001b[A\n","Iteration:  65% 274/422 [02:10<01:08,  2.17it/s]\u001b[A\n","Iteration:  65% 275/422 [02:11<01:08,  2.16it/s]\u001b[A\n","Iteration:  65% 276/422 [02:11<01:07,  2.16it/s]\u001b[A\n","Iteration:  66% 277/422 [02:12<01:07,  2.16it/s]\u001b[A\n","Iteration:  66% 278/422 [02:12<01:06,  2.16it/s]\u001b[A\n","Iteration:  66% 279/422 [02:12<01:06,  2.16it/s]\u001b[A\n","Iteration:  66% 280/422 [02:13<01:05,  2.16it/s]\u001b[A\n","Iteration:  67% 281/422 [02:13<01:05,  2.16it/s]\u001b[A\n","Iteration:  67% 282/422 [02:14<01:04,  2.16it/s]\u001b[A\n","Iteration:  67% 283/422 [02:14<01:04,  2.16it/s]\u001b[A\n","Iteration:  67% 284/422 [02:15<01:03,  2.16it/s]\u001b[A\n","Iteration:  68% 285/422 [02:15<01:03,  2.16it/s]\u001b[A\n","Iteration:  68% 286/422 [02:16<01:02,  2.16it/s]\u001b[A\n","Iteration:  68% 287/422 [02:16<01:02,  2.16it/s]\u001b[A\n","Iteration:  68% 288/422 [02:17<01:01,  2.16it/s]\u001b[A\n","Iteration:  68% 289/422 [02:17<01:01,  2.16it/s]\u001b[A\n","Iteration:  69% 290/422 [02:18<01:01,  2.16it/s]\u001b[A\n","Iteration:  69% 291/422 [02:18<01:00,  2.16it/s]\u001b[A\n","Iteration:  69% 292/422 [02:18<01:00,  2.16it/s]\u001b[A\n","Iteration:  69% 293/422 [02:19<00:59,  2.16it/s]\u001b[A\n","Iteration:  70% 294/422 [02:19<00:59,  2.17it/s]\u001b[A\n","Iteration:  70% 295/422 [02:20<00:58,  2.16it/s]\u001b[A\n","Iteration:  70% 296/422 [02:20<00:58,  2.16it/s]\u001b[A\n","Iteration:  70% 297/422 [02:21<00:57,  2.16it/s]\u001b[A\n","Iteration:  71% 298/422 [02:21<00:57,  2.16it/s]\u001b[A\n","Iteration:  71% 299/422 [02:22<00:56,  2.16it/s]\u001b[A\n","Iteration:  71% 300/422 [02:22<00:56,  2.16it/s]\u001b[A\n","Iteration:  71% 301/422 [02:23<00:55,  2.16it/s]\u001b[A\n","Iteration:  72% 302/422 [02:23<00:55,  2.17it/s]\u001b[A\n","Iteration:  72% 303/422 [02:24<00:54,  2.17it/s]\u001b[A\n","Iteration:  72% 304/422 [02:24<00:54,  2.16it/s]\u001b[A\n","Iteration:  72% 305/422 [02:24<00:54,  2.16it/s]\u001b[A\n","Iteration:  73% 306/422 [02:25<00:53,  2.17it/s]\u001b[A\n","Iteration:  73% 307/422 [02:25<00:53,  2.17it/s]\u001b[A\n","Iteration:  73% 308/422 [02:26<00:52,  2.16it/s]\u001b[A\n","Iteration:  73% 309/422 [02:26<00:52,  2.16it/s]\u001b[A\n","Iteration:  73% 310/422 [02:27<00:51,  2.17it/s]\u001b[A\n","Iteration:  74% 311/422 [02:27<00:51,  2.16it/s]\u001b[A\n","Iteration:  74% 312/422 [02:28<00:50,  2.16it/s]\u001b[A\n","Iteration:  74% 313/422 [02:28<00:50,  2.16it/s]\u001b[A\n","Iteration:  74% 314/422 [02:29<00:49,  2.16it/s]\u001b[A\n","Iteration:  75% 315/422 [02:29<00:49,  2.16it/s]\u001b[A\n","Iteration:  75% 316/422 [02:30<00:48,  2.16it/s]\u001b[A\n","Iteration:  75% 317/422 [02:30<00:48,  2.17it/s]\u001b[A\n","Iteration:  75% 318/422 [02:30<00:48,  2.17it/s]\u001b[A\n","Iteration:  76% 319/422 [02:31<00:47,  2.17it/s]\u001b[A\n","Iteration:  76% 320/422 [02:31<00:47,  2.17it/s]\u001b[A\n","Iteration:  76% 321/422 [02:32<00:46,  2.16it/s]\u001b[A\n","Iteration:  76% 322/422 [02:32<00:46,  2.17it/s]\u001b[A\n","Iteration:  77% 323/422 [02:33<00:45,  2.16it/s]\u001b[A\n","Iteration:  77% 324/422 [02:33<00:45,  2.17it/s]\u001b[A\n","Iteration:  77% 325/422 [02:34<00:44,  2.17it/s]\u001b[A\n","Iteration:  77% 326/422 [02:34<00:44,  2.17it/s]\u001b[A\n","Iteration:  77% 327/422 [02:35<00:43,  2.17it/s]\u001b[A\n","Iteration:  78% 328/422 [02:35<00:43,  2.17it/s]\u001b[A\n","Iteration:  78% 329/422 [02:36<00:42,  2.17it/s]\u001b[A\n","Iteration:  78% 330/422 [02:36<00:42,  2.16it/s]\u001b[A\n","Iteration:  78% 331/422 [02:36<00:42,  2.16it/s]\u001b[A\n","Iteration:  79% 332/422 [02:37<00:41,  2.16it/s]\u001b[A\n","Iteration:  79% 333/422 [02:37<00:41,  2.16it/s]\u001b[A\n","Iteration:  79% 334/422 [02:38<00:40,  2.16it/s]\u001b[A\n","Iteration:  79% 335/422 [02:38<00:40,  2.16it/s]\u001b[A\n","Iteration:  80% 336/422 [02:39<00:39,  2.16it/s]\u001b[A\n","Iteration:  80% 337/422 [02:39<00:39,  2.17it/s]\u001b[A\n","Iteration:  80% 338/422 [02:40<00:38,  2.16it/s]\u001b[A\n","Iteration:  80% 339/422 [02:40<00:38,  2.16it/s]\u001b[A\n","Iteration:  81% 340/422 [02:41<00:37,  2.16it/s]\u001b[A\n","Iteration:  81% 341/422 [02:41<00:37,  2.16it/s]\u001b[A\n","Iteration:  81% 342/422 [02:42<00:36,  2.16it/s]\u001b[A\n","Iteration:  81% 343/422 [02:42<00:36,  2.16it/s]\u001b[A\n","Iteration:  82% 344/422 [02:43<00:36,  2.16it/s]\u001b[A\n","Iteration:  82% 345/422 [02:43<00:35,  2.16it/s]\u001b[A\n","Iteration:  82% 346/422 [02:43<00:35,  2.16it/s]\u001b[A\n","Iteration:  82% 347/422 [02:44<00:34,  2.16it/s]\u001b[A\n","Iteration:  82% 348/422 [02:44<00:34,  2.16it/s]\u001b[A\n","Iteration:  83% 349/422 [02:45<00:33,  2.16it/s]\u001b[A\n","Iteration:  83% 350/422 [02:45<00:33,  2.16it/s]\u001b[A\n","Iteration:  83% 351/422 [02:46<00:32,  2.16it/s]\u001b[A\n","Iteration:  83% 352/422 [02:46<00:32,  2.16it/s]\u001b[A\n","Iteration:  84% 353/422 [02:47<00:31,  2.16it/s]\u001b[A\n","Iteration:  84% 354/422 [02:47<00:31,  2.16it/s]\u001b[A\n","Iteration:  84% 355/422 [02:48<00:31,  2.16it/s]\u001b[A\n","Iteration:  84% 356/422 [02:48<00:30,  2.16it/s]\u001b[A\n","Iteration:  85% 357/422 [02:49<00:30,  2.16it/s]\u001b[A\n","Iteration:  85% 358/422 [02:49<00:29,  2.16it/s]\u001b[A\n","Iteration:  85% 359/422 [02:49<00:29,  2.16it/s]\u001b[A\n","Iteration:  85% 360/422 [02:50<00:28,  2.16it/s]\u001b[A\n","Iteration:  86% 361/422 [02:50<00:28,  2.16it/s]\u001b[A\n","Iteration:  86% 362/422 [02:51<00:27,  2.16it/s]\u001b[A\n","Iteration:  86% 363/422 [02:51<00:27,  2.16it/s]\u001b[A\n","Iteration:  86% 364/422 [02:52<00:26,  2.16it/s]\u001b[A\n","Iteration:  86% 365/422 [02:52<00:26,  2.16it/s]\u001b[A\n","Iteration:  87% 366/422 [02:53<00:25,  2.16it/s]\u001b[A\n","Iteration:  87% 367/422 [02:53<00:25,  2.17it/s]\u001b[A\n","Iteration:  87% 368/422 [02:54<00:24,  2.17it/s]\u001b[A\n","Iteration:  87% 369/422 [02:54<00:24,  2.17it/s]\u001b[A\n","Iteration:  88% 370/422 [02:55<00:24,  2.17it/s]\u001b[A\n","Iteration:  88% 371/422 [02:55<00:23,  2.16it/s]\u001b[A\n","Iteration:  88% 372/422 [02:55<00:23,  2.17it/s]\u001b[A\n","Iteration:  88% 373/422 [02:56<00:22,  2.17it/s]\u001b[A\n","Iteration:  89% 374/422 [02:56<00:22,  2.16it/s]\u001b[A\n","Iteration:  89% 375/422 [02:57<00:21,  2.16it/s]\u001b[A\n","Iteration:  89% 376/422 [02:57<00:21,  2.16it/s]\u001b[A\n","Iteration:  89% 377/422 [02:58<00:20,  2.16it/s]\u001b[A\n","Iteration:  90% 378/422 [02:58<00:20,  2.16it/s]\u001b[A\n","Iteration:  90% 379/422 [02:59<00:19,  2.16it/s]\u001b[A\n","Iteration:  90% 380/422 [02:59<00:19,  2.16it/s]\u001b[A\n","Iteration:  90% 381/422 [03:00<00:18,  2.16it/s]\u001b[A\n","Iteration:  91% 382/422 [03:00<00:18,  2.16it/s]\u001b[A\n","Iteration:  91% 383/422 [03:01<00:18,  2.16it/s]\u001b[A\n","Iteration:  91% 384/422 [03:01<00:17,  2.16it/s]\u001b[A\n","Iteration:  91% 385/422 [03:01<00:17,  2.16it/s]\u001b[A\n","Iteration:  91% 386/422 [03:02<00:16,  2.16it/s]\u001b[A\n","Iteration:  92% 387/422 [03:02<00:16,  2.16it/s]\u001b[A\n","Iteration:  92% 388/422 [03:03<00:15,  2.17it/s]\u001b[A\n","Iteration:  92% 389/422 [03:03<00:15,  2.17it/s]\u001b[A\n","Iteration:  92% 390/422 [03:04<00:14,  2.16it/s]\u001b[A\n","Iteration:  93% 391/422 [03:04<00:14,  2.17it/s]\u001b[A\n","Iteration:  93% 392/422 [03:05<00:13,  2.17it/s]\u001b[A\n","Iteration:  93% 393/422 [03:05<00:13,  2.17it/s]\u001b[A\n","Iteration:  93% 394/422 [03:06<00:12,  2.17it/s]\u001b[A\n","Iteration:  94% 395/422 [03:06<00:12,  2.16it/s]\u001b[A\n","Iteration:  94% 396/422 [03:07<00:12,  2.16it/s]\u001b[A\n","Iteration:  94% 397/422 [03:07<00:11,  2.16it/s]\u001b[A\n","Iteration:  94% 398/422 [03:07<00:11,  2.17it/s]\u001b[A\n","Iteration:  95% 399/422 [03:08<00:10,  2.17it/s]\u001b[A\n","Iteration:  95% 400/422 [03:08<00:10,  2.17it/s]\u001b[A\n","Iteration:  95% 401/422 [03:09<00:09,  2.16it/s]\u001b[A\n","Iteration:  95% 402/422 [03:09<00:09,  2.17it/s]\u001b[A\n","Iteration:  95% 403/422 [03:10<00:08,  2.17it/s]\u001b[A\n","Iteration:  96% 404/422 [03:10<00:08,  2.17it/s]\u001b[A\n","Iteration:  96% 405/422 [03:11<00:07,  2.17it/s]\u001b[A\n","Iteration:  96% 406/422 [03:11<00:07,  2.17it/s]\u001b[A\n","Iteration:  96% 407/422 [03:12<00:06,  2.17it/s]\u001b[A\n","Iteration:  97% 408/422 [03:12<00:06,  2.17it/s]\u001b[A\n","Iteration:  97% 409/422 [03:13<00:05,  2.17it/s]\u001b[A\n","Iteration:  97% 410/422 [03:13<00:05,  2.16it/s]\u001b[A\n","Iteration:  97% 411/422 [03:13<00:05,  2.17it/s]\u001b[A\n","Iteration:  98% 412/422 [03:14<00:04,  2.17it/s]\u001b[A\n","Iteration:  98% 413/422 [03:14<00:04,  2.17it/s]\u001b[A\n","Iteration:  98% 414/422 [03:15<00:03,  2.17it/s]\u001b[A\n","Iteration:  98% 415/422 [03:15<00:03,  2.17it/s]\u001b[A\n","Iteration:  99% 416/422 [03:16<00:02,  2.17it/s]\u001b[A\n","Iteration:  99% 417/422 [03:16<00:02,  2.17it/s]\u001b[A\n","Iteration:  99% 418/422 [03:17<00:01,  2.17it/s]\u001b[A\n","Iteration:  99% 419/422 [03:17<00:01,  2.17it/s]\u001b[A\n","Iteration: 100% 420/422 [03:18<00:00,  2.17it/s]\u001b[A\n","Iteration: 100% 421/422 [03:18<00:00,  2.17it/s]\u001b[A\n","Iteration: 100% 422/422 [03:19<00:00,  2.12it/s]\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","[2022-07-23 08:47:53,338][__main__][DEBUG] - early stop\n","Epoch:   1% 1/100 [13:31<22:19:35, 811.88s/it]\n"]}]},{"cell_type":"code","source":["!HYDRA_FULL_ERROR=1 python eval.py hydra.job.chdir=False hydra.verbose=True"],"metadata":{"id":"w7sdHHb0uFV-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658567292382,"user_tz":-540,"elapsed":1085669,"user":{"displayName":"Kosuke Aigo","userId":"01303562836135023230"}},"outputId":"fb9d01b4-b8ab-42d1-ef41-653b4eb53e73"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Using backend: pytorch\n","eval.py:18: UserWarning: \n","The version_base parameter is not specified.\n","Please specify a compatability version level, or None.\n","Will assume defaults for version 1.1\n","  @hydra.main(config_path=\"config\", config_name='config')\n","/usr/local/lib/python3.7/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n","  warnings.warn(msg, UserWarning)\n","[2022-07-23 08:48:01,507][HYDRA] Hydra 1.2.0\n","[2022-07-23 08:48:01,507][HYDRA] ===========\n","[2022-07-23 08:48:01,507][HYDRA] Installed Hydra Plugins\n","[2022-07-23 08:48:01,507][HYDRA] ***********************\n","[2022-07-23 08:48:01,507][HYDRA] \tConfigSource:\n","[2022-07-23 08:48:01,507][HYDRA] \t-------------\n","[2022-07-23 08:48:01,507][HYDRA] \t\tFileConfigSource\n","[2022-07-23 08:48:01,508][HYDRA] \t\tImportlibResourcesConfigSource\n","[2022-07-23 08:48:01,508][HYDRA] \t\tStructuredConfigSource\n","[2022-07-23 08:48:01,508][HYDRA] \tCompletionPlugin:\n","[2022-07-23 08:48:01,508][HYDRA] \t-----------------\n","[2022-07-23 08:48:01,508][HYDRA] \t\tBashCompletion\n","[2022-07-23 08:48:01,508][HYDRA] \t\tFishCompletion\n","[2022-07-23 08:48:01,508][HYDRA] \t\tZshCompletion\n","[2022-07-23 08:48:01,508][HYDRA] \tLauncher:\n","[2022-07-23 08:48:01,508][HYDRA] \t---------\n","[2022-07-23 08:48:01,508][HYDRA] \t\tBasicLauncher\n","[2022-07-23 08:48:01,508][HYDRA] \tSweeper:\n","[2022-07-23 08:48:01,508][HYDRA] \t--------\n","[2022-07-23 08:48:01,508][HYDRA] \t\tBasicSweeper\n","[2022-07-23 08:48:01,508][HYDRA] \n","[2022-07-23 08:48:01,508][HYDRA] Config search path\n","[2022-07-23 08:48:01,508][HYDRA] ******************\n","[2022-07-23 08:48:01,652][HYDRA] | Provider | Search path                                                           |\n","[2022-07-23 08:48:01,652][HYDRA] ------------------------------------------------------------------------------------\n","[2022-07-23 08:48:01,652][HYDRA] | hydra    | pkg://hydra.conf                                                      |\n","[2022-07-23 08:48:01,652][HYDRA] | main     | file:///content/drive/MyDrive/ColabNotebooks/Research/GBTI-Exp/config |\n","[2022-07-23 08:48:01,652][HYDRA] | schema   | structured://                                                         |\n","[2022-07-23 08:48:01,652][HYDRA] ------------------------------------------------------------------------------------\n","[2022-07-23 08:48:01,728][HYDRA] \n","[2022-07-23 08:48:01,728][HYDRA] Defaults Tree\n","[2022-07-23 08:48:01,728][HYDRA] *************\n","[2022-07-23 08:48:01,728][HYDRA] <root>:\n","[2022-07-23 08:48:01,728][HYDRA]   hydra/config:\n","[2022-07-23 08:48:01,728][HYDRA]     hydra/output: default\n","[2022-07-23 08:48:01,728][HYDRA]     hydra/launcher: basic\n","[2022-07-23 08:48:01,729][HYDRA]     hydra/sweeper: basic\n","[2022-07-23 08:48:01,729][HYDRA]     hydra/help: default\n","[2022-07-23 08:48:01,729][HYDRA]     hydra/hydra_help: default\n","[2022-07-23 08:48:01,729][HYDRA]     hydra/hydra_logging: default\n","[2022-07-23 08:48:01,729][HYDRA]     hydra/job_logging: default\n","[2022-07-23 08:48:01,729][HYDRA]     hydra/callbacks: null\n","[2022-07-23 08:48:01,729][HYDRA]     hydra/env: default\n","[2022-07-23 08:48:01,729][HYDRA]     _self_\n","[2022-07-23 08:48:01,729][HYDRA]   config:\n","[2022-07-23 08:48:01,729][HYDRA]     data/YAGO43kET\n","[2022-07-23 08:48:01,729][HYDRA]     model/JointGT\n","[2022-07-23 08:48:01,729][HYDRA]     _self_\n","[2022-07-23 08:48:01,809][HYDRA] \n","[2022-07-23 08:48:01,810][HYDRA] Defaults List\n","[2022-07-23 08:48:01,810][HYDRA] *************\n","[2022-07-23 08:48:01,810][HYDRA] | Config path                 | Package             | _self_ | Parent       | \n","[2022-07-23 08:48:01,810][HYDRA] ------------------------------------------------------------------------------\n","[2022-07-23 08:48:01,810][HYDRA] | hydra/output/default        | hydra               | False  | hydra/config |\n","[2022-07-23 08:48:01,810][HYDRA] | hydra/launcher/basic        | hydra.launcher      | False  | hydra/config |\n","[2022-07-23 08:48:01,810][HYDRA] | hydra/sweeper/basic         | hydra.sweeper       | False  | hydra/config |\n","[2022-07-23 08:48:01,810][HYDRA] | hydra/help/default          | hydra.help          | False  | hydra/config |\n","[2022-07-23 08:48:01,810][HYDRA] | hydra/hydra_help/default    | hydra.hydra_help    | False  | hydra/config |\n","[2022-07-23 08:48:01,810][HYDRA] | hydra/hydra_logging/default | hydra.hydra_logging | False  | hydra/config |\n","[2022-07-23 08:48:01,810][HYDRA] | hydra/job_logging/default   | hydra.job_logging   | False  | hydra/config |\n","[2022-07-23 08:48:01,810][HYDRA] | hydra/env/default           | hydra.env           | False  | hydra/config |\n","[2022-07-23 08:48:01,810][HYDRA] | hydra/config                | hydra               | True   | <root>       |\n","[2022-07-23 08:48:01,810][HYDRA] | data/YAGO43kET              | data                | False  | config       |\n","[2022-07-23 08:48:01,810][HYDRA] | model/JointGT               | model               | False  | config       |\n","[2022-07-23 08:48:01,810][HYDRA] | config                      |                     | True   | <root>       |\n","[2022-07-23 08:48:01,810][HYDRA] ------------------------------------------------------------------------------\n","[2022-07-23 08:48:01,941][HYDRA] Config\n","[2022-07-23 08:48:01,941][HYDRA] ******\n","[2022-07-23 08:48:01,945][HYDRA] data:\n","  name: YAGO43kET\n","  data_dir: data\n","  overwrite: false\n","  is_unigraph: false\n","  lowest_level: false\n","  remove_under_score: true\n","model:\n","  cuda: true\n","  debug: false\n","  save_dir: save\n","  name: JointGT\n","  tokenizer_path: pretrain_model/jointgt_bart\n","  model_path: pretrain_model/jointgt_bart\n","  train_dataset: ET_1_1_train.txt\n","  pretrained_model: Bart\n","  append_another_bos: false\n","  is_in_edge: true\n","  is_out_edge: true\n","  max_node_length: 50\n","  max_edge_length: 60\n","  max_epoch: 100\n","  train_batch_size: 8\n","  valid_batch_size: 8\n","  test_batch_size: 8\n","  num_workers: 4\n","  temperature: 0.5\n","  n_gpus: 1\n","  max_input_length: 256\n","  max_output_length: 128\n","  gradient_accumulation_steps: 1\n","  max_grad_norm: 1.0\n","  early_stopping: true\n","  optimizer:\n","    algorithm: Adam\n","    learning_rate: 1.0e-06\n","  valid:\n","    valid_dataset: ET_1_1_valid.txt\n","    valid_epoch: 1\n","    num_beams: 5\n","    length_penalty: 1.0\n","    max_output_length: 128\n","    clean_up_spaces: true\n","    save_outputs: true\n","    save_one_batch: true\n","  test:\n","    test_dataset: ET_1_1_test.txt\n","    save_outputs: true\n","preprocess:\n","  load_ET: false\n","  load_KG: true\n","  neighbor_sampling: true\n","  neighbor_num: 10\n","  num_layers: 1\n","\n","[2022-07-23 08:48:02,029][transformers.tokenization_utils_base][INFO] - Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.\n","2022-07-23 08:48:02,029 INFO     Model name 'pretrain_model/jointgt_bart' not found in model shortcut name list (facebook/bart-base, facebook/bart-large, facebook/bart-large-mnli, facebook/bart-large-cnn, facebook/bart-large-xsum, yjernite/bart_eli5). Assuming 'pretrain_model/jointgt_bart' is a path, a model identifier, or url to a directory containing tokenizer files.\n","[2022-07-23 08:48:02,030][transformers.tokenization_utils_base][INFO] - Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.\n","2022-07-23 08:48:02,030 INFO     Didn't find file pretrain_model/jointgt_bart/added_tokens.json. We won't load it.\n","[2022-07-23 08:48:02,031][transformers.tokenization_utils_base][INFO] - Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.\n","2022-07-23 08:48:02,031 INFO     Didn't find file pretrain_model/jointgt_bart/special_tokens_map.json. We won't load it.\n","[2022-07-23 08:48:02,031][transformers.tokenization_utils_base][INFO] - Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.\n","2022-07-23 08:48:02,031 INFO     Didn't find file pretrain_model/jointgt_bart/tokenizer_config.json. We won't load it.\n","[2022-07-23 08:48:02,032][transformers.tokenization_utils_base][INFO] - Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.\n","2022-07-23 08:48:02,032 INFO     Didn't find file pretrain_model/jointgt_bart/tokenizer.json. We won't load it.\n","[2022-07-23 08:48:02,032][transformers.tokenization_utils_base][INFO] - loading file pretrain_model/jointgt_bart/vocab.json\n","2022-07-23 08:48:02,032 INFO     loading file pretrain_model/jointgt_bart/vocab.json\n","[2022-07-23 08:48:02,032][transformers.tokenization_utils_base][INFO] - loading file pretrain_model/jointgt_bart/merges.txt\n","2022-07-23 08:48:02,032 INFO     loading file pretrain_model/jointgt_bart/merges.txt\n","[2022-07-23 08:48:02,033][transformers.tokenization_utils_base][INFO] - loading file None\n","2022-07-23 08:48:02,033 INFO     loading file None\n","[2022-07-23 08:48:02,033][transformers.tokenization_utils_base][INFO] - loading file None\n","2022-07-23 08:48:02,033 INFO     loading file None\n","[2022-07-23 08:48:02,033][transformers.tokenization_utils_base][INFO] - loading file None\n","2022-07-23 08:48:02,033 INFO     loading file None\n","[2022-07-23 08:48:02,033][transformers.tokenization_utils_base][INFO] - loading file None\n","2022-07-23 08:48:02,033 INFO     loading file None\n","tcmalloc: large alloc 7651123200 bytes == 0xc552000 @  0x7f23e3badb6b 0x7f23e3bcd379 0x7f232ca3c26e 0x7f232ca3d9e2 0x7f232ddb8e19 0x7f232ddb9b67 0x7f232e196059 0x7f232e8fae6a 0x7f232e8ddf8e 0x7f232e4e2cd5 0x7f232e19a0c0 0x7f232ea6ce64 0x7f232e8caec4 0x7f232e8e1287 0x7f232e53c441 0x7f23d14abd25 0x593784 0x548c51 0x51566f 0x593dd7 0x5118f8 0x549576 0x593fce 0x5118f8 0x4bc98a 0x532b86 0x594a96 0x515600 0x593dd7 0x5118f8 0x549576\n","[2022-07-23 08:48:04,951][transformers.configuration_utils][INFO] - loading configuration file pretrain_model/jointgt_bart/config.json\n","2022-07-23 08:48:04,951 INFO     loading configuration file pretrain_model/jointgt_bart/config.json\n","[2022-07-23 08:48:04,952][transformers.configuration_utils][INFO] - Model config BartConfig {\n","  \"activation_dropout\": 0.1,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"MyBartPretrain\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"d_model\": 768,\n","  \"decoder_attention_heads\": 12,\n","  \"decoder_ffn_dim\": 3072,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 12,\n","  \"encoder_ffn_dim\": 3072,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 6,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"length_penalty\": 1.0,\n","      \"max_length\": 128,\n","      \"min_length\": 12,\n","      \"num_beams\": 4\n","    },\n","    \"summarization_cnn\": {\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"num_beams\": 4\n","    },\n","    \"summarization_xsum\": {\n","      \"length_penalty\": 1.0,\n","      \"max_length\": 62,\n","      \"min_length\": 11,\n","      \"num_beams\": 6\n","    }\n","  },\n","  \"vocab_size\": 50265\n","}\n","\n","2022-07-23 08:48:04,952 INFO     Model config BartConfig {\n","  \"activation_dropout\": 0.1,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"MyBartPretrain\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"d_model\": 768,\n","  \"decoder_attention_heads\": 12,\n","  \"decoder_ffn_dim\": 3072,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 12,\n","  \"encoder_ffn_dim\": 3072,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 6,\n","  \"eos_token_id\": 2,\n","  \"extra_pos_embeddings\": 2,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"length_penalty\": 1.0,\n","      \"max_length\": 128,\n","      \"min_length\": 12,\n","      \"num_beams\": 4\n","    },\n","    \"summarization_cnn\": {\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 142,\n","      \"min_length\": 56,\n","      \"num_beams\": 4\n","    },\n","    \"summarization_xsum\": {\n","      \"length_penalty\": 1.0,\n","      \"max_length\": 62,\n","      \"min_length\": 11,\n","      \"num_beams\": 6\n","    }\n","  },\n","  \"vocab_size\": 50265\n","}\n","\n","[2022-07-23 08:48:04,953][transformers.modeling_utils][INFO] - loading weights file pretrain_model/jointgt_bart/pytorch_model.bin\n","2022-07-23 08:48:04,953 INFO     loading weights file pretrain_model/jointgt_bart/pytorch_model.bin\n","[2022-07-23 08:48:12,138][transformers.modeling_utils][INFO] - All model checkpoint weights were used when initializing MyBartForConditionalGeneration.\n","\n","2022-07-23 08:48:12,138 INFO     All model checkpoint weights were used when initializing MyBartForConditionalGeneration.\n","\n","[2022-07-23 08:48:12,139][transformers.modeling_utils][INFO] - All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at pretrain_model/jointgt_bart.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.\n","2022-07-23 08:48:12,139 INFO     All the weights of MyBartForConditionalGeneration were initialized from the model checkpoint at pretrain_model/jointgt_bart.\n","If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyBartForConditionalGeneration for predictions without further training.\n","[2022-07-23 08:48:16,331][root][DEBUG] - Parameter model.model.shared.weight: torch.Size([50265, 768]), require_grad=True\n","2022-07-23 08:48:16,331 DEBUG    Parameter model.model.shared.weight: torch.Size([50265, 768]), require_grad=True\n","[2022-07-23 08:48:16,332][root][DEBUG] - Parameter model.model.encoder.embed_positions.weight: torch.Size([1026, 768]), require_grad=True\n","2022-07-23 08:48:16,332 DEBUG    Parameter model.model.encoder.embed_positions.weight: torch.Size([1026, 768]), require_grad=True\n","[2022-07-23 08:48:16,332][root][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,332 DEBUG    Parameter model.model.encoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,332][root][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,332 DEBUG    Parameter model.model.encoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,332][root][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,332 DEBUG    Parameter model.model.encoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,333][root][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,333 DEBUG    Parameter model.model.encoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,333][root][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,333 DEBUG    Parameter model.model.encoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,333][root][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,333 DEBUG    Parameter model.model.encoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,333][root][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,333 DEBUG    Parameter model.model.encoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,333][root][DEBUG] - Parameter model.model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,333 DEBUG    Parameter model.model.encoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,333][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,333 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,334][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,334 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,334][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,334 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,334][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,334 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,334][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,334 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,335][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,335 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,335][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,335 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,335][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,335 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,335][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,335 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,335][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,335 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,335][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,335 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,336][root][DEBUG] - Parameter model.model.encoder.layers.0.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,336 DEBUG    Parameter model.model.encoder.layers.0.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,336][root][DEBUG] - Parameter model.model.encoder.layers.0.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,336 DEBUG    Parameter model.model.encoder.layers.0.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,336][root][DEBUG] - Parameter model.model.encoder.layers.0.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,336 DEBUG    Parameter model.model.encoder.layers.0.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,336][root][DEBUG] - Parameter model.model.encoder.layers.0.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,336 DEBUG    Parameter model.model.encoder.layers.0.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,336][root][DEBUG] - Parameter model.model.encoder.layers.0.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,336 DEBUG    Parameter model.model.encoder.layers.0.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,336][root][DEBUG] - Parameter model.model.encoder.layers.0.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,336 DEBUG    Parameter model.model.encoder.layers.0.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,336][root][DEBUG] - Parameter model.model.encoder.layers.0.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,336 DEBUG    Parameter model.model.encoder.layers.0.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,337][root][DEBUG] - Parameter model.model.encoder.layers.0.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,337 DEBUG    Parameter model.model.encoder.layers.0.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,337][root][DEBUG] - Parameter model.model.encoder.layers.0.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,337 DEBUG    Parameter model.model.encoder.layers.0.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,337][root][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,337 DEBUG    Parameter model.model.encoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,337][root][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,337 DEBUG    Parameter model.model.encoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,337][root][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,337 DEBUG    Parameter model.model.encoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,337][root][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,337 DEBUG    Parameter model.model.encoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,338][root][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,338 DEBUG    Parameter model.model.encoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,338][root][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,338 DEBUG    Parameter model.model.encoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,338][root][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,338 DEBUG    Parameter model.model.encoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,338][root][DEBUG] - Parameter model.model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,338 DEBUG    Parameter model.model.encoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,338][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,338 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,338][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,338 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,339][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,339 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,339][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,339 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,339][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,339 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,339][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,339 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,339][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,339 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,339][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,339 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,339][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,339 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,340][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,340 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,340][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,340 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,340][root][DEBUG] - Parameter model.model.encoder.layers.1.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,340 DEBUG    Parameter model.model.encoder.layers.1.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,340][root][DEBUG] - Parameter model.model.encoder.layers.1.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,340 DEBUG    Parameter model.model.encoder.layers.1.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,340][root][DEBUG] - Parameter model.model.encoder.layers.1.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,340 DEBUG    Parameter model.model.encoder.layers.1.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,340][root][DEBUG] - Parameter model.model.encoder.layers.1.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,340 DEBUG    Parameter model.model.encoder.layers.1.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,341][root][DEBUG] - Parameter model.model.encoder.layers.1.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,341 DEBUG    Parameter model.model.encoder.layers.1.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,341][root][DEBUG] - Parameter model.model.encoder.layers.1.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,341 DEBUG    Parameter model.model.encoder.layers.1.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,341][root][DEBUG] - Parameter model.model.encoder.layers.1.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,341 DEBUG    Parameter model.model.encoder.layers.1.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,341][root][DEBUG] - Parameter model.model.encoder.layers.1.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,341 DEBUG    Parameter model.model.encoder.layers.1.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,341][root][DEBUG] - Parameter model.model.encoder.layers.1.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,341 DEBUG    Parameter model.model.encoder.layers.1.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,341][root][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,341 DEBUG    Parameter model.model.encoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,342][root][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,342 DEBUG    Parameter model.model.encoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,342][root][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,342 DEBUG    Parameter model.model.encoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,342][root][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,342 DEBUG    Parameter model.model.encoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,342][root][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,342 DEBUG    Parameter model.model.encoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,407][root][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,407 DEBUG    Parameter model.model.encoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,407][root][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,407 DEBUG    Parameter model.model.encoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,407][root][DEBUG] - Parameter model.model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,407 DEBUG    Parameter model.model.encoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,407][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,407 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,408][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,408 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,408][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,408 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,408][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,408 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,408][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,408 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,409][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,409 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,409][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,409 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,409][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,409 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,409][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,409 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,410][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,410 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,410][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,410 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,410][root][DEBUG] - Parameter model.model.encoder.layers.2.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,410 DEBUG    Parameter model.model.encoder.layers.2.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,411][root][DEBUG] - Parameter model.model.encoder.layers.2.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,411 DEBUG    Parameter model.model.encoder.layers.2.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,411][root][DEBUG] - Parameter model.model.encoder.layers.2.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,411 DEBUG    Parameter model.model.encoder.layers.2.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,411][root][DEBUG] - Parameter model.model.encoder.layers.2.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,411 DEBUG    Parameter model.model.encoder.layers.2.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,412][root][DEBUG] - Parameter model.model.encoder.layers.2.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,412 DEBUG    Parameter model.model.encoder.layers.2.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,412][root][DEBUG] - Parameter model.model.encoder.layers.2.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,412 DEBUG    Parameter model.model.encoder.layers.2.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,413][root][DEBUG] - Parameter model.model.encoder.layers.2.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,413 DEBUG    Parameter model.model.encoder.layers.2.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,413][root][DEBUG] - Parameter model.model.encoder.layers.2.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,413 DEBUG    Parameter model.model.encoder.layers.2.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,414][root][DEBUG] - Parameter model.model.encoder.layers.2.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,414 DEBUG    Parameter model.model.encoder.layers.2.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,414][root][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,414 DEBUG    Parameter model.model.encoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,415][root][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,415 DEBUG    Parameter model.model.encoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,415][root][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,415 DEBUG    Parameter model.model.encoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,415][root][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,415 DEBUG    Parameter model.model.encoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,415][root][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,415 DEBUG    Parameter model.model.encoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,416][root][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,416 DEBUG    Parameter model.model.encoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,416][root][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,416 DEBUG    Parameter model.model.encoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,416][root][DEBUG] - Parameter model.model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,416 DEBUG    Parameter model.model.encoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,416][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,416 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,416][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,416 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,416][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,416 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,417][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,417 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,417][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,417 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,417][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,417 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,417][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,417 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,421][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,421 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,421][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,421 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,422][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,422 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,422][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,422 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,422][root][DEBUG] - Parameter model.model.encoder.layers.3.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,422 DEBUG    Parameter model.model.encoder.layers.3.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,422][root][DEBUG] - Parameter model.model.encoder.layers.3.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,422 DEBUG    Parameter model.model.encoder.layers.3.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,423][root][DEBUG] - Parameter model.model.encoder.layers.3.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,423 DEBUG    Parameter model.model.encoder.layers.3.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,423][root][DEBUG] - Parameter model.model.encoder.layers.3.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,423 DEBUG    Parameter model.model.encoder.layers.3.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,423][root][DEBUG] - Parameter model.model.encoder.layers.3.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,423 DEBUG    Parameter model.model.encoder.layers.3.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,423][root][DEBUG] - Parameter model.model.encoder.layers.3.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,423 DEBUG    Parameter model.model.encoder.layers.3.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,423][root][DEBUG] - Parameter model.model.encoder.layers.3.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,423 DEBUG    Parameter model.model.encoder.layers.3.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,424][root][DEBUG] - Parameter model.model.encoder.layers.3.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,424 DEBUG    Parameter model.model.encoder.layers.3.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,424][root][DEBUG] - Parameter model.model.encoder.layers.3.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,424 DEBUG    Parameter model.model.encoder.layers.3.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,424][root][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,424 DEBUG    Parameter model.model.encoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,424][root][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,424 DEBUG    Parameter model.model.encoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,424][root][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,424 DEBUG    Parameter model.model.encoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,425][root][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,425 DEBUG    Parameter model.model.encoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,425][root][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,425 DEBUG    Parameter model.model.encoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,425][root][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,425 DEBUG    Parameter model.model.encoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,425][root][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,425 DEBUG    Parameter model.model.encoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,425][root][DEBUG] - Parameter model.model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,425 DEBUG    Parameter model.model.encoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,426][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,426 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,426][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,426 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,426][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,426 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,426][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,426 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,426][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,426 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,427][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,427 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,427][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,427 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,427][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,427 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,427][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,427 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,427][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,427 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,428][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,428 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,428][root][DEBUG] - Parameter model.model.encoder.layers.4.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,428 DEBUG    Parameter model.model.encoder.layers.4.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,428][root][DEBUG] - Parameter model.model.encoder.layers.4.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,428 DEBUG    Parameter model.model.encoder.layers.4.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,428][root][DEBUG] - Parameter model.model.encoder.layers.4.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,428 DEBUG    Parameter model.model.encoder.layers.4.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,428][root][DEBUG] - Parameter model.model.encoder.layers.4.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,428 DEBUG    Parameter model.model.encoder.layers.4.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,429][root][DEBUG] - Parameter model.model.encoder.layers.4.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,429 DEBUG    Parameter model.model.encoder.layers.4.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,429][root][DEBUG] - Parameter model.model.encoder.layers.4.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,429 DEBUG    Parameter model.model.encoder.layers.4.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,429][root][DEBUG] - Parameter model.model.encoder.layers.4.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,429 DEBUG    Parameter model.model.encoder.layers.4.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,429][root][DEBUG] - Parameter model.model.encoder.layers.4.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,429 DEBUG    Parameter model.model.encoder.layers.4.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,429][root][DEBUG] - Parameter model.model.encoder.layers.4.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,429 DEBUG    Parameter model.model.encoder.layers.4.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,430][root][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,430 DEBUG    Parameter model.model.encoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,430][root][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,430 DEBUG    Parameter model.model.encoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,430][root][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,430 DEBUG    Parameter model.model.encoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,430][root][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,430 DEBUG    Parameter model.model.encoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,431][root][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,431 DEBUG    Parameter model.model.encoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,431][root][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,431 DEBUG    Parameter model.model.encoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,431][root][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,431 DEBUG    Parameter model.model.encoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,431][root][DEBUG] - Parameter model.model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,431 DEBUG    Parameter model.model.encoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,515][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,515 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,516][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,516 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,516][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,516 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,516][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,516 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,517][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,517 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,517][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,517 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,517][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,517 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.rel_v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,517][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,517 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.rel_v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,518][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,518 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.rel_k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,518][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,518 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.rel_k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,518][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,518 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,518][root][DEBUG] - Parameter model.model.encoder.layers.5.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,518 DEBUG    Parameter model.model.encoder.layers.5.self_struc_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,518][root][DEBUG] - Parameter model.model.encoder.layers.5.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,518 DEBUG    Parameter model.model.encoder.layers.5.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,519][root][DEBUG] - Parameter model.model.encoder.layers.5.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,519 DEBUG    Parameter model.model.encoder.layers.5.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,519][root][DEBUG] - Parameter model.model.encoder.layers.5.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,519 DEBUG    Parameter model.model.encoder.layers.5.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,519][root][DEBUG] - Parameter model.model.encoder.layers.5.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,519 DEBUG    Parameter model.model.encoder.layers.5.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,519][root][DEBUG] - Parameter model.model.encoder.layers.5.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,519 DEBUG    Parameter model.model.encoder.layers.5.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,519][root][DEBUG] - Parameter model.model.encoder.layers.5.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,519 DEBUG    Parameter model.model.encoder.layers.5.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,520][root][DEBUG] - Parameter model.model.encoder.layers.5.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,520 DEBUG    Parameter model.model.encoder.layers.5.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,520][root][DEBUG] - Parameter model.model.encoder.layers.5.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,520 DEBUG    Parameter model.model.encoder.layers.5.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,520][root][DEBUG] - Parameter model.model.encoder.layernorm_embedding.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,520 DEBUG    Parameter model.model.encoder.layernorm_embedding.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,520][root][DEBUG] - Parameter model.model.encoder.layernorm_embedding.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,520 DEBUG    Parameter model.model.encoder.layernorm_embedding.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,521][root][DEBUG] - Parameter model.model.decoder.embed_positions.weight: torch.Size([1026, 768]), require_grad=True\n","2022-07-23 08:48:16,521 DEBUG    Parameter model.model.decoder.embed_positions.weight: torch.Size([1026, 768]), require_grad=True\n","[2022-07-23 08:48:16,521][root][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,521 DEBUG    Parameter model.model.decoder.layers.0.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,521][root][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,521 DEBUG    Parameter model.model.decoder.layers.0.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,521][root][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,521 DEBUG    Parameter model.model.decoder.layers.0.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,521][root][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,521 DEBUG    Parameter model.model.decoder.layers.0.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,522][root][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,522 DEBUG    Parameter model.model.decoder.layers.0.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,522][root][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,522 DEBUG    Parameter model.model.decoder.layers.0.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,522][root][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,522 DEBUG    Parameter model.model.decoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,522][root][DEBUG] - Parameter model.model.decoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,522 DEBUG    Parameter model.model.decoder.layers.0.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,523][root][DEBUG] - Parameter model.model.decoder.layers.0.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,523 DEBUG    Parameter model.model.decoder.layers.0.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,523][root][DEBUG] - Parameter model.model.decoder.layers.0.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,523 DEBUG    Parameter model.model.decoder.layers.0.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,523][root][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,523 DEBUG    Parameter model.model.decoder.layers.0.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,523][root][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,523 DEBUG    Parameter model.model.decoder.layers.0.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,523][root][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,523 DEBUG    Parameter model.model.decoder.layers.0.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,524][root][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,524 DEBUG    Parameter model.model.decoder.layers.0.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,524][root][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,524 DEBUG    Parameter model.model.decoder.layers.0.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,524][root][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,524 DEBUG    Parameter model.model.decoder.layers.0.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,524][root][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,524 DEBUG    Parameter model.model.decoder.layers.0.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,524][root][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,524 DEBUG    Parameter model.model.decoder.layers.0.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,525][root][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,525 DEBUG    Parameter model.model.decoder.layers.0.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,525][root][DEBUG] - Parameter model.model.decoder.layers.0.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,525 DEBUG    Parameter model.model.decoder.layers.0.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,525][root][DEBUG] - Parameter model.model.decoder.layers.0.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,525 DEBUG    Parameter model.model.decoder.layers.0.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,525][root][DEBUG] - Parameter model.model.decoder.layers.0.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,525 DEBUG    Parameter model.model.decoder.layers.0.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,525][root][DEBUG] - Parameter model.model.decoder.layers.0.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,525 DEBUG    Parameter model.model.decoder.layers.0.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,526][root][DEBUG] - Parameter model.model.decoder.layers.0.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,526 DEBUG    Parameter model.model.decoder.layers.0.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,526][root][DEBUG] - Parameter model.model.decoder.layers.0.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,526 DEBUG    Parameter model.model.decoder.layers.0.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,526][root][DEBUG] - Parameter model.model.decoder.layers.0.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,526 DEBUG    Parameter model.model.decoder.layers.0.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,526][root][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,526 DEBUG    Parameter model.model.decoder.layers.1.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,527][root][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,527 DEBUG    Parameter model.model.decoder.layers.1.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,527][root][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,527 DEBUG    Parameter model.model.decoder.layers.1.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,527][root][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,527 DEBUG    Parameter model.model.decoder.layers.1.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,527][root][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,527 DEBUG    Parameter model.model.decoder.layers.1.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,527][root][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,527 DEBUG    Parameter model.model.decoder.layers.1.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,528][root][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,528 DEBUG    Parameter model.model.decoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,528][root][DEBUG] - Parameter model.model.decoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,528 DEBUG    Parameter model.model.decoder.layers.1.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,528][root][DEBUG] - Parameter model.model.decoder.layers.1.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,528 DEBUG    Parameter model.model.decoder.layers.1.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,528][root][DEBUG] - Parameter model.model.decoder.layers.1.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,528 DEBUG    Parameter model.model.decoder.layers.1.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,528][root][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,528 DEBUG    Parameter model.model.decoder.layers.1.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,529][root][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,529 DEBUG    Parameter model.model.decoder.layers.1.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,529][root][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,529 DEBUG    Parameter model.model.decoder.layers.1.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,529][root][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,529 DEBUG    Parameter model.model.decoder.layers.1.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,529][root][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,529 DEBUG    Parameter model.model.decoder.layers.1.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,530][root][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,530 DEBUG    Parameter model.model.decoder.layers.1.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,530][root][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,530 DEBUG    Parameter model.model.decoder.layers.1.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,530][root][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,530 DEBUG    Parameter model.model.decoder.layers.1.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,530][root][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,530 DEBUG    Parameter model.model.decoder.layers.1.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,531][root][DEBUG] - Parameter model.model.decoder.layers.1.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,531 DEBUG    Parameter model.model.decoder.layers.1.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,531][root][DEBUG] - Parameter model.model.decoder.layers.1.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,531 DEBUG    Parameter model.model.decoder.layers.1.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,532][root][DEBUG] - Parameter model.model.decoder.layers.1.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,532 DEBUG    Parameter model.model.decoder.layers.1.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,532][root][DEBUG] - Parameter model.model.decoder.layers.1.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,532 DEBUG    Parameter model.model.decoder.layers.1.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,532][root][DEBUG] - Parameter model.model.decoder.layers.1.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,532 DEBUG    Parameter model.model.decoder.layers.1.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,532][root][DEBUG] - Parameter model.model.decoder.layers.1.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,532 DEBUG    Parameter model.model.decoder.layers.1.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,532][root][DEBUG] - Parameter model.model.decoder.layers.1.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,532 DEBUG    Parameter model.model.decoder.layers.1.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,533][root][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,533 DEBUG    Parameter model.model.decoder.layers.2.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,533][root][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,533 DEBUG    Parameter model.model.decoder.layers.2.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,533][root][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,533 DEBUG    Parameter model.model.decoder.layers.2.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,533][root][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,533 DEBUG    Parameter model.model.decoder.layers.2.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,534][root][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,534 DEBUG    Parameter model.model.decoder.layers.2.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,534][root][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,534 DEBUG    Parameter model.model.decoder.layers.2.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,534][root][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,534 DEBUG    Parameter model.model.decoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,534][root][DEBUG] - Parameter model.model.decoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,534 DEBUG    Parameter model.model.decoder.layers.2.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,535][root][DEBUG] - Parameter model.model.decoder.layers.2.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,535 DEBUG    Parameter model.model.decoder.layers.2.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,535][root][DEBUG] - Parameter model.model.decoder.layers.2.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,535 DEBUG    Parameter model.model.decoder.layers.2.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,535][root][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,535 DEBUG    Parameter model.model.decoder.layers.2.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,535][root][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,535 DEBUG    Parameter model.model.decoder.layers.2.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,535][root][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,535 DEBUG    Parameter model.model.decoder.layers.2.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,536][root][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,536 DEBUG    Parameter model.model.decoder.layers.2.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,536][root][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,536 DEBUG    Parameter model.model.decoder.layers.2.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,536][root][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,536 DEBUG    Parameter model.model.decoder.layers.2.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,536][root][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,536 DEBUG    Parameter model.model.decoder.layers.2.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,536][root][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,536 DEBUG    Parameter model.model.decoder.layers.2.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,537][root][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,537 DEBUG    Parameter model.model.decoder.layers.2.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,537][root][DEBUG] - Parameter model.model.decoder.layers.2.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,537 DEBUG    Parameter model.model.decoder.layers.2.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,537][root][DEBUG] - Parameter model.model.decoder.layers.2.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,537 DEBUG    Parameter model.model.decoder.layers.2.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,537][root][DEBUG] - Parameter model.model.decoder.layers.2.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,537 DEBUG    Parameter model.model.decoder.layers.2.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,537][root][DEBUG] - Parameter model.model.decoder.layers.2.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,537 DEBUG    Parameter model.model.decoder.layers.2.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,538][root][DEBUG] - Parameter model.model.decoder.layers.2.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,538 DEBUG    Parameter model.model.decoder.layers.2.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,538][root][DEBUG] - Parameter model.model.decoder.layers.2.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,538 DEBUG    Parameter model.model.decoder.layers.2.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,538][root][DEBUG] - Parameter model.model.decoder.layers.2.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,538 DEBUG    Parameter model.model.decoder.layers.2.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,538][root][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,538 DEBUG    Parameter model.model.decoder.layers.3.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,538][root][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,538 DEBUG    Parameter model.model.decoder.layers.3.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,538][root][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,538 DEBUG    Parameter model.model.decoder.layers.3.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,539][root][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,539 DEBUG    Parameter model.model.decoder.layers.3.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,539][root][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,539 DEBUG    Parameter model.model.decoder.layers.3.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,539][root][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,539 DEBUG    Parameter model.model.decoder.layers.3.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,539][root][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,539 DEBUG    Parameter model.model.decoder.layers.3.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,539][root][DEBUG] - Parameter model.model.decoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,539 DEBUG    Parameter model.model.decoder.layers.3.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,539][root][DEBUG] - Parameter model.model.decoder.layers.3.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,539 DEBUG    Parameter model.model.decoder.layers.3.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,540][root][DEBUG] - Parameter model.model.decoder.layers.3.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,540 DEBUG    Parameter model.model.decoder.layers.3.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,540][root][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,540 DEBUG    Parameter model.model.decoder.layers.3.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,540][root][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,540 DEBUG    Parameter model.model.decoder.layers.3.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,540][root][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,540 DEBUG    Parameter model.model.decoder.layers.3.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,540][root][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,540 DEBUG    Parameter model.model.decoder.layers.3.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,540][root][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,540 DEBUG    Parameter model.model.decoder.layers.3.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,541][root][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,541 DEBUG    Parameter model.model.decoder.layers.3.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,541][root][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,541 DEBUG    Parameter model.model.decoder.layers.3.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,541][root][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,541 DEBUG    Parameter model.model.decoder.layers.3.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,541][root][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,541 DEBUG    Parameter model.model.decoder.layers.3.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,541][root][DEBUG] - Parameter model.model.decoder.layers.3.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,541 DEBUG    Parameter model.model.decoder.layers.3.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,541][root][DEBUG] - Parameter model.model.decoder.layers.3.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,541 DEBUG    Parameter model.model.decoder.layers.3.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,541][root][DEBUG] - Parameter model.model.decoder.layers.3.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,541 DEBUG    Parameter model.model.decoder.layers.3.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,542][root][DEBUG] - Parameter model.model.decoder.layers.3.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,542 DEBUG    Parameter model.model.decoder.layers.3.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,542][root][DEBUG] - Parameter model.model.decoder.layers.3.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,542 DEBUG    Parameter model.model.decoder.layers.3.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,542][root][DEBUG] - Parameter model.model.decoder.layers.3.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,542 DEBUG    Parameter model.model.decoder.layers.3.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,542][root][DEBUG] - Parameter model.model.decoder.layers.3.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,542 DEBUG    Parameter model.model.decoder.layers.3.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,542][root][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,542 DEBUG    Parameter model.model.decoder.layers.4.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,542][root][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,542 DEBUG    Parameter model.model.decoder.layers.4.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,542][root][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,542 DEBUG    Parameter model.model.decoder.layers.4.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,543][root][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,543 DEBUG    Parameter model.model.decoder.layers.4.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,631][root][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,631 DEBUG    Parameter model.model.decoder.layers.4.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,632][root][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,632 DEBUG    Parameter model.model.decoder.layers.4.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,632][root][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,632 DEBUG    Parameter model.model.decoder.layers.4.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,632][root][DEBUG] - Parameter model.model.decoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,632 DEBUG    Parameter model.model.decoder.layers.4.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,632][root][DEBUG] - Parameter model.model.decoder.layers.4.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,632 DEBUG    Parameter model.model.decoder.layers.4.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,633][root][DEBUG] - Parameter model.model.decoder.layers.4.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,633 DEBUG    Parameter model.model.decoder.layers.4.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,633][root][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,633 DEBUG    Parameter model.model.decoder.layers.4.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,633][root][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,633 DEBUG    Parameter model.model.decoder.layers.4.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,633][root][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,633 DEBUG    Parameter model.model.decoder.layers.4.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,634][root][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,634 DEBUG    Parameter model.model.decoder.layers.4.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,634][root][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,634 DEBUG    Parameter model.model.decoder.layers.4.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,634][root][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,634 DEBUG    Parameter model.model.decoder.layers.4.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,634][root][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,634 DEBUG    Parameter model.model.decoder.layers.4.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,635][root][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,635 DEBUG    Parameter model.model.decoder.layers.4.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,635][root][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,635 DEBUG    Parameter model.model.decoder.layers.4.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,635][root][DEBUG] - Parameter model.model.decoder.layers.4.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,635 DEBUG    Parameter model.model.decoder.layers.4.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,635][root][DEBUG] - Parameter model.model.decoder.layers.4.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,635 DEBUG    Parameter model.model.decoder.layers.4.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,635][root][DEBUG] - Parameter model.model.decoder.layers.4.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,635 DEBUG    Parameter model.model.decoder.layers.4.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,636][root][DEBUG] - Parameter model.model.decoder.layers.4.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,636 DEBUG    Parameter model.model.decoder.layers.4.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,636][root][DEBUG] - Parameter model.model.decoder.layers.4.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,636 DEBUG    Parameter model.model.decoder.layers.4.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,636][root][DEBUG] - Parameter model.model.decoder.layers.4.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,636 DEBUG    Parameter model.model.decoder.layers.4.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,636][root][DEBUG] - Parameter model.model.decoder.layers.4.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,636 DEBUG    Parameter model.model.decoder.layers.4.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,637][root][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,637 DEBUG    Parameter model.model.decoder.layers.5.self_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,637][root][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,637 DEBUG    Parameter model.model.decoder.layers.5.self_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,637][root][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,637 DEBUG    Parameter model.model.decoder.layers.5.self_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,637][root][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,637 DEBUG    Parameter model.model.decoder.layers.5.self_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,637][root][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,637 DEBUG    Parameter model.model.decoder.layers.5.self_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,638][root][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,638 DEBUG    Parameter model.model.decoder.layers.5.self_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,638][root][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,638 DEBUG    Parameter model.model.decoder.layers.5.self_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,638][root][DEBUG] - Parameter model.model.decoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,638 DEBUG    Parameter model.model.decoder.layers.5.self_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,638][root][DEBUG] - Parameter model.model.decoder.layers.5.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,638 DEBUG    Parameter model.model.decoder.layers.5.self_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,639][root][DEBUG] - Parameter model.model.decoder.layers.5.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,639 DEBUG    Parameter model.model.decoder.layers.5.self_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,639][root][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,639 DEBUG    Parameter model.model.decoder.layers.5.encoder_attn.k_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,639][root][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,639 DEBUG    Parameter model.model.decoder.layers.5.encoder_attn.k_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,639][root][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,639 DEBUG    Parameter model.model.decoder.layers.5.encoder_attn.v_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,640][root][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,640 DEBUG    Parameter model.model.decoder.layers.5.encoder_attn.v_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,640][root][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,640 DEBUG    Parameter model.model.decoder.layers.5.encoder_attn.q_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,640][root][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,640 DEBUG    Parameter model.model.decoder.layers.5.encoder_attn.q_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,640][root][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","2022-07-23 08:48:16,640 DEBUG    Parameter model.model.decoder.layers.5.encoder_attn.out_proj.weight: torch.Size([768, 768]), require_grad=True\n","[2022-07-23 08:48:16,640][root][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,640 DEBUG    Parameter model.model.decoder.layers.5.encoder_attn.out_proj.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,641][root][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,641 DEBUG    Parameter model.model.decoder.layers.5.encoder_attn_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,641][root][DEBUG] - Parameter model.model.decoder.layers.5.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,641 DEBUG    Parameter model.model.decoder.layers.5.encoder_attn_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,641][root][DEBUG] - Parameter model.model.decoder.layers.5.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","2022-07-23 08:48:16,641 DEBUG    Parameter model.model.decoder.layers.5.fc1.weight: torch.Size([3072, 768]), require_grad=True\n","[2022-07-23 08:48:16,641][root][DEBUG] - Parameter model.model.decoder.layers.5.fc1.bias: torch.Size([3072]), require_grad=True\n","2022-07-23 08:48:16,641 DEBUG    Parameter model.model.decoder.layers.5.fc1.bias: torch.Size([3072]), require_grad=True\n","[2022-07-23 08:48:16,642][root][DEBUG] - Parameter model.model.decoder.layers.5.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","2022-07-23 08:48:16,642 DEBUG    Parameter model.model.decoder.layers.5.fc2.weight: torch.Size([768, 3072]), require_grad=True\n","[2022-07-23 08:48:16,642][root][DEBUG] - Parameter model.model.decoder.layers.5.fc2.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,642 DEBUG    Parameter model.model.decoder.layers.5.fc2.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,642][root][DEBUG] - Parameter model.model.decoder.layers.5.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,642 DEBUG    Parameter model.model.decoder.layers.5.final_layer_norm.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,642][root][DEBUG] - Parameter model.model.decoder.layers.5.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,642 DEBUG    Parameter model.model.decoder.layers.5.final_layer_norm.bias: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,643][root][DEBUG] - Parameter model.model.decoder.layernorm_embedding.weight: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,643 DEBUG    Parameter model.model.decoder.layernorm_embedding.weight: torch.Size([768]), require_grad=True\n","[2022-07-23 08:48:16,643][root][DEBUG] - Parameter model.model.decoder.layernorm_embedding.bias: torch.Size([768]), require_grad=True\n","2022-07-23 08:48:16,643 DEBUG    Parameter model.model.decoder.layernorm_embedding.bias: torch.Size([768]), require_grad=True\n","Iteration:   0% 0/1400 [00:00<?, ?it/s]The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","The current process just got forked. Disabling parallelism to avoid deadlocks...\n","To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)\n","/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n","To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n","  return torch.floor_divide(self, other)\n","Iteration: 100% 1400/1400 [17:37<00:00,  1.32it/s]\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","[2022-07-23 09:05:59,739][root][INFO] - test data set : ET_1_1_test.txt\n","2022-07-23 09:05:59,739 INFO     test data set : ET_1_1_test.txt\n","[2022-07-23 09:05:59,740][root][INFO] - N-grams: 1-0.0025195015979211526, 2-2.2675206331205e-310, 3-2.2675206331205e-310, 4-2.2675206331205e-310\n","2022-07-23 09:05:59,740 INFO     N-grams: 1-0.0025195015979211526, 2-2.2675206331205e-310, 3-2.2675206331205e-310, 4-2.2675206331205e-310\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"xiRHm4nruPw9"},"execution_count":null,"outputs":[]}]}