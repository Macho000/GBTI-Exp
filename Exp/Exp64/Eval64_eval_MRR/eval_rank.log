[2022-12-03 19:25:02,672][transformers.tokenization_utils_base][INFO] - Model name 'pretrain_model/jointgt_t5' not found in model shortcut name list (t5-small, t5-base, t5-large, t5-3b, t5-11b). Assuming 'pretrain_model/jointgt_t5' is a path, a model identifier, or url to a directory containing tokenizer files.
[2022-12-03 19:25:02,673][transformers.tokenization_utils_base][INFO] - Didn't find file pretrain_model/jointgt_t5/added_tokens.json. We won't load it.
[2022-12-03 19:25:02,674][transformers.tokenization_utils_base][INFO] - Didn't find file pretrain_model/jointgt_t5/special_tokens_map.json. We won't load it.
[2022-12-03 19:25:02,674][transformers.tokenization_utils_base][INFO] - Didn't find file pretrain_model/jointgt_t5/tokenizer_config.json. We won't load it.
[2022-12-03 19:25:02,675][transformers.tokenization_utils_base][INFO] - loading file pretrain_model/jointgt_t5/spiece.model
[2022-12-03 19:25:02,676][transformers.tokenization_utils_base][INFO] - loading file None
[2022-12-03 19:25:02,676][transformers.tokenization_utils_base][INFO] - loading file None
[2022-12-03 19:25:02,676][transformers.tokenization_utils_base][INFO] - loading file None
[2022-12-03 19:25:02,677][transformers.tokenization_utils_base][INFO] - loading file pretrain_model/jointgt_t5/tokenizer.json
[2022-12-03 19:25:25,551][transformers.configuration_utils][INFO] - loading configuration file pretrain_model/jointgt_t5/config.json
[2022-12-03 19:25:25,558][transformers.configuration_utils][INFO] - Model config T5Config {
  "architectures": [
    "MyT5Pretrain"
  ],
  "d_ff": 3072,
  "d_kv": 64,
  "d_model": 768,
  "decoder_start_token_id": 0,
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_heads": 12,
  "num_layers": 12,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "vocab_size": 32128
}

[2022-12-03 19:25:25,559][transformers.modeling_utils][INFO] - loading weights file pretrain_model/jointgt_t5/pytorch_model.bin
[2022-12-03 19:25:37,261][transformers.modeling_utils][INFO] - All model checkpoint weights were used when initializing MyT5ForConditionalGeneration.

[2022-12-03 19:25:37,262][transformers.modeling_utils][INFO] - All the weights of MyT5ForConditionalGeneration were initialized from the model checkpoint at pretrain_model/jointgt_t5.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use MyT5ForConditionalGeneration for predictions without further training.
[2022-12-03 19:25:40,881][root][INFO] - MRR: 0.126280
[2022-12-03 19:25:40,882][root][INFO] - MR: 2975.501030
[2022-12-03 19:25:40,882][root][INFO] - HITS@1: 0.070322
[2022-12-03 19:25:40,882][root][INFO] - HITS@3: 0.177932
[2022-12-03 19:25:40,882][root][INFO] - HITS@10: 0.225962
